{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import logging\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "\n",
    "# gym\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import gym_ple\n",
    "\n",
    "# disply\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select gpu\n",
    "gpu_index = 0\n",
    "from tensorflow.keras.backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=str(gpu_index),\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    state: gym.spaces.Box\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: gym.spaces.Box\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir='./logs/', dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.keras.callbacks.TensorBoard(self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "    \n",
    "    @property\n",
    "    def ckpt_path(self):\n",
    "        return os.path.join(self.log_dir, 'model.ckpt')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, is_training: bool, num_outputs: int):\n",
    "        super(AgentModel, self).__init__()\n",
    "        self.is_training = is_training\n",
    "        self.num_outputs = num_outputs\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        relu = tf.nn.relu\n",
    "        self.conv_01 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, padding='same', kernel_initializer=k_init, activation=relu) \n",
    "        self.conv_02 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=4, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.conv_03 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(256, kernel_initializer=k_init, activation=relu)\n",
    "        self.output_layer = tf.keras.layers.Dense(num_outputs, kernel_initializer=k_init)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = inputs\n",
    "        outputs = self.conv_01(outputs)\n",
    "        outputs = self.conv_02(outputs)\n",
    "        outputs = self.conv_03(outputs)\n",
    "        outputs = self.flatten(outputs)\n",
    "        outputs = self.dense(outputs)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, actions, epsilon, input_shape, learning_rate=0.0001):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.input_shape = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self._teacher_model = None\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.build()\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        \n",
    "    def save(self, model_path):\n",
    "        self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        input_shape = (env.width, env.height, env.frame_count)\n",
    "        agent = cls(actions, epsilon, input_shape)\n",
    "        agent.initialize()\n",
    "        agent.model.load_weights(model_path)\n",
    "        return agent\n",
    "        \n",
    "    def build(self):\n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "        model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "        outputs = model(inputs)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # teacher_model を更新するため、両方のモデルで一度計算し重みを取得する\n",
    "        self._teacher_model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "        dummy = np.random.randn(1, *self.input_shape).astype(np.float32)\n",
    "        dummy = tf.convert_to_tensor(dummy)\n",
    "        _ = self.model.call(dummy)\n",
    "        _ = self._teacher_model.call(dummy)\n",
    "        self.update_teacher()\n",
    "    \n",
    "    def policy(self, state) -> int:\n",
    "        '''\n",
    "        epsilon greedy で受け取った state をもとに行動を決定する\n",
    "        '''\n",
    "        # epsilon greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(state)\n",
    "            return np.argmax(estimates)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        '''\n",
    "        ある state の状態価値を推定する\n",
    "        '''\n",
    "        state_as_batch = np.array([state])\n",
    "        return self.model.predict(state_as_batch)[0]\n",
    "    \n",
    "    def update(self, experiences, gamma):\n",
    "        '''\n",
    "        与えられた experiences をもとに学習\n",
    "        '''\n",
    "        states = np.array([e.state for e in experiences])\n",
    "        next_states = np.array([e.next_state for e in experiences])\n",
    "\n",
    "        estimated_values = self.model.predict(states)\n",
    "        next_state_values = self._teacher_model.predict(next_states)\n",
    "        \n",
    "        # train\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.reward\n",
    "            if not e.done:\n",
    "                reward += gamma * np.max(next_state_values[i])\n",
    "            estimated_values[i][e.action] = reward\n",
    "        loss = self.model.train_on_batch(states, estimated_values)\n",
    "        return loss\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def play(self, env, episode_count: int = 2, render: bool = True):\n",
    "        total_rewards = []\n",
    "        for e in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step_with_raw_reward(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                episode_reward += step_reward\n",
    "                state = next_state\n",
    "            print('episode {}, total reward: {:.4f}'.format(e, episode_reward))\n",
    "            total_rewards.append(episode_reward)\n",
    "                \n",
    "        env.reset()\n",
    "        print('reward by {}, mean: {:.4f}, std: {:.4f}'.format(episode_count, np.mean(total_rewards), np.std(total_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer:\n",
    "    \n",
    "    def __init__(self, env, frame_count, width, height, render=False, outdir='./playlogs/'):\n",
    "        self._env = env\n",
    "        if render:\n",
    "            outdir = outdir\n",
    "            env = Monitor(env, directory=outdir, video_callable=(lambda x: x % 5 == 0), force=True)\n",
    "            self._env = env\n",
    "        self.frame_count = frame_count\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "        \n",
    "    def render(self):\n",
    "        self._env.render(mode = 'rgb_array')\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), self.reward_shaping(reward), done\n",
    "    \n",
    "    def step_with_raw_reward(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), reward, done\n",
    "        \n",
    "    def transform(self, state):\n",
    "        state = state[:400, :, :]\n",
    "        grayed = Image.fromarray(state).convert('L')  # h, w, c -> h, w\n",
    "        \n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(np.float32)\n",
    "        resized = np.transpose(resized, (1, 0)) # h, w -> w, h\n",
    "        normalized = resized / 255.0\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        feature = np.transpose(feature, (1, 2, 0))  # [f, w, h] -> [w, h, f]\n",
    "        \n",
    "        return feature\n",
    "\n",
    "    def reward_shaping(self, reward):\n",
    "        if 0.01 > reward > -0.01:\n",
    "            return 0.01\n",
    "        elif reward <= -0.1:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, filename, buffer_size=50000, batch_size=32, gamma=0.98, report_interval=20):\n",
    "        self.file_name = filename\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        self.logger = Logger(dir_name=filename)\n",
    "        self._max_reward = 0\n",
    "        self.teacher_update_freq = 10000\n",
    "        \n",
    "    def train(self, env, episode_count=10000, initial_count=200, model_path = None):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if model_path:\n",
    "            agent = Agent.load(env, model_path, epsilon=0.1)\n",
    "        else:\n",
    "            agent = Agent(actions, 0.1, input_shape=(env.width, env.height, env.frame_count))\n",
    "        self.train_loop(env, agent, episode_count, initial_count)\n",
    "        \n",
    "    def train_loop(self, env, agent, episode_count, initial_count):\n",
    "        \n",
    "        for episode in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                e = Experience(state, action, step_reward, next_state, done)\n",
    "                self.experiences.append(e)\n",
    "                episode_reward += step_reward\n",
    "                loss = self.step(episode, agent)\n",
    "                state = next_state\n",
    "                \n",
    "                if not self.training and (len(self.experiences) >= self.buffer_size or episode >= initial_count):\n",
    "                    self.begin_training(agent)\n",
    "                    self.training = True\n",
    "            \n",
    "            self.end_episode(episode, episode_reward, loss, agent)\n",
    "    \n",
    "    def step(self, step, agent):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            loss = agent.update(batch, self.gamma)\n",
    "            self.training_count += 1\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "            return loss\n",
    "            \n",
    "    def begin_training(self, agent):\n",
    "        print('start training!')\n",
    "        self.logger.set_model(agent.model)\n",
    "        \n",
    "    def end_episode(self, episode, reward, loss, agent):\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.ckpt_path)\n",
    "                self._max_reward = reward\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)\n",
    "        \n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '01_dqn_05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128)\n",
    "trainer = Trainer(filename=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 20, reward is 0.202 (+/-0.048)\n",
      "At episode 40, reward is 0.198 (+/-0.043)\n",
      "At episode 60, reward is 0.241 (+/-0.099)\n",
      "At episode 80, reward is 0.201 (+/-0.037)\n",
      "At episode 100, reward is 0.197 (+/-0.034)\n",
      "At episode 120, reward is 0.19 (+/-0.03)\n",
      "At episode 140, reward is 0.189 (+/-0.027)\n",
      "At episode 160, reward is 0.216 (+/-0.072)\n",
      "At episode 180, reward is 0.192 (+/-0.029)\n",
      "start training!\n",
      "At episode 200, reward is 0.212 (+/-0.079)\n",
      "At episode 220, reward is 0.213 (+/-0.042)\n",
      "At episode 240, reward is 0.471 (+/-0.152)\n",
      "At episode 260, reward is 0.473 (+/-0.161)\n",
      "At episode 280, reward is 0.593 (+/-0.392)\n",
      "At episode 300, reward is 0.666 (+/-0.617)\n",
      "At episode 320, reward is 0.685 (+/-0.524)\n",
      "At episode 340, reward is 0.652 (+/-0.31)\n",
      "At episode 360, reward is 0.769 (+/-0.468)\n",
      "At episode 380, reward is 0.691 (+/-0.335)\n",
      "At episode 400, reward is 0.803 (+/-0.486)\n",
      "At episode 420, reward is 0.914 (+/-0.692)\n",
      "At episode 440, reward is 0.642 (+/-0.247)\n",
      "At episode 460, reward is 0.749 (+/-0.494)\n",
      "At episode 480, reward is 0.746 (+/-0.449)\n",
      "At episode 500, reward is 0.787 (+/-0.467)\n",
      "At episode 520, reward is 0.829 (+/-0.734)\n",
      "At episode 540, reward is 0.819 (+/-0.954)\n",
      "At episode 560, reward is 1.039 (+/-0.933)\n",
      "At episode 580, reward is 0.808 (+/-0.532)\n",
      "At episode 600, reward is 0.759 (+/-0.6)\n",
      "At episode 620, reward is 0.872 (+/-0.551)\n",
      "At episode 640, reward is 0.784 (+/-0.658)\n",
      "At episode 660, reward is 0.732 (+/-0.408)\n",
      "At episode 680, reward is 0.708 (+/-0.326)\n",
      "At episode 700, reward is 0.928 (+/-0.665)\n",
      "At episode 720, reward is 0.81 (+/-0.614)\n",
      "At episode 740, reward is 0.796 (+/-0.493)\n",
      "At episode 760, reward is 0.92 (+/-0.575)\n",
      "At episode 780, reward is 0.897 (+/-0.708)\n",
      "At episode 800, reward is 0.81 (+/-0.459)\n",
      "At episode 820, reward is 0.606 (+/-0.336)\n",
      "At episode 840, reward is 0.662 (+/-0.404)\n",
      "At episode 860, reward is 0.698 (+/-0.396)\n",
      "At episode 880, reward is 0.686 (+/-0.396)\n",
      "At episode 900, reward is 1.026 (+/-0.868)\n",
      "At episode 920, reward is 0.972 (+/-0.829)\n",
      "At episode 940, reward is 0.642 (+/-0.239)\n",
      "At episode 960, reward is 0.978 (+/-0.719)\n",
      "At episode 980, reward is 0.827 (+/-0.695)\n",
      "At episode 1000, reward is 0.976 (+/-0.706)\n",
      "At episode 1020, reward is 0.792 (+/-0.431)\n",
      "At episode 1040, reward is 1.186 (+/-0.835)\n",
      "At episode 1060, reward is 1.24 (+/-1.062)\n",
      "At episode 1080, reward is 0.948 (+/-0.822)\n",
      "At episode 1100, reward is 0.968 (+/-0.961)\n",
      "At episode 1120, reward is 0.965 (+/-0.543)\n",
      "At episode 1140, reward is 0.786 (+/-0.457)\n",
      "At episode 1160, reward is 1.204 (+/-1.331)\n",
      "At episode 1180, reward is 0.891 (+/-0.687)\n",
      "At episode 1200, reward is 0.66 (+/-0.307)\n",
      "At episode 1220, reward is 0.96 (+/-0.7)\n",
      "At episode 1240, reward is 0.656 (+/-0.307)\n",
      "At episode 1260, reward is 0.976 (+/-0.564)\n",
      "At episode 1280, reward is 1.041 (+/-0.702)\n",
      "At episode 1300, reward is 0.87 (+/-0.52)\n",
      "At episode 1320, reward is 1.076 (+/-0.892)\n",
      "At episode 1340, reward is 0.723 (+/-0.38)\n",
      "At episode 1360, reward is 0.796 (+/-0.652)\n",
      "At episode 1380, reward is 0.857 (+/-0.678)\n",
      "At episode 1400, reward is 0.647 (+/-0.261)\n",
      "At episode 1420, reward is 0.666 (+/-0.244)\n",
      "At episode 1440, reward is 0.786 (+/-0.576)\n",
      "At episode 1460, reward is 1.416 (+/-0.736)\n",
      "At episode 1480, reward is 1.363 (+/-1.114)\n",
      "At episode 1500, reward is 1.491 (+/-1.239)\n",
      "At episode 1520, reward is 0.973 (+/-0.678)\n",
      "At episode 1540, reward is 1.193 (+/-0.982)\n",
      "At episode 1560, reward is 1.398 (+/-1.507)\n",
      "At episode 1580, reward is 0.716 (+/-0.339)\n",
      "At episode 1600, reward is 1.014 (+/-0.9)\n",
      "At episode 1620, reward is 0.839 (+/-0.494)\n",
      "At episode 1640, reward is 0.83 (+/-0.438)\n",
      "At episode 1660, reward is 0.893 (+/-0.494)\n",
      "At episode 1680, reward is 1.218 (+/-1.029)\n",
      "At episode 1700, reward is 1.25 (+/-0.866)\n",
      "At episode 1720, reward is 0.99 (+/-0.613)\n",
      "At episode 1740, reward is 1.068 (+/-0.809)\n",
      "At episode 1760, reward is 0.866 (+/-0.636)\n",
      "At episode 1780, reward is 0.81 (+/-0.446)\n",
      "At episode 1800, reward is 1.033 (+/-0.817)\n",
      "At episode 1820, reward is 1.111 (+/-0.931)\n",
      "At episode 1840, reward is 1.2 (+/-0.866)\n",
      "At episode 1860, reward is 1.316 (+/-1.224)\n",
      "At episode 1880, reward is 1.42 (+/-1.375)\n",
      "At episode 1900, reward is 1.479 (+/-1.257)\n",
      "At episode 1920, reward is 1.26 (+/-1.045)\n",
      "At episode 1940, reward is 1.333 (+/-1.569)\n",
      "At episode 1960, reward is 1.228 (+/-0.951)\n",
      "At episode 1980, reward is 1.782 (+/-1.372)\n",
      "At episode 2000, reward is 1.752 (+/-1.289)\n",
      "At episode 2020, reward is 1.45 (+/-0.892)\n",
      "At episode 2040, reward is 2.172 (+/-2.014)\n",
      "At episode 2060, reward is 1.19 (+/-0.999)\n",
      "At episode 2080, reward is 1.35 (+/-1.378)\n",
      "At episode 2100, reward is 1.503 (+/-1.155)\n",
      "At episode 2120, reward is 1.371 (+/-1.432)\n",
      "At episode 2140, reward is 1.208 (+/-0.992)\n",
      "At episode 2160, reward is 2.234 (+/-2.344)\n",
      "At episode 2180, reward is 1.64 (+/-1.299)\n",
      "At episode 2200, reward is 1.585 (+/-1.594)\n",
      "At episode 2220, reward is 1.317 (+/-1.652)\n",
      "At episode 2240, reward is 2.697 (+/-2.967)\n",
      "At episode 2260, reward is 1.189 (+/-1.725)\n",
      "At episode 2280, reward is 1.525 (+/-1.871)\n",
      "At episode 2300, reward is 2.054 (+/-1.291)\n",
      "At episode 2320, reward is 1.382 (+/-1.205)\n",
      "At episode 2340, reward is 1.638 (+/-1.525)\n",
      "At episode 2360, reward is 2.72 (+/-2.889)\n",
      "At episode 2380, reward is 1.228 (+/-1.272)\n",
      "At episode 2400, reward is 1.66 (+/-1.892)\n",
      "At episode 2420, reward is 2.021 (+/-1.883)\n",
      "At episode 2440, reward is 1.546 (+/-1.155)\n",
      "At episode 2460, reward is 1.334 (+/-1.152)\n",
      "At episode 2480, reward is 1.483 (+/-1.827)\n",
      "At episode 2500, reward is 2.986 (+/-1.776)\n",
      "At episode 2520, reward is 2.293 (+/-1.867)\n",
      "At episode 2540, reward is 2.104 (+/-2.108)\n",
      "At episode 2560, reward is 2.069 (+/-1.791)\n",
      "At episode 2580, reward is 1.17 (+/-1.115)\n",
      "At episode 2600, reward is 1.879 (+/-1.823)\n",
      "At episode 2620, reward is 2.718 (+/-3.634)\n",
      "At episode 2640, reward is 1.411 (+/-1.01)\n",
      "At episode 2660, reward is 1.985 (+/-2.099)\n",
      "At episode 2680, reward is 2.505 (+/-2.153)\n",
      "At episode 2700, reward is 2.013 (+/-2.057)\n",
      "At episode 2720, reward is 1.772 (+/-1.409)\n",
      "At episode 2740, reward is 1.642 (+/-1.396)\n",
      "At episode 2760, reward is 3.023 (+/-4.092)\n",
      "At episode 2780, reward is 2.118 (+/-2.013)\n",
      "At episode 2800, reward is 2.518 (+/-2.703)\n",
      "At episode 2820, reward is 1.775 (+/-2.397)\n",
      "At episode 2840, reward is 1.77 (+/-1.64)\n",
      "At episode 2860, reward is 4.194 (+/-3.797)\n",
      "At episode 2880, reward is 2.12 (+/-2.502)\n",
      "At episode 2900, reward is 2.921 (+/-2.875)\n",
      "At episode 2920, reward is 2.486 (+/-2.86)\n",
      "At episode 2940, reward is 2.959 (+/-2.777)\n",
      "At episode 2960, reward is 2.245 (+/-2.381)\n",
      "At episode 2980, reward is 2.592 (+/-3.223)\n",
      "At episode 3000, reward is 2.304 (+/-2.509)\n",
      "At episode 3020, reward is 3.14 (+/-2.503)\n",
      "At episode 3040, reward is 2.845 (+/-3.718)\n",
      "At episode 3060, reward is 1.795 (+/-2.572)\n",
      "At episode 3080, reward is 3.695 (+/-3.654)\n",
      "At episode 3100, reward is 2.859 (+/-3.436)\n",
      "At episode 3120, reward is 2.495 (+/-2.907)\n",
      "At episode 3140, reward is 3.042 (+/-2.334)\n",
      "At episode 3160, reward is 3.15 (+/-3.58)\n",
      "At episode 3180, reward is 1.659 (+/-2.077)\n",
      "At episode 3200, reward is 3.392 (+/-2.904)\n",
      "At episode 3220, reward is 3.648 (+/-3.503)\n",
      "At episode 3240, reward is 3.12 (+/-4.078)\n",
      "At episode 3260, reward is 3.535 (+/-4.7)\n",
      "At episode 3280, reward is 2.992 (+/-2.584)\n",
      "At episode 3300, reward is 5.509 (+/-4.223)\n",
      "At episode 3320, reward is 6.363 (+/-6.803)\n",
      "At episode 3340, reward is 6.166 (+/-5.932)\n",
      "At episode 3360, reward is 5.173 (+/-4.075)\n",
      "At episode 3380, reward is 4.247 (+/-4.543)\n",
      "At episode 3400, reward is 3.221 (+/-2.874)\n",
      "At episode 3420, reward is 3.098 (+/-3.016)\n",
      "At episode 3440, reward is 3.34 (+/-3.443)\n",
      "At episode 3460, reward is 4.888 (+/-4.637)\n",
      "At episode 3480, reward is 6.16 (+/-4.912)\n",
      "At episode 3500, reward is 6.701 (+/-4.231)\n",
      "At episode 3520, reward is 5.825 (+/-5.803)\n",
      "At episode 3540, reward is 5.897 (+/-7.461)\n",
      "At episode 3560, reward is 4.248 (+/-4.754)\n",
      "At episode 3580, reward is 7.599 (+/-7.81)\n",
      "At episode 3600, reward is 5.419 (+/-4.998)\n",
      "At episode 3620, reward is 5.299 (+/-5.748)\n",
      "At episode 3640, reward is 5.848 (+/-8.338)\n",
      "At episode 3660, reward is 7.886 (+/-9.968)\n",
      "At episode 3680, reward is 6.3 (+/-7.886)\n",
      "At episode 3700, reward is 8.163 (+/-7.128)\n",
      "At episode 3720, reward is 8.812 (+/-10.083)\n",
      "At episode 3740, reward is 9.751 (+/-9.232)\n",
      "At episode 3760, reward is 11.254 (+/-13.812)\n",
      "At episode 3780, reward is 9.552 (+/-14.382)\n",
      "At episode 3800, reward is 6.2 (+/-10.328)\n",
      "At episode 3820, reward is 9.306 (+/-6.848)\n",
      "At episode 3840, reward is 11.678 (+/-9.235)\n",
      "At episode 3860, reward is 10.044 (+/-9.78)\n",
      "At episode 3880, reward is 7.897 (+/-6.145)\n",
      "At episode 3900, reward is 10.459 (+/-13.311)\n",
      "At episode 3920, reward is 7.277 (+/-9.825)\n",
      "At episode 3940, reward is 7.589 (+/-5.651)\n",
      "At episode 3960, reward is 11.053 (+/-8.143)\n",
      "At episode 3980, reward is 11.027 (+/-8.92)\n",
      "At episode 4000, reward is 12.277 (+/-10.113)\n",
      "At episode 4020, reward is 9.898 (+/-10.507)\n",
      "At episode 4040, reward is 15.468 (+/-14.416)\n",
      "At episode 4060, reward is 9.004 (+/-7.973)\n",
      "At episode 4080, reward is 12.114 (+/-12.759)\n",
      "At episode 4100, reward is 7.854 (+/-7.907)\n",
      "At episode 4120, reward is 9.778 (+/-7.474)\n",
      "At episode 4140, reward is 10.248 (+/-12.451)\n",
      "At episode 4160, reward is 8.154 (+/-7.418)\n",
      "At episode 4180, reward is 11.494 (+/-8.952)\n",
      "At episode 4200, reward is 13.484 (+/-10.903)\n",
      "At episode 4220, reward is 10.671 (+/-11.238)\n",
      "At episode 4240, reward is 15.498 (+/-12.583)\n",
      "At episode 4260, reward is 12.132 (+/-11.364)\n",
      "At episode 4280, reward is 12.958 (+/-10.524)\n",
      "At episode 4300, reward is 12.49 (+/-10.996)\n",
      "At episode 4320, reward is 14.664 (+/-10.589)\n",
      "At episode 4340, reward is 11.021 (+/-8.733)\n",
      "At episode 4360, reward is 10.522 (+/-8.349)\n",
      "At episode 4380, reward is 11.077 (+/-10.06)\n",
      "At episode 4400, reward is 10.214 (+/-14.299)\n",
      "At episode 4420, reward is 13.723 (+/-12.323)\n",
      "At episode 4440, reward is 16.788 (+/-15.567)\n",
      "At episode 4460, reward is 8.752 (+/-5.978)\n",
      "At episode 4480, reward is 8.419 (+/-10.504)\n",
      "At episode 4500, reward is 15.01 (+/-12.234)\n",
      "At episode 4520, reward is 12.843 (+/-11.975)\n",
      "At episode 4540, reward is 7.228 (+/-4.754)\n",
      "At episode 4560, reward is 13.319 (+/-13.0)\n",
      "At episode 4580, reward is 9.234 (+/-9.493)\n",
      "At episode 4600, reward is 13.174 (+/-12.177)\n",
      "At episode 4620, reward is 8.784 (+/-6.512)\n",
      "At episode 4640, reward is 11.035 (+/-11.584)\n",
      "At episode 4660, reward is 14.556 (+/-13.5)\n",
      "At episode 4680, reward is 15.993 (+/-16.232)\n",
      "At episode 4700, reward is 14.671 (+/-14.52)\n",
      "At episode 4720, reward is 12.984 (+/-10.781)\n",
      "At episode 4740, reward is 9.392 (+/-7.75)\n",
      "At episode 4760, reward is 13.748 (+/-11.938)\n",
      "At episode 4780, reward is 13.843 (+/-9.973)\n",
      "At episode 4800, reward is 8.938 (+/-8.255)\n",
      "At episode 4820, reward is 15.737 (+/-10.045)\n",
      "At episode 4840, reward is 11.054 (+/-7.985)\n",
      "At episode 4860, reward is 9.634 (+/-7.28)\n",
      "At episode 4880, reward is 11.203 (+/-11.455)\n",
      "At episode 4900, reward is 8.84 (+/-5.818)\n",
      "At episode 4920, reward is 13.978 (+/-11.713)\n",
      "At episode 4940, reward is 8.512 (+/-6.089)\n",
      "At episode 4960, reward is 9.752 (+/-9.384)\n",
      "At episode 4980, reward is 13.375 (+/-9.88)\n",
      "At episode 5000, reward is 13.56 (+/-14.359)\n",
      "At episode 5020, reward is 9.379 (+/-7.848)\n",
      "At episode 5040, reward is 9.957 (+/-6.808)\n",
      "At episode 5060, reward is 9.696 (+/-7.431)\n",
      "At episode 5080, reward is 12.704 (+/-10.117)\n",
      "At episode 5100, reward is 12.804 (+/-7.478)\n",
      "At episode 5120, reward is 8.412 (+/-9.575)\n",
      "At episode 5140, reward is 7.477 (+/-4.74)\n",
      "At episode 5160, reward is 10.972 (+/-9.627)\n",
      "At episode 5180, reward is 8.759 (+/-7.698)\n",
      "At episode 5200, reward is 9.749 (+/-6.642)\n",
      "At episode 5220, reward is 11.487 (+/-13.524)\n",
      "At episode 5240, reward is 11.697 (+/-13.274)\n",
      "At episode 5260, reward is 12.129 (+/-14.588)\n",
      "At episode 5280, reward is 20.1 (+/-16.88)\n",
      "At episode 5300, reward is 13.267 (+/-15.039)\n",
      "At episode 5320, reward is 9.207 (+/-7.406)\n",
      "At episode 5340, reward is 12.881 (+/-13.188)\n",
      "At episode 5360, reward is 6.451 (+/-5.645)\n",
      "At episode 5380, reward is 9.109 (+/-7.661)\n",
      "At episode 5400, reward is 10.355 (+/-11.454)\n",
      "At episode 5420, reward is 14.233 (+/-12.592)\n",
      "At episode 5440, reward is 11.412 (+/-13.187)\n",
      "At episode 5460, reward is 9.09 (+/-8.44)\n",
      "At episode 5480, reward is 17.816 (+/-18.506)\n",
      "At episode 5500, reward is 8.354 (+/-6.811)\n",
      "At episode 5520, reward is 6.505 (+/-5.653)\n",
      "At episode 5540, reward is 8.924 (+/-8.678)\n",
      "At episode 5560, reward is 8.956 (+/-7.115)\n",
      "At episode 5580, reward is 11.948 (+/-10.322)\n",
      "At episode 5600, reward is 10.39 (+/-9.374)\n",
      "At episode 5620, reward is 12.05 (+/-10.576)\n",
      "At episode 5640, reward is 10.209 (+/-8.416)\n",
      "At episode 5660, reward is 12.636 (+/-8.735)\n",
      "At episode 5680, reward is 13.872 (+/-14.471)\n",
      "At episode 5700, reward is 9.68 (+/-8.512)\n",
      "At episode 5720, reward is 12.053 (+/-10.553)\n",
      "At episode 5740, reward is 11.6 (+/-10.227)\n",
      "At episode 5760, reward is 10.535 (+/-10.019)\n",
      "At episode 5780, reward is 9.895 (+/-9.49)\n",
      "At episode 5800, reward is 13.725 (+/-12.983)\n",
      "At episode 5820, reward is 7.515 (+/-8.502)\n",
      "At episode 5840, reward is 10.311 (+/-7.608)\n",
      "At episode 5860, reward is 9.588 (+/-10.6)\n",
      "At episode 5880, reward is 8.732 (+/-7.44)\n",
      "At episode 5900, reward is 7.759 (+/-7.099)\n",
      "At episode 5920, reward is 10.924 (+/-9.461)\n",
      "At episode 5940, reward is 11.159 (+/-8.602)\n",
      "At episode 5960, reward is 11.97 (+/-8.707)\n",
      "At episode 5980, reward is 12.476 (+/-7.348)\n",
      "At episode 6000, reward is 14.474 (+/-15.522)\n",
      "At episode 6020, reward is 10.406 (+/-8.964)\n",
      "At episode 6040, reward is 9.451 (+/-7.462)\n",
      "At episode 6060, reward is 14.469 (+/-12.325)\n",
      "At episode 6080, reward is 9.233 (+/-11.176)\n",
      "At episode 6100, reward is 12.079 (+/-10.354)\n",
      "At episode 6120, reward is 10.88 (+/-9.581)\n",
      "At episode 6140, reward is 10.162 (+/-7.593)\n",
      "At episode 6160, reward is 14.229 (+/-10.971)\n",
      "At episode 6180, reward is 15.192 (+/-14.21)\n",
      "At episode 6200, reward is 7.174 (+/-7.349)\n",
      "At episode 6220, reward is 9.897 (+/-10.761)\n",
      "At episode 6240, reward is 8.731 (+/-7.334)\n",
      "At episode 6260, reward is 10.473 (+/-11.846)\n",
      "At episode 6280, reward is 10.458 (+/-10.285)\n",
      "At episode 6300, reward is 15.607 (+/-14.263)\n",
      "At episode 6320, reward is 10.66 (+/-10.245)\n",
      "At episode 6340, reward is 15.492 (+/-13.58)\n",
      "At episode 6360, reward is 10.739 (+/-7.107)\n",
      "At episode 6380, reward is 10.527 (+/-13.749)\n",
      "At episode 6400, reward is 9.403 (+/-7.35)\n",
      "At episode 6420, reward is 6.831 (+/-5.964)\n",
      "At episode 6440, reward is 9.639 (+/-9.864)\n",
      "At episode 6460, reward is 11.584 (+/-12.546)\n",
      "At episode 6480, reward is 10.172 (+/-7.447)\n",
      "At episode 6500, reward is 9.669 (+/-8.212)\n",
      "At episode 6520, reward is 13.919 (+/-10.533)\n",
      "At episode 6540, reward is 12.248 (+/-6.468)\n",
      "At episode 6560, reward is 12.562 (+/-14.814)\n",
      "At episode 6580, reward is 7.984 (+/-4.98)\n",
      "At episode 6600, reward is 12.431 (+/-15.204)\n",
      "At episode 6620, reward is 8.097 (+/-6.285)\n",
      "At episode 6640, reward is 12.916 (+/-12.292)\n",
      "At episode 6660, reward is 7.175 (+/-7.833)\n",
      "At episode 6680, reward is 8.086 (+/-6.782)\n",
      "At episode 6700, reward is 9.553 (+/-10.263)\n",
      "At episode 6720, reward is 8.759 (+/-8.303)\n",
      "At episode 6740, reward is 10.406 (+/-16.161)\n",
      "At episode 6760, reward is 7.34 (+/-8.058)\n",
      "At episode 6780, reward is 12.007 (+/-12.859)\n",
      "At episode 6800, reward is 10.656 (+/-7.69)\n",
      "At episode 6820, reward is 10.697 (+/-12.776)\n",
      "At episode 6840, reward is 12.244 (+/-10.923)\n",
      "At episode 6860, reward is 9.171 (+/-6.356)\n",
      "At episode 6880, reward is 11.187 (+/-10.927)\n",
      "At episode 6900, reward is 7.71 (+/-5.954)\n",
      "At episode 6920, reward is 12.193 (+/-11.694)\n",
      "At episode 6940, reward is 10.536 (+/-10.387)\n",
      "At episode 6960, reward is 5.969 (+/-4.718)\n",
      "At episode 6980, reward is 12.035 (+/-8.564)\n",
      "At episode 7000, reward is 13.66 (+/-12.366)\n",
      "At episode 7020, reward is 10.532 (+/-8.171)\n",
      "At episode 7040, reward is 10.167 (+/-7.524)\n",
      "At episode 7060, reward is 10.01 (+/-7.132)\n",
      "At episode 7080, reward is 9.007 (+/-6.788)\n",
      "At episode 7100, reward is 6.795 (+/-6.354)\n",
      "At episode 7120, reward is 10.414 (+/-8.013)\n",
      "At episode 7140, reward is 7.553 (+/-7.022)\n",
      "At episode 7160, reward is 7.758 (+/-5.854)\n",
      "At episode 7180, reward is 9.958 (+/-6.724)\n",
      "At episode 7200, reward is 8.671 (+/-9.628)\n",
      "At episode 7220, reward is 10.326 (+/-8.72)\n",
      "At episode 7240, reward is 12.155 (+/-9.29)\n",
      "At episode 7260, reward is 13.927 (+/-13.225)\n",
      "At episode 7280, reward is 10.293 (+/-8.036)\n",
      "At episode 7300, reward is 10.321 (+/-12.453)\n",
      "At episode 7320, reward is 11.556 (+/-8.569)\n",
      "At episode 7340, reward is 9.464 (+/-9.667)\n",
      "At episode 7360, reward is 7.634 (+/-10.718)\n",
      "At episode 7380, reward is 6.764 (+/-5.58)\n",
      "At episode 7400, reward is 13.301 (+/-11.267)\n",
      "At episode 7420, reward is 10.331 (+/-7.567)\n",
      "At episode 7440, reward is 8.837 (+/-8.904)\n",
      "At episode 7460, reward is 6.621 (+/-4.684)\n",
      "At episode 7480, reward is 12.751 (+/-10.727)\n",
      "At episode 7500, reward is 9.294 (+/-8.016)\n",
      "At episode 7520, reward is 9.808 (+/-10.181)\n",
      "At episode 7540, reward is 10.962 (+/-7.898)\n",
      "At episode 7560, reward is 8.481 (+/-9.228)\n",
      "At episode 7580, reward is 11.057 (+/-9.589)\n",
      "At episode 7600, reward is 10.767 (+/-10.003)\n",
      "At episode 7620, reward is 6.31 (+/-4.959)\n",
      "At episode 7640, reward is 4.485 (+/-3.091)\n",
      "At episode 7660, reward is 13.695 (+/-12.04)\n",
      "At episode 7680, reward is 8.19 (+/-5.084)\n",
      "At episode 7700, reward is 10.676 (+/-8.232)\n",
      "At episode 7720, reward is 8.777 (+/-8.098)\n",
      "At episode 7740, reward is 8.207 (+/-8.696)\n",
      "At episode 7760, reward is 6.824 (+/-6.476)\n",
      "At episode 7780, reward is 11.866 (+/-8.182)\n",
      "At episode 7800, reward is 11.834 (+/-8.605)\n",
      "At episode 7820, reward is 7.426 (+/-6.331)\n",
      "At episode 7840, reward is 11.042 (+/-13.485)\n",
      "At episode 7860, reward is 8.902 (+/-6.501)\n",
      "At episode 7880, reward is 9.562 (+/-7.607)\n",
      "At episode 7900, reward is 12.165 (+/-9.618)\n",
      "At episode 7920, reward is 12.889 (+/-11.486)\n",
      "At episode 7940, reward is 10.46 (+/-9.141)\n",
      "At episode 7960, reward is 9.622 (+/-8.578)\n",
      "At episode 7980, reward is 14.535 (+/-18.896)\n",
      "At episode 8000, reward is 8.932 (+/-8.18)\n",
      "At episode 8020, reward is 8.061 (+/-8.314)\n",
      "At episode 8040, reward is 10.88 (+/-7.669)\n",
      "At episode 8060, reward is 8.864 (+/-5.421)\n",
      "At episode 8080, reward is 4.55 (+/-3.662)\n",
      "At episode 8100, reward is 10.206 (+/-6.886)\n",
      "At episode 8120, reward is 7.983 (+/-8.363)\n",
      "At episode 8140, reward is 8.736 (+/-7.067)\n",
      "At episode 8160, reward is 11.648 (+/-7.34)\n",
      "At episode 8180, reward is 10.928 (+/-11.434)\n",
      "At episode 8200, reward is 11.883 (+/-12.162)\n",
      "At episode 8220, reward is 7.314 (+/-5.437)\n",
      "At episode 8240, reward is 12.157 (+/-7.228)\n",
      "At episode 8260, reward is 8.387 (+/-6.514)\n",
      "At episode 8280, reward is 9.637 (+/-9.113)\n",
      "At episode 8300, reward is 6.394 (+/-5.627)\n",
      "At episode 8320, reward is 10.548 (+/-8.92)\n",
      "At episode 8340, reward is 14.775 (+/-13.887)\n",
      "At episode 8360, reward is 10.168 (+/-6.906)\n",
      "At episode 8380, reward is 8.371 (+/-9.262)\n",
      "At episode 8400, reward is 12.703 (+/-13.739)\n",
      "At episode 8420, reward is 6.194 (+/-6.114)\n",
      "At episode 8440, reward is 12.163 (+/-7.303)\n",
      "At episode 8460, reward is 14.618 (+/-14.642)\n",
      "At episode 8480, reward is 10.922 (+/-10.541)\n",
      "At episode 8500, reward is 5.914 (+/-5.81)\n",
      "At episode 8520, reward is 7.284 (+/-5.384)\n",
      "At episode 8540, reward is 13.291 (+/-12.43)\n",
      "At episode 8560, reward is 9.931 (+/-10.24)\n",
      "At episode 8580, reward is 11.877 (+/-10.257)\n",
      "At episode 8600, reward is 11.767 (+/-14.188)\n",
      "At episode 8620, reward is 11.191 (+/-15.061)\n",
      "At episode 8640, reward is 10.149 (+/-12.895)\n",
      "At episode 8660, reward is 13.924 (+/-14.647)\n",
      "At episode 8680, reward is 8.392 (+/-7.695)\n",
      "At episode 8700, reward is 8.999 (+/-10.548)\n",
      "At episode 8720, reward is 8.273 (+/-6.465)\n",
      "At episode 8740, reward is 7.387 (+/-6.758)\n",
      "At episode 8760, reward is 9.759 (+/-9.969)\n",
      "At episode 8780, reward is 9.89 (+/-8.906)\n",
      "At episode 8800, reward is 8.585 (+/-6.504)\n",
      "At episode 8820, reward is 7.961 (+/-6.686)\n",
      "At episode 8840, reward is 12.026 (+/-7.754)\n",
      "At episode 8860, reward is 5.021 (+/-3.793)\n",
      "At episode 8880, reward is 9.415 (+/-6.533)\n",
      "At episode 8900, reward is 8.53 (+/-10.234)\n",
      "At episode 8920, reward is 15.282 (+/-13.427)\n",
      "At episode 8940, reward is 12.865 (+/-11.615)\n",
      "At episode 8960, reward is 10.081 (+/-11.947)\n",
      "At episode 8980, reward is 11.603 (+/-8.199)\n",
      "At episode 9000, reward is 6.584 (+/-6.548)\n",
      "At episode 9020, reward is 11.593 (+/-11.88)\n",
      "At episode 9040, reward is 12.614 (+/-11.096)\n",
      "At episode 9060, reward is 11.691 (+/-10.524)\n",
      "At episode 9080, reward is 10.682 (+/-7.27)\n",
      "At episode 9100, reward is 8.701 (+/-7.68)\n",
      "At episode 9120, reward is 9.181 (+/-8.773)\n",
      "At episode 9140, reward is 10.318 (+/-10.79)\n",
      "At episode 9160, reward is 7.785 (+/-8.152)\n",
      "At episode 9180, reward is 7.714 (+/-6.777)\n",
      "At episode 9200, reward is 8.862 (+/-6.148)\n",
      "At episode 9220, reward is 10.19 (+/-12.191)\n",
      "At episode 9240, reward is 9.36 (+/-6.578)\n",
      "At episode 9260, reward is 8.151 (+/-6.948)\n",
      "At episode 9280, reward is 8.702 (+/-7.51)\n",
      "At episode 9300, reward is 11.089 (+/-10.272)\n",
      "At episode 9320, reward is 8.7 (+/-8.051)\n",
      "At episode 9340, reward is 11.184 (+/-13.862)\n",
      "At episode 9360, reward is 11.299 (+/-10.984)\n",
      "At episode 9380, reward is 11.715 (+/-8.949)\n",
      "At episode 9400, reward is 6.451 (+/-5.661)\n",
      "At episode 9420, reward is 9.137 (+/-8.715)\n",
      "At episode 9440, reward is 8.074 (+/-5.261)\n",
      "At episode 9460, reward is 8.197 (+/-7.124)\n",
      "At episode 9480, reward is 7.359 (+/-6.087)\n",
      "At episode 9500, reward is 9.333 (+/-9.122)\n",
      "At episode 9520, reward is 7.508 (+/-4.39)\n",
      "At episode 9540, reward is 5.962 (+/-7.886)\n",
      "At episode 9560, reward is 5.888 (+/-5.147)\n",
      "At episode 9580, reward is 13.069 (+/-14.12)\n",
      "At episode 9600, reward is 14.769 (+/-10.04)\n",
      "At episode 9620, reward is 9.02 (+/-7.06)\n",
      "At episode 9640, reward is 6.685 (+/-4.927)\n",
      "At episode 9660, reward is 12.144 (+/-19.843)\n",
      "At episode 9680, reward is 10.33 (+/-6.692)\n",
      "At episode 9700, reward is 7.279 (+/-5.744)\n",
      "At episode 9720, reward is 10.429 (+/-11.887)\n",
      "At episode 9740, reward is 10.157 (+/-7.829)\n",
      "At episode 9760, reward is 12.018 (+/-9.451)\n",
      "At episode 9780, reward is 9.522 (+/-7.499)\n",
      "At episode 9800, reward is 6.203 (+/-5.761)\n",
      "At episode 9820, reward is 8.795 (+/-5.926)\n",
      "At episode 9840, reward is 9.802 (+/-9.884)\n",
      "At episode 9860, reward is 8.75 (+/-7.835)\n",
      "At episode 9880, reward is 8.791 (+/-7.885)\n",
      "At episode 9900, reward is 7.316 (+/-9.236)\n",
      "At episode 9920, reward is 8.833 (+/-10.144)\n",
      "At episode 9940, reward is 8.691 (+/-9.563)\n",
      "At episode 9960, reward is 13.016 (+/-15.425)\n",
      "At episode 9980, reward is 12.65 (+/-12.207)\n"
     ]
    }
   ],
   "source": [
    "trainer.train(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/flappybird-try-wqQoCzq9/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 61.0000\n",
      "episode 1, total reward: 62.0000\n",
      "episode 2, total reward: 103.0000\n",
      "episode 3, total reward: 264.0000\n",
      "episode 4, total reward: 8.0000\n",
      "episode 5, total reward: 34.0000\n",
      "episode 6, total reward: 22.0000\n",
      "episode 7, total reward: 10.0000\n",
      "episode 8, total reward: 12.0000\n",
      "episode 9, total reward: 49.0000\n",
      "episode 10, total reward: 22.0000\n",
      "episode 11, total reward: 76.0000\n",
      "episode 12, total reward: 208.0000\n",
      "episode 13, total reward: 19.0000\n",
      "episode 14, total reward: 130.0000\n",
      "episode 15, total reward: 68.0000\n",
      "episode 16, total reward: 23.0000\n",
      "episode 17, total reward: 164.0000\n",
      "episode 18, total reward: 55.0000\n",
      "episode 19, total reward: 21.0000\n",
      "episode 20, total reward: 57.0000\n",
      "episode 21, total reward: 28.0000\n",
      "episode 22, total reward: 61.0000\n",
      "episode 23, total reward: 28.0000\n",
      "episode 24, total reward: 26.0000\n",
      "episode 25, total reward: 61.0000\n",
      "episode 26, total reward: 1.0000\n",
      "episode 27, total reward: 24.0000\n",
      "episode 28, total reward: 155.0000\n",
      "episode 29, total reward: 88.0000\n",
      "episode 30, total reward: 94.0000\n",
      "episode 31, total reward: 66.0000\n",
      "episode 32, total reward: 48.0000\n",
      "episode 33, total reward: 82.0000\n",
      "episode 34, total reward: 106.0000\n",
      "episode 35, total reward: 153.0000\n",
      "episode 36, total reward: 1.0000\n",
      "episode 37, total reward: 1.0000\n",
      "episode 38, total reward: 37.0000\n",
      "episode 39, total reward: 29.0000\n",
      "episode 40, total reward: 58.0000\n",
      "episode 41, total reward: 13.0000\n",
      "episode 42, total reward: 11.0000\n",
      "episode 43, total reward: 31.0000\n",
      "episode 44, total reward: 80.0000\n",
      "episode 45, total reward: 72.0000\n",
      "episode 46, total reward: 41.0000\n",
      "episode 47, total reward: 4.0000\n",
      "episode 48, total reward: 146.0000\n",
      "episode 49, total reward: 7.0000\n",
      "reward by 50, mean: 60.4000, std: 55.8086\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "model_name = '01_dqn_01'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 44.0000\n",
      "episode 1, total reward: 28.0000\n",
      "episode 2, total reward: 7.0000\n",
      "episode 3, total reward: 50.0000\n",
      "episode 4, total reward: 46.0000\n",
      "episode 5, total reward: 37.0000\n",
      "episode 6, total reward: 7.0000\n",
      "episode 7, total reward: 4.0000\n",
      "episode 8, total reward: 31.0000\n",
      "episode 9, total reward: 29.0000\n",
      "episode 10, total reward: 12.0000\n",
      "episode 11, total reward: 3.0000\n",
      "episode 12, total reward: 2.0000\n",
      "episode 13, total reward: 59.0000\n",
      "episode 14, total reward: 13.0000\n",
      "episode 15, total reward: 19.0000\n",
      "episode 16, total reward: 15.0000\n",
      "episode 17, total reward: 37.0000\n",
      "episode 18, total reward: 46.0000\n",
      "episode 19, total reward: 220.0000\n",
      "episode 20, total reward: 46.0000\n",
      "episode 21, total reward: 22.0000\n",
      "episode 22, total reward: 232.0000\n",
      "episode 23, total reward: 44.0000\n",
      "episode 24, total reward: 16.0000\n",
      "episode 25, total reward: 2.0000\n",
      "episode 26, total reward: 209.0000\n",
      "episode 27, total reward: 205.0000\n",
      "episode 28, total reward: 17.0000\n",
      "episode 29, total reward: 25.0000\n",
      "episode 30, total reward: 55.0000\n",
      "episode 31, total reward: 25.0000\n",
      "episode 32, total reward: 18.0000\n",
      "episode 33, total reward: 16.0000\n",
      "episode 34, total reward: 21.0000\n",
      "episode 35, total reward: 81.0000\n",
      "episode 36, total reward: 96.0000\n",
      "episode 37, total reward: 55.0000\n",
      "episode 38, total reward: 5.0000\n",
      "episode 39, total reward: 62.0000\n",
      "episode 40, total reward: 1.0000\n",
      "episode 41, total reward: 33.0000\n",
      "episode 42, total reward: 13.0000\n",
      "episode 43, total reward: 17.0000\n",
      "episode 44, total reward: 7.0000\n",
      "episode 45, total reward: 20.0000\n",
      "episode 46, total reward: 68.0000\n",
      "episode 47, total reward: 35.0000\n",
      "episode 48, total reward: 58.0000\n",
      "episode 49, total reward: 38.0000\n",
      "reward by 50, mean: 45.0200, std: 54.9172\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "model_name = '01_dqn_02'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 91.0000\n",
      "episode 1, total reward: 14.0000\n",
      "episode 2, total reward: 3.0000\n",
      "episode 3, total reward: 21.0000\n",
      "episode 4, total reward: 53.0000\n",
      "episode 5, total reward: 42.0000\n",
      "episode 6, total reward: 132.0000\n",
      "episode 7, total reward: 2.0000\n",
      "episode 8, total reward: 35.0000\n",
      "episode 9, total reward: 55.0000\n",
      "episode 10, total reward: 61.0000\n",
      "episode 11, total reward: 52.0000\n",
      "episode 12, total reward: 64.0000\n",
      "episode 13, total reward: 9.0000\n",
      "episode 14, total reward: 8.0000\n",
      "episode 15, total reward: 32.0000\n",
      "episode 16, total reward: 26.0000\n",
      "episode 17, total reward: 47.0000\n",
      "episode 18, total reward: 70.0000\n",
      "episode 19, total reward: 27.0000\n",
      "episode 20, total reward: 16.0000\n",
      "episode 21, total reward: 109.0000\n",
      "episode 22, total reward: 47.0000\n",
      "episode 23, total reward: 86.0000\n",
      "episode 24, total reward: 14.0000\n",
      "episode 25, total reward: 13.0000\n",
      "episode 26, total reward: 59.0000\n",
      "episode 27, total reward: 12.0000\n",
      "episode 28, total reward: 28.0000\n",
      "episode 29, total reward: 6.0000\n",
      "episode 30, total reward: 2.0000\n",
      "episode 31, total reward: 35.0000\n",
      "episode 32, total reward: 3.0000\n",
      "episode 33, total reward: 10.0000\n",
      "episode 34, total reward: 116.0000\n",
      "episode 35, total reward: 167.0000\n",
      "episode 36, total reward: 1.0000\n",
      "episode 37, total reward: 115.0000\n",
      "episode 38, total reward: 55.0000\n",
      "episode 39, total reward: 22.0000\n",
      "episode 40, total reward: 65.0000\n",
      "episode 41, total reward: 106.0000\n",
      "episode 42, total reward: 62.0000\n",
      "episode 43, total reward: 37.0000\n",
      "episode 44, total reward: 264.0000\n",
      "episode 45, total reward: 16.0000\n",
      "episode 46, total reward: 7.0000\n",
      "episode 47, total reward: 19.0000\n",
      "episode 48, total reward: 2.0000\n",
      "episode 49, total reward: 18.0000\n",
      "reward by 50, mean: 47.1200, std: 49.3358\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "model_name = '01_dqn_03'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 7.0000\n",
      "episode 1, total reward: 2.0000\n",
      "episode 2, total reward: 15.0000\n",
      "episode 3, total reward: 101.0000\n",
      "episode 4, total reward: 3.0000\n",
      "episode 5, total reward: 19.0000\n",
      "episode 6, total reward: 33.0000\n",
      "episode 7, total reward: 9.0000\n",
      "episode 8, total reward: 60.0000\n",
      "episode 9, total reward: 11.0000\n",
      "episode 10, total reward: 126.0000\n",
      "episode 11, total reward: 13.0000\n",
      "episode 12, total reward: 25.0000\n",
      "episode 13, total reward: 16.0000\n",
      "episode 14, total reward: 5.0000\n",
      "episode 15, total reward: 1.0000\n",
      "episode 16, total reward: 9.0000\n",
      "episode 17, total reward: 25.0000\n",
      "episode 18, total reward: 49.0000\n",
      "episode 19, total reward: 62.0000\n",
      "episode 20, total reward: 35.0000\n",
      "episode 21, total reward: 148.0000\n",
      "episode 22, total reward: 13.0000\n",
      "episode 23, total reward: 44.0000\n",
      "episode 24, total reward: 4.0000\n",
      "episode 25, total reward: 4.0000\n",
      "episode 26, total reward: 27.0000\n",
      "episode 27, total reward: 8.0000\n",
      "episode 28, total reward: 242.0000\n",
      "episode 29, total reward: 40.0000\n",
      "episode 30, total reward: 19.0000\n",
      "episode 31, total reward: 10.0000\n",
      "episode 32, total reward: 264.0000\n",
      "episode 33, total reward: 155.0000\n",
      "episode 34, total reward: 38.0000\n",
      "episode 35, total reward: 41.0000\n",
      "episode 36, total reward: 18.0000\n",
      "episode 37, total reward: 45.0000\n",
      "episode 38, total reward: 17.0000\n",
      "episode 39, total reward: 55.0000\n",
      "episode 40, total reward: 124.0000\n",
      "episode 41, total reward: 26.0000\n",
      "episode 42, total reward: 187.0000\n",
      "episode 43, total reward: 169.0000\n",
      "episode 44, total reward: 91.0000\n",
      "episode 45, total reward: 95.0000\n",
      "episode 46, total reward: 1.0000\n",
      "episode 47, total reward: 1.0000\n",
      "episode 48, total reward: 77.0000\n",
      "episode 49, total reward: 154.0000\n",
      "reward by 50, mean: 54.8600, std: 64.0141\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "model_name = '01_dqn_04'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 32.0000\n",
      "episode 1, total reward: 76.0000\n",
      "episode 2, total reward: 25.0000\n",
      "episode 3, total reward: 264.0000\n",
      "episode 4, total reward: 34.0000\n",
      "episode 5, total reward: 34.0000\n",
      "episode 6, total reward: 2.0000\n",
      "episode 7, total reward: 25.0000\n",
      "episode 8, total reward: 86.0000\n",
      "episode 9, total reward: 125.0000\n",
      "episode 10, total reward: 5.0000\n",
      "episode 11, total reward: 98.0000\n",
      "episode 12, total reward: 74.0000\n",
      "episode 13, total reward: 52.0000\n",
      "episode 14, total reward: 8.0000\n",
      "episode 15, total reward: 44.0000\n",
      "episode 16, total reward: 4.0000\n",
      "episode 17, total reward: 10.0000\n",
      "episode 18, total reward: 79.0000\n",
      "episode 19, total reward: 5.0000\n",
      "episode 20, total reward: 27.0000\n",
      "episode 21, total reward: 4.0000\n",
      "episode 22, total reward: 113.0000\n",
      "episode 23, total reward: 38.0000\n",
      "episode 24, total reward: 101.0000\n",
      "episode 25, total reward: 1.0000\n",
      "episode 26, total reward: 14.0000\n",
      "episode 27, total reward: 47.0000\n",
      "episode 28, total reward: 7.0000\n",
      "episode 29, total reward: 101.0000\n",
      "episode 30, total reward: 3.0000\n",
      "episode 31, total reward: 63.0000\n",
      "episode 32, total reward: 18.0000\n",
      "episode 33, total reward: 67.0000\n",
      "episode 34, total reward: 13.0000\n",
      "episode 35, total reward: 71.0000\n",
      "episode 36, total reward: 2.0000\n",
      "episode 37, total reward: 61.0000\n",
      "episode 38, total reward: 12.0000\n",
      "episode 39, total reward: 24.0000\n",
      "episode 40, total reward: 50.0000\n",
      "episode 41, total reward: 164.0000\n",
      "episode 42, total reward: 140.0000\n",
      "episode 43, total reward: 7.0000\n",
      "episode 44, total reward: 58.0000\n",
      "episode 45, total reward: 4.0000\n",
      "episode 46, total reward: 142.0000\n",
      "episode 47, total reward: 51.0000\n",
      "episode 48, total reward: 4.0000\n",
      "episode 49, total reward: 73.0000\n",
      "reward by 50, mean: 51.2400, std: 51.9217\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "model_name = '01_dqn_05'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 51.728, median: 51.240\n"
     ]
    }
   ],
   "source": [
    "res = np.array([60.4000, 45.0200, 47.1200, 54.8600, 51.2400])\n",
    "print('mean: {:.3f}, median: {:.3f}'.format(res.mean(), np.median(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
