{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import logging\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "\n",
    "# gym\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import gym_ple\n",
    "\n",
    "# disply\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select gpu\n",
    "gpu_index = 1\n",
    "from tensorflow.keras.backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=str(gpu_index),\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    state: gym.spaces.Box\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: gym.spaces.Box\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir='./logs/', dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.keras.callbacks.TensorBoard(self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "    \n",
    "    @property\n",
    "    def ckpt_path(self):\n",
    "        return os.path.join(self.log_dir, 'model.ckpt')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, is_training: bool, num_outputs: int):\n",
    "        super(AgentModel, self).__init__()\n",
    "        self.is_training = is_training\n",
    "        self.num_outputs = num_outputs\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        relu = tf.nn.relu\n",
    "        self.conv_01 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, padding='same', kernel_initializer=k_init, activation=relu) \n",
    "        self.conv_02 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=4, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.conv_03 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(256, kernel_initializer=k_init, activation=relu)\n",
    "        self.output_layer = tf.keras.layers.Dense(num_outputs, kernel_initializer=k_init)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = inputs\n",
    "        outputs = self.conv_01(outputs)\n",
    "        outputs = self.conv_02(outputs)\n",
    "        outputs = self.conv_03(outputs)\n",
    "        outputs = self.flatten(outputs)\n",
    "        outputs = self.dense(outputs)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, actions, epsilon, input_shape, learning_rate=0.0001):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.input_shape = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self._teacher_model = None\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.build()\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        \n",
    "    def save(self, model_path):\n",
    "        self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        input_shape = (env.width, env.height, env.frame_count)\n",
    "        agent = cls(actions, epsilon, input_shape)\n",
    "        agent.initialize()\n",
    "        agent.model.load_weights(model_path)\n",
    "        return agent\n",
    "        \n",
    "    def build(self):\n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "        model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "        outputs = model(inputs)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # teacher_model を更新するため、両方のモデルで一度計算し重みを取得する\n",
    "        self._teacher_model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "        dummy = np.random.randn(1, *self.input_shape).astype(np.float32)\n",
    "        dummy = tf.convert_to_tensor(dummy)\n",
    "        _ = self.model.call(dummy)\n",
    "        _ = self._teacher_model.call(dummy)\n",
    "        self.update_teacher()\n",
    "    \n",
    "    def policy(self, state) -> int:\n",
    "        '''\n",
    "        epsilon greedy で受け取った state をもとに行動を決定する\n",
    "        '''\n",
    "        # epsilon greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(state)\n",
    "            return np.argmax(estimates)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        '''\n",
    "        ある state の状態価値を推定する\n",
    "        '''\n",
    "        state_as_batch = np.array([state])\n",
    "        return self.model.predict(state_as_batch)[0]\n",
    "    \n",
    "    def update(self, experiences, gamma):\n",
    "        '''\n",
    "        与えられた experiences をもとに学習\n",
    "        '''\n",
    "        states = np.array([e.state for e in experiences])\n",
    "        next_states = np.array([e.next_state for e in experiences])\n",
    "\n",
    "        estimated_values = self.model.predict(states)\n",
    "        next_state_values = self._teacher_model.predict(next_states)\n",
    "        \n",
    "        # train\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.reward\n",
    "            if not e.done:\n",
    "                # DQN と違い、action の推定は学習中のモデルで、その価値の推定は target network で行う\n",
    "                next_action = np.argmax(estimated_values[i])\n",
    "                reward += gamma * next_state_values[i][next_action]\n",
    "            estimated_values[i][e.action] = reward\n",
    "        loss = self.model.train_on_batch(states, estimated_values)\n",
    "        return loss\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def play(self, env, episode_count: int = 2, render: bool = True):\n",
    "        total_rewards = []\n",
    "        for e in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step_with_raw_reward(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                episode_reward += step_reward\n",
    "                state = next_state\n",
    "            print('episode {}, total reward: {:.4f}'.format(e, episode_reward))\n",
    "            total_rewards.append(episode_reward)\n",
    "                \n",
    "        env.reset()\n",
    "        print('reward by {}, mean: {:.4f}, std: {:.4f}'.format(episode_count, np.mean(total_rewards), np.std(total_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer:\n",
    "    \n",
    "    def __init__(self, env, frame_count, width, height, render=False, outdir='./playlogs/'):\n",
    "        self._env = env\n",
    "        if render:\n",
    "            outdir = outdir\n",
    "            env = Monitor(env, directory=outdir, video_callable=(lambda x: x % 5 == 0), force=True)\n",
    "            self._env = env\n",
    "        self.frame_count = frame_count\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "        \n",
    "    def render(self):\n",
    "        self._env.render(mode = 'rgb_array')\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), self.reward_shaping(reward), done\n",
    "    \n",
    "    def step_with_raw_reward(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), reward, done\n",
    "        \n",
    "    def transform(self, state):\n",
    "        state = state[:400, :, :]\n",
    "        grayed = Image.fromarray(state).convert('L')  # h, w, c -> h, w\n",
    "        \n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(np.float32)\n",
    "        resized = np.transpose(resized, (1, 0)) # h, w -> w, h\n",
    "        normalized = resized / 255.0\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        feature = np.transpose(feature, (1, 2, 0))  # [f, w, h] -> [w, h, f]\n",
    "        \n",
    "        return feature\n",
    "\n",
    "    def reward_shaping(self, reward):\n",
    "        if 0.01 > reward > -0.01:\n",
    "            return 0.01\n",
    "        elif reward <= -0.1:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, filename, buffer_size=50000, batch_size=32, gamma=0.98, report_interval=20):\n",
    "        self.file_name = filename\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        self.logger = Logger(dir_name=filename)\n",
    "        self._max_reward = 0\n",
    "        self.teacher_update_freq = 10000\n",
    "        \n",
    "    def train(self, env, episode_count=10000, initial_count=200, model_path = None):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if model_path:\n",
    "            agent = Agent.load(env, model_path, epsilon=0.1)\n",
    "        else:\n",
    "            agent = Agent(actions, 0.1, input_shape=(env.width, env.height, env.frame_count))\n",
    "        self.train_loop(env, agent, episode_count, initial_count)\n",
    "        \n",
    "    def train_loop(self, env, agent, episode_count, initial_count):\n",
    "        \n",
    "        for episode in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                e = Experience(state, action, step_reward, next_state, done)\n",
    "                self.experiences.append(e)\n",
    "                episode_reward += step_reward\n",
    "                loss = self.step(episode, agent)\n",
    "                state = next_state\n",
    "                \n",
    "                if not self.training and (len(self.experiences) >= self.buffer_size or episode >= initial_count):\n",
    "                    self.begin_training(agent)\n",
    "                    self.training = True\n",
    "            \n",
    "            self.end_episode(episode, episode_reward, loss, agent)\n",
    "    \n",
    "    def step(self, step, agent):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            loss = agent.update(batch, self.gamma)\n",
    "            self.training_count += 1\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "            return loss\n",
    "            \n",
    "    def begin_training(self, agent):\n",
    "        print('start training!')\n",
    "        self.logger.set_model(agent.model)\n",
    "        \n",
    "    def end_episode(self, episode, reward, loss, agent):\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.ckpt_path)\n",
    "                self._max_reward = reward\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)\n",
    "        \n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '02_doubledqn_05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128)\n",
    "trainer = Trainer(filename=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 20, reward is 0.214 (+/-0.096)\n",
      "At episode 40, reward is 0.211 (+/-0.082)\n",
      "At episode 60, reward is 0.193 (+/-0.054)\n",
      "At episode 80, reward is 0.209 (+/-0.059)\n",
      "At episode 100, reward is 0.222 (+/-0.089)\n",
      "At episode 120, reward is 0.192 (+/-0.029)\n",
      "At episode 140, reward is 0.184 (+/-0.017)\n",
      "At episode 160, reward is 0.207 (+/-0.052)\n",
      "At episode 180, reward is 0.222 (+/-0.089)\n",
      "start training!\n",
      "At episode 200, reward is 0.195 (+/-0.036)\n",
      "At episode 220, reward is 0.205 (+/-0.051)\n",
      "At episode 240, reward is 0.504 (+/-0.327)\n",
      "At episode 260, reward is 0.561 (+/-0.353)\n",
      "At episode 280, reward is 0.462 (+/-0.152)\n",
      "At episode 300, reward is 0.642 (+/-0.575)\n",
      "At episode 320, reward is 0.579 (+/-0.286)\n",
      "At episode 340, reward is 0.552 (+/-0.092)\n",
      "At episode 360, reward is 0.664 (+/-0.443)\n",
      "At episode 380, reward is 0.758 (+/-0.825)\n",
      "At episode 400, reward is 0.536 (+/-0.136)\n",
      "At episode 420, reward is 0.658 (+/-0.307)\n",
      "At episode 440, reward is 0.573 (+/-0.345)\n",
      "At episode 460, reward is 0.521 (+/-0.119)\n",
      "At episode 480, reward is 0.591 (+/-0.333)\n",
      "At episode 500, reward is 0.562 (+/-0.354)\n",
      "At episode 520, reward is 0.554 (+/-0.311)\n",
      "At episode 540, reward is 0.655 (+/-0.627)\n",
      "At episode 560, reward is 0.66 (+/-0.408)\n",
      "At episode 580, reward is 0.449 (+/-0.145)\n",
      "At episode 600, reward is 0.511 (+/-0.121)\n",
      "At episode 620, reward is 0.477 (+/-0.149)\n",
      "At episode 640, reward is 0.554 (+/-0.112)\n",
      "At episode 660, reward is 0.504 (+/-0.303)\n",
      "At episode 680, reward is 0.435 (+/-0.128)\n",
      "At episode 700, reward is 0.577 (+/-0.066)\n",
      "At episode 720, reward is 0.568 (+/-0.05)\n",
      "At episode 740, reward is 0.542 (+/-0.287)\n",
      "At episode 760, reward is 0.582 (+/-0.349)\n",
      "At episode 780, reward is 0.497 (+/-0.136)\n",
      "At episode 800, reward is 0.474 (+/-0.157)\n",
      "At episode 820, reward is 0.53 (+/-0.363)\n",
      "At episode 840, reward is 0.611 (+/-0.291)\n",
      "At episode 860, reward is 0.599 (+/-0.335)\n",
      "At episode 880, reward is 0.581 (+/-0.415)\n",
      "At episode 900, reward is 0.52 (+/-0.138)\n",
      "At episode 920, reward is 0.541 (+/-0.317)\n",
      "At episode 940, reward is 0.503 (+/-0.139)\n",
      "At episode 960, reward is 0.598 (+/-0.34)\n",
      "At episode 980, reward is 0.638 (+/-0.263)\n",
      "At episode 1000, reward is 0.756 (+/-0.392)\n",
      "At episode 1020, reward is 0.943 (+/-0.57)\n",
      "At episode 1040, reward is 1.117 (+/-0.757)\n",
      "At episode 1060, reward is 0.795 (+/-0.469)\n",
      "At episode 1080, reward is 0.702 (+/-0.398)\n",
      "At episode 1100, reward is 0.82 (+/-0.702)\n",
      "At episode 1120, reward is 0.785 (+/-0.47)\n",
      "At episode 1140, reward is 0.857 (+/-0.493)\n",
      "At episode 1160, reward is 0.69 (+/-0.363)\n",
      "At episode 1180, reward is 0.784 (+/-0.419)\n",
      "At episode 1200, reward is 0.907 (+/-0.523)\n",
      "At episode 1220, reward is 0.795 (+/-0.451)\n",
      "At episode 1240, reward is 0.765 (+/-0.432)\n",
      "At episode 1260, reward is 0.63 (+/-0.254)\n",
      "At episode 1280, reward is 0.811 (+/-0.676)\n",
      "At episode 1300, reward is 0.698 (+/-0.339)\n",
      "At episode 1320, reward is 0.627 (+/-0.323)\n",
      "At episode 1340, reward is 0.641 (+/-0.246)\n",
      "At episode 1360, reward is 0.617 (+/-0.27)\n",
      "At episode 1380, reward is 0.892 (+/-0.682)\n",
      "At episode 1400, reward is 0.869 (+/-0.654)\n",
      "At episode 1420, reward is 0.651 (+/-0.296)\n",
      "At episode 1440, reward is 0.776 (+/-0.463)\n",
      "At episode 1460, reward is 0.89 (+/-0.706)\n",
      "At episode 1480, reward is 0.626 (+/-0.276)\n",
      "At episode 1500, reward is 0.655 (+/-0.311)\n",
      "At episode 1520, reward is 0.846 (+/-0.686)\n",
      "At episode 1540, reward is 0.798 (+/-0.491)\n",
      "At episode 1560, reward is 0.714 (+/-0.423)\n",
      "At episode 1580, reward is 0.894 (+/-0.561)\n",
      "At episode 1600, reward is 0.742 (+/-0.406)\n",
      "At episode 1620, reward is 0.608 (+/-0.027)\n",
      "At episode 1640, reward is 0.858 (+/-0.632)\n",
      "At episode 1660, reward is 0.798 (+/-0.449)\n",
      "At episode 1680, reward is 0.867 (+/-0.516)\n",
      "At episode 1700, reward is 0.906 (+/-0.527)\n",
      "At episode 1720, reward is 0.839 (+/-0.466)\n",
      "At episode 1740, reward is 0.925 (+/-0.778)\n",
      "At episode 1760, reward is 0.922 (+/-0.576)\n",
      "At episode 1780, reward is 0.974 (+/-0.79)\n",
      "At episode 1800, reward is 0.725 (+/-0.417)\n",
      "At episode 1820, reward is 0.91 (+/-0.655)\n",
      "At episode 1840, reward is 0.796 (+/-0.466)\n",
      "At episode 1860, reward is 0.793 (+/-0.452)\n",
      "At episode 1880, reward is 0.745 (+/-0.405)\n",
      "At episode 1900, reward is 0.866 (+/-0.515)\n",
      "At episode 1920, reward is 0.763 (+/-0.386)\n",
      "At episode 1940, reward is 0.73 (+/-0.363)\n",
      "At episode 1960, reward is 0.612 (+/-0.01)\n",
      "At episode 1980, reward is 0.842 (+/-0.465)\n",
      "At episode 2000, reward is 0.844 (+/-0.471)\n",
      "At episode 2020, reward is 0.881 (+/-0.465)\n",
      "At episode 2040, reward is 0.679 (+/-0.298)\n",
      "At episode 2060, reward is 0.993 (+/-0.824)\n",
      "At episode 2080, reward is 0.908 (+/-0.521)\n",
      "At episode 2100, reward is 0.852 (+/-0.488)\n",
      "At episode 2120, reward is 0.731 (+/-0.366)\n",
      "At episode 2140, reward is 0.891 (+/-0.535)\n",
      "At episode 2160, reward is 0.788 (+/-0.425)\n",
      "At episode 2180, reward is 0.912 (+/-0.526)\n",
      "At episode 2200, reward is 0.92 (+/-0.667)\n",
      "At episode 2220, reward is 1.036 (+/-0.882)\n",
      "At episode 2240, reward is 0.853 (+/-0.485)\n",
      "At episode 2260, reward is 0.98 (+/-0.564)\n",
      "At episode 2280, reward is 0.98 (+/-0.568)\n",
      "At episode 2300, reward is 0.867 (+/-0.67)\n",
      "At episode 2320, reward is 1.304 (+/-1.376)\n",
      "At episode 2340, reward is 1.528 (+/-1.135)\n",
      "At episode 2360, reward is 1.267 (+/-0.992)\n",
      "At episode 2380, reward is 0.924 (+/-0.543)\n",
      "At episode 2400, reward is 0.92 (+/-0.686)\n",
      "At episode 2420, reward is 1.134 (+/-0.702)\n",
      "At episode 2440, reward is 1.06 (+/-0.697)\n",
      "At episode 2460, reward is 1.262 (+/-0.99)\n",
      "At episode 2480, reward is 1.01 (+/-0.908)\n",
      "At episode 2500, reward is 1.273 (+/-1.067)\n",
      "At episode 2520, reward is 0.854 (+/-0.621)\n",
      "At episode 2540, reward is 0.772 (+/-0.388)\n",
      "At episode 2560, reward is 1.17 (+/-1.067)\n",
      "At episode 2580, reward is 0.837 (+/-0.466)\n",
      "At episode 2600, reward is 1.438 (+/-1.11)\n",
      "At episode 2620, reward is 1.215 (+/-0.745)\n",
      "At episode 2640, reward is 1.144 (+/-0.707)\n",
      "At episode 2660, reward is 0.949 (+/-0.672)\n",
      "At episode 2680, reward is 1.102 (+/-1.217)\n",
      "At episode 2700, reward is 0.843 (+/-0.469)\n",
      "At episode 2720, reward is 1.005 (+/-0.546)\n",
      "At episode 2740, reward is 1.262 (+/-0.713)\n",
      "At episode 2760, reward is 1.216 (+/-0.843)\n",
      "At episode 2780, reward is 1.038 (+/-0.726)\n",
      "At episode 2800, reward is 1.598 (+/-1.452)\n",
      "At episode 2820, reward is 1.423 (+/-1.215)\n",
      "At episode 2840, reward is 1.546 (+/-1.539)\n",
      "At episode 2860, reward is 1.275 (+/-0.832)\n",
      "At episode 2880, reward is 1.491 (+/-1.293)\n",
      "At episode 2900, reward is 1.148 (+/-0.804)\n",
      "At episode 2920, reward is 1.652 (+/-1.642)\n",
      "At episode 2940, reward is 1.211 (+/-0.825)\n",
      "At episode 2960, reward is 1.022 (+/-0.683)\n",
      "At episode 2980, reward is 1.134 (+/-0.807)\n",
      "At episode 3000, reward is 1.676 (+/-1.051)\n",
      "At episode 3020, reward is 1.736 (+/-1.505)\n",
      "At episode 3040, reward is 1.793 (+/-1.113)\n",
      "At episode 3060, reward is 0.984 (+/-0.726)\n",
      "At episode 3080, reward is 1.552 (+/-1.458)\n",
      "At episode 3100, reward is 1.434 (+/-1.318)\n",
      "At episode 3120, reward is 1.44 (+/-1.374)\n",
      "At episode 3140, reward is 2.045 (+/-1.512)\n",
      "At episode 3160, reward is 1.565 (+/-1.302)\n",
      "At episode 3180, reward is 1.145 (+/-0.597)\n",
      "At episode 3200, reward is 2.031 (+/-1.747)\n",
      "At episode 3220, reward is 2.459 (+/-2.029)\n",
      "At episode 3240, reward is 2.178 (+/-1.516)\n",
      "At episode 3260, reward is 2.316 (+/-1.763)\n",
      "At episode 3280, reward is 1.167 (+/-0.923)\n",
      "At episode 3300, reward is 2.484 (+/-2.59)\n",
      "At episode 3320, reward is 1.993 (+/-2.218)\n",
      "At episode 3340, reward is 1.981 (+/-2.19)\n",
      "At episode 3360, reward is 2.636 (+/-2.138)\n",
      "At episode 3380, reward is 2.211 (+/-2.419)\n",
      "At episode 3400, reward is 2.081 (+/-2.649)\n",
      "At episode 3420, reward is 2.643 (+/-2.459)\n",
      "At episode 3440, reward is 1.544 (+/-1.407)\n",
      "At episode 3460, reward is 1.533 (+/-1.58)\n",
      "At episode 3480, reward is 1.757 (+/-1.76)\n",
      "At episode 3500, reward is 2.495 (+/-1.858)\n",
      "At episode 3520, reward is 2.766 (+/-1.919)\n",
      "At episode 3540, reward is 1.616 (+/-1.471)\n",
      "At episode 3560, reward is 2.392 (+/-1.908)\n",
      "At episode 3580, reward is 2.15 (+/-1.76)\n",
      "At episode 3600, reward is 1.78 (+/-1.448)\n",
      "At episode 3620, reward is 2.373 (+/-2.02)\n",
      "At episode 3640, reward is 2.956 (+/-2.603)\n",
      "At episode 3660, reward is 2.438 (+/-2.115)\n",
      "At episode 3680, reward is 3.162 (+/-2.697)\n",
      "At episode 3700, reward is 2.622 (+/-2.457)\n",
      "At episode 3720, reward is 2.733 (+/-2.156)\n",
      "At episode 3740, reward is 3.776 (+/-3.823)\n",
      "At episode 3760, reward is 2.594 (+/-2.574)\n",
      "At episode 3780, reward is 2.994 (+/-2.45)\n",
      "At episode 3800, reward is 2.43 (+/-2.627)\n",
      "At episode 3820, reward is 3.833 (+/-4.05)\n",
      "At episode 3840, reward is 1.802 (+/-1.443)\n",
      "At episode 3860, reward is 2.14 (+/-2.118)\n",
      "At episode 3880, reward is 2.67 (+/-3.042)\n",
      "At episode 3900, reward is 4.418 (+/-3.006)\n",
      "At episode 3920, reward is 3.314 (+/-2.649)\n",
      "At episode 3940, reward is 5.039 (+/-4.599)\n",
      "At episode 3960, reward is 4.705 (+/-3.801)\n",
      "At episode 3980, reward is 3.91 (+/-2.825)\n",
      "At episode 4000, reward is 5.194 (+/-3.435)\n",
      "At episode 4020, reward is 3.252 (+/-2.742)\n",
      "At episode 4040, reward is 4.859 (+/-4.558)\n",
      "At episode 4060, reward is 9.127 (+/-6.037)\n",
      "At episode 4080, reward is 7.87 (+/-8.835)\n",
      "At episode 4100, reward is 4.743 (+/-3.134)\n",
      "At episode 4120, reward is 7.655 (+/-4.414)\n",
      "At episode 4140, reward is 5.268 (+/-4.841)\n",
      "At episode 4160, reward is 6.237 (+/-4.022)\n",
      "At episode 4180, reward is 4.489 (+/-4.105)\n",
      "At episode 4200, reward is 7.484 (+/-6.397)\n",
      "At episode 4220, reward is 5.06 (+/-4.998)\n",
      "At episode 4240, reward is 8.051 (+/-10.066)\n",
      "At episode 4260, reward is 6.537 (+/-5.798)\n",
      "At episode 4280, reward is 5.889 (+/-3.856)\n",
      "At episode 4300, reward is 9.308 (+/-6.036)\n",
      "At episode 4320, reward is 7.797 (+/-8.291)\n",
      "At episode 4340, reward is 5.81 (+/-5.86)\n",
      "At episode 4360, reward is 6.924 (+/-4.964)\n",
      "At episode 4380, reward is 6.31 (+/-6.252)\n",
      "At episode 4400, reward is 8.01 (+/-6.94)\n",
      "At episode 4420, reward is 7.461 (+/-5.8)\n",
      "At episode 4440, reward is 7.847 (+/-7.326)\n",
      "At episode 4460, reward is 8.437 (+/-11.522)\n",
      "At episode 4480, reward is 7.401 (+/-6.952)\n",
      "At episode 4500, reward is 8.217 (+/-7.127)\n",
      "At episode 4520, reward is 11.35 (+/-12.331)\n",
      "At episode 4540, reward is 7.269 (+/-6.093)\n",
      "At episode 4560, reward is 12.25 (+/-6.563)\n",
      "At episode 4580, reward is 8.195 (+/-8.72)\n",
      "At episode 4600, reward is 8.645 (+/-7.531)\n",
      "At episode 4620, reward is 10.727 (+/-8.351)\n",
      "At episode 4640, reward is 7.83 (+/-6.242)\n",
      "At episode 4660, reward is 10.547 (+/-11.903)\n",
      "At episode 4680, reward is 12.931 (+/-11.964)\n",
      "At episode 4700, reward is 7.954 (+/-5.598)\n",
      "At episode 4720, reward is 7.685 (+/-7.074)\n",
      "At episode 4740, reward is 11.661 (+/-8.578)\n",
      "At episode 4760, reward is 13.237 (+/-10.933)\n",
      "At episode 4780, reward is 11.225 (+/-11.068)\n",
      "At episode 4800, reward is 10.329 (+/-7.215)\n",
      "At episode 4820, reward is 8.953 (+/-8.731)\n",
      "At episode 4840, reward is 9.506 (+/-7.179)\n",
      "At episode 4860, reward is 11.155 (+/-12.696)\n",
      "At episode 4880, reward is 9.147 (+/-10.692)\n",
      "At episode 4900, reward is 8.642 (+/-8.845)\n",
      "At episode 4920, reward is 13.731 (+/-11.585)\n",
      "At episode 4940, reward is 8.122 (+/-5.628)\n",
      "At episode 4960, reward is 8.148 (+/-6.881)\n",
      "At episode 4980, reward is 11.789 (+/-9.869)\n",
      "At episode 5000, reward is 13.745 (+/-11.336)\n",
      "At episode 5020, reward is 10.73 (+/-8.675)\n",
      "At episode 5040, reward is 8.813 (+/-8.661)\n",
      "At episode 5060, reward is 8.644 (+/-6.746)\n",
      "At episode 5080, reward is 5.941 (+/-4.075)\n",
      "At episode 5100, reward is 10.198 (+/-10.143)\n",
      "At episode 5120, reward is 10.841 (+/-11.291)\n",
      "At episode 5140, reward is 8.37 (+/-6.265)\n",
      "At episode 5160, reward is 10.708 (+/-11.655)\n",
      "At episode 5180, reward is 9.719 (+/-6.86)\n",
      "At episode 5200, reward is 15.052 (+/-15.995)\n",
      "At episode 5220, reward is 10.825 (+/-7.68)\n",
      "At episode 5240, reward is 13.942 (+/-14.822)\n",
      "At episode 5260, reward is 15.121 (+/-12.987)\n",
      "At episode 5280, reward is 10.355 (+/-9.769)\n",
      "At episode 5300, reward is 12.697 (+/-12.478)\n",
      "At episode 5320, reward is 11.981 (+/-9.151)\n",
      "At episode 5340, reward is 11.441 (+/-8.924)\n",
      "At episode 5360, reward is 8.503 (+/-11.19)\n",
      "At episode 5380, reward is 11.414 (+/-8.293)\n",
      "At episode 5400, reward is 11.45 (+/-10.05)\n",
      "At episode 5420, reward is 10.372 (+/-10.087)\n",
      "At episode 5440, reward is 6.513 (+/-7.679)\n",
      "At episode 5460, reward is 6.956 (+/-5.268)\n",
      "At episode 5480, reward is 14.0 (+/-11.656)\n",
      "At episode 5500, reward is 14.981 (+/-10.451)\n",
      "At episode 5520, reward is 8.09 (+/-7.318)\n",
      "At episode 5540, reward is 9.731 (+/-8.099)\n",
      "At episode 5560, reward is 10.792 (+/-7.614)\n",
      "At episode 5580, reward is 6.703 (+/-5.804)\n",
      "At episode 5600, reward is 11.289 (+/-8.759)\n",
      "At episode 5620, reward is 7.108 (+/-7.961)\n",
      "At episode 5640, reward is 10.262 (+/-9.196)\n",
      "At episode 5660, reward is 10.002 (+/-9.555)\n",
      "At episode 5680, reward is 11.462 (+/-10.033)\n",
      "At episode 5700, reward is 10.058 (+/-11.653)\n",
      "At episode 5720, reward is 9.884 (+/-8.613)\n",
      "At episode 5740, reward is 12.65 (+/-13.054)\n",
      "At episode 5760, reward is 8.946 (+/-8.386)\n",
      "At episode 5780, reward is 14.525 (+/-14.253)\n",
      "At episode 5800, reward is 7.764 (+/-7.611)\n",
      "At episode 5820, reward is 10.004 (+/-8.951)\n",
      "At episode 5840, reward is 6.727 (+/-5.88)\n",
      "At episode 5860, reward is 7.509 (+/-6.138)\n",
      "At episode 5880, reward is 13.525 (+/-9.72)\n",
      "At episode 5900, reward is 11.453 (+/-11.491)\n",
      "At episode 5920, reward is 15.117 (+/-14.832)\n",
      "At episode 5940, reward is 12.312 (+/-12.497)\n",
      "At episode 5960, reward is 9.295 (+/-10.022)\n",
      "At episode 5980, reward is 11.414 (+/-11.48)\n",
      "At episode 6000, reward is 11.111 (+/-9.555)\n",
      "At episode 6020, reward is 16.407 (+/-14.734)\n",
      "At episode 6040, reward is 9.309 (+/-10.143)\n",
      "At episode 6060, reward is 13.329 (+/-11.232)\n",
      "At episode 6080, reward is 9.82 (+/-12.966)\n",
      "At episode 6100, reward is 16.389 (+/-14.51)\n",
      "At episode 6120, reward is 9.717 (+/-11.065)\n",
      "At episode 6140, reward is 12.904 (+/-14.627)\n",
      "At episode 6160, reward is 12.523 (+/-11.379)\n",
      "At episode 6180, reward is 16.971 (+/-17.459)\n",
      "At episode 6200, reward is 12.805 (+/-13.695)\n",
      "At episode 6220, reward is 13.377 (+/-11.453)\n",
      "At episode 6240, reward is 19.094 (+/-19.65)\n",
      "At episode 6260, reward is 14.152 (+/-17.003)\n",
      "At episode 6280, reward is 10.394 (+/-9.222)\n",
      "At episode 6300, reward is 11.997 (+/-13.121)\n",
      "At episode 6320, reward is 13.022 (+/-14.12)\n",
      "At episode 6340, reward is 7.544 (+/-9.076)\n",
      "At episode 6360, reward is 8.105 (+/-7.687)\n",
      "At episode 6380, reward is 12.551 (+/-12.661)\n",
      "At episode 6400, reward is 12.586 (+/-12.991)\n",
      "At episode 6420, reward is 11.429 (+/-10.34)\n",
      "At episode 6440, reward is 12.727 (+/-10.592)\n",
      "At episode 6460, reward is 9.809 (+/-11.626)\n",
      "At episode 6480, reward is 8.895 (+/-5.83)\n",
      "At episode 6500, reward is 12.193 (+/-10.7)\n",
      "At episode 6520, reward is 10.651 (+/-10.881)\n",
      "At episode 6540, reward is 12.818 (+/-11.149)\n",
      "At episode 6560, reward is 15.292 (+/-15.498)\n",
      "At episode 6580, reward is 12.969 (+/-10.213)\n",
      "At episode 6600, reward is 8.399 (+/-7.961)\n",
      "At episode 6620, reward is 15.37 (+/-12.346)\n",
      "At episode 6640, reward is 10.294 (+/-10.744)\n",
      "At episode 6660, reward is 14.745 (+/-11.918)\n",
      "At episode 6680, reward is 5.694 (+/-4.978)\n",
      "At episode 6700, reward is 9.378 (+/-11.536)\n",
      "At episode 6720, reward is 10.399 (+/-8.269)\n",
      "At episode 6740, reward is 8.647 (+/-6.483)\n",
      "At episode 6760, reward is 10.245 (+/-11.268)\n",
      "At episode 6780, reward is 12.644 (+/-11.248)\n",
      "At episode 6800, reward is 15.575 (+/-16.555)\n",
      "At episode 6820, reward is 16.877 (+/-16.299)\n",
      "At episode 6840, reward is 11.499 (+/-9.896)\n",
      "At episode 6860, reward is 11.678 (+/-13.195)\n",
      "At episode 6880, reward is 8.366 (+/-9.943)\n",
      "At episode 6900, reward is 7.671 (+/-7.187)\n",
      "At episode 6920, reward is 12.092 (+/-13.043)\n",
      "At episode 6940, reward is 11.194 (+/-9.687)\n",
      "At episode 6960, reward is 12.102 (+/-12.849)\n",
      "At episode 6980, reward is 7.348 (+/-8.456)\n",
      "At episode 7000, reward is 11.139 (+/-10.129)\n",
      "At episode 7020, reward is 7.956 (+/-8.633)\n",
      "At episode 7040, reward is 14.222 (+/-10.66)\n",
      "At episode 7060, reward is 9.032 (+/-8.474)\n",
      "At episode 7080, reward is 11.84 (+/-11.312)\n",
      "At episode 7100, reward is 12.389 (+/-13.267)\n",
      "At episode 7120, reward is 10.539 (+/-9.965)\n",
      "At episode 7140, reward is 18.683 (+/-19.777)\n",
      "At episode 7160, reward is 13.819 (+/-10.548)\n",
      "At episode 7180, reward is 9.743 (+/-8.443)\n",
      "At episode 7200, reward is 6.09 (+/-4.153)\n",
      "At episode 7220, reward is 11.375 (+/-6.505)\n",
      "At episode 7240, reward is 10.751 (+/-11.835)\n",
      "At episode 7260, reward is 10.189 (+/-8.509)\n",
      "At episode 7280, reward is 9.116 (+/-7.534)\n",
      "At episode 7300, reward is 10.809 (+/-12.363)\n",
      "At episode 7320, reward is 6.335 (+/-4.322)\n",
      "At episode 7340, reward is 8.194 (+/-7.903)\n",
      "At episode 7360, reward is 11.978 (+/-13.662)\n",
      "At episode 7380, reward is 11.855 (+/-7.775)\n",
      "At episode 7400, reward is 8.225 (+/-8.122)\n",
      "At episode 7420, reward is 11.539 (+/-8.595)\n",
      "At episode 7440, reward is 11.92 (+/-9.357)\n",
      "At episode 7460, reward is 10.261 (+/-8.243)\n",
      "At episode 7480, reward is 12.288 (+/-11.108)\n",
      "At episode 7500, reward is 8.693 (+/-7.852)\n",
      "At episode 7520, reward is 5.116 (+/-4.053)\n",
      "At episode 7540, reward is 11.061 (+/-9.278)\n",
      "At episode 7560, reward is 10.698 (+/-8.499)\n",
      "At episode 7580, reward is 11.148 (+/-9.346)\n",
      "At episode 7600, reward is 13.259 (+/-11.38)\n",
      "At episode 7620, reward is 13.022 (+/-20.193)\n",
      "At episode 7640, reward is 11.828 (+/-10.32)\n",
      "At episode 7660, reward is 10.973 (+/-11.734)\n",
      "At episode 7680, reward is 8.87 (+/-7.473)\n",
      "At episode 7700, reward is 12.087 (+/-12.307)\n",
      "At episode 7720, reward is 13.93 (+/-16.28)\n",
      "At episode 7740, reward is 6.686 (+/-6.124)\n",
      "At episode 7760, reward is 15.971 (+/-19.985)\n",
      "At episode 7780, reward is 11.791 (+/-8.211)\n",
      "At episode 7800, reward is 5.814 (+/-4.544)\n",
      "At episode 7820, reward is 9.181 (+/-8.723)\n",
      "At episode 7840, reward is 16.213 (+/-20.916)\n",
      "At episode 7860, reward is 12.125 (+/-13.089)\n",
      "At episode 7880, reward is 9.576 (+/-8.559)\n",
      "At episode 7900, reward is 11.868 (+/-12.917)\n",
      "At episode 7920, reward is 8.971 (+/-7.359)\n",
      "At episode 7940, reward is 6.845 (+/-7.667)\n",
      "At episode 7960, reward is 12.699 (+/-11.139)\n",
      "At episode 7980, reward is 11.73 (+/-8.969)\n",
      "At episode 8000, reward is 10.987 (+/-10.486)\n",
      "At episode 8020, reward is 9.58 (+/-13.189)\n",
      "At episode 8040, reward is 9.216 (+/-8.063)\n",
      "At episode 8060, reward is 7.457 (+/-6.568)\n",
      "At episode 8080, reward is 11.896 (+/-11.174)\n",
      "At episode 8100, reward is 12.834 (+/-7.779)\n",
      "At episode 8120, reward is 22.492 (+/-18.301)\n",
      "At episode 8140, reward is 8.653 (+/-5.373)\n",
      "At episode 8160, reward is 10.256 (+/-12.773)\n",
      "At episode 8180, reward is 11.54 (+/-13.73)\n",
      "At episode 8200, reward is 10.772 (+/-10.063)\n",
      "At episode 8220, reward is 7.846 (+/-7.287)\n",
      "At episode 8240, reward is 6.453 (+/-6.248)\n",
      "At episode 8260, reward is 14.434 (+/-18.428)\n",
      "At episode 8280, reward is 7.501 (+/-7.03)\n",
      "At episode 8300, reward is 10.03 (+/-10.772)\n",
      "At episode 8320, reward is 11.804 (+/-8.962)\n",
      "At episode 8340, reward is 10.991 (+/-12.271)\n",
      "At episode 8360, reward is 13.143 (+/-14.349)\n",
      "At episode 8380, reward is 13.406 (+/-11.73)\n",
      "At episode 8400, reward is 11.586 (+/-12.6)\n",
      "At episode 8420, reward is 9.642 (+/-5.822)\n",
      "At episode 8440, reward is 8.593 (+/-7.515)\n",
      "At episode 8460, reward is 10.732 (+/-8.498)\n",
      "At episode 8480, reward is 7.583 (+/-6.478)\n",
      "At episode 8500, reward is 10.037 (+/-8.647)\n",
      "At episode 8520, reward is 14.159 (+/-14.341)\n",
      "At episode 8540, reward is 10.025 (+/-11.306)\n",
      "At episode 8560, reward is 13.395 (+/-7.022)\n",
      "At episode 8580, reward is 9.5 (+/-7.991)\n",
      "At episode 8600, reward is 12.769 (+/-14.8)\n",
      "At episode 8620, reward is 8.699 (+/-9.241)\n",
      "At episode 8640, reward is 7.327 (+/-5.266)\n",
      "At episode 8660, reward is 8.424 (+/-7.879)\n",
      "At episode 8680, reward is 7.472 (+/-5.846)\n",
      "At episode 8700, reward is 9.449 (+/-7.781)\n",
      "At episode 8720, reward is 8.682 (+/-7.754)\n",
      "At episode 8740, reward is 6.742 (+/-6.866)\n",
      "At episode 8760, reward is 9.516 (+/-9.412)\n",
      "At episode 8780, reward is 6.657 (+/-6.144)\n",
      "At episode 8800, reward is 9.732 (+/-9.77)\n",
      "At episode 8820, reward is 18.226 (+/-13.162)\n",
      "At episode 8840, reward is 10.747 (+/-10.82)\n",
      "At episode 8860, reward is 10.77 (+/-9.067)\n",
      "At episode 8880, reward is 12.22 (+/-14.137)\n",
      "At episode 8900, reward is 11.839 (+/-9.212)\n",
      "At episode 8920, reward is 15.502 (+/-13.026)\n",
      "At episode 8940, reward is 12.985 (+/-14.394)\n",
      "At episode 8960, reward is 14.375 (+/-18.053)\n",
      "At episode 8980, reward is 12.551 (+/-9.88)\n",
      "At episode 9000, reward is 10.431 (+/-9.528)\n",
      "At episode 9020, reward is 8.582 (+/-6.143)\n",
      "At episode 9040, reward is 13.825 (+/-12.246)\n",
      "At episode 9060, reward is 13.424 (+/-12.029)\n",
      "At episode 9080, reward is 11.133 (+/-12.978)\n",
      "At episode 9100, reward is 15.763 (+/-13.947)\n",
      "At episode 9120, reward is 11.293 (+/-9.387)\n",
      "At episode 9140, reward is 12.286 (+/-10.375)\n",
      "At episode 9160, reward is 12.529 (+/-12.884)\n",
      "At episode 9180, reward is 8.595 (+/-7.979)\n",
      "At episode 9200, reward is 8.77 (+/-5.468)\n",
      "At episode 9220, reward is 8.437 (+/-7.067)\n",
      "At episode 9240, reward is 9.946 (+/-11.684)\n",
      "At episode 9260, reward is 12.689 (+/-11.641)\n",
      "At episode 9280, reward is 11.789 (+/-14.781)\n",
      "At episode 9300, reward is 9.772 (+/-10.864)\n",
      "At episode 9320, reward is 13.776 (+/-10.531)\n",
      "At episode 9340, reward is 8.373 (+/-7.251)\n",
      "At episode 9360, reward is 14.011 (+/-13.44)\n",
      "At episode 9380, reward is 11.859 (+/-13.563)\n",
      "At episode 9400, reward is 12.313 (+/-11.896)\n",
      "At episode 9420, reward is 9.483 (+/-7.787)\n",
      "At episode 9440, reward is 9.221 (+/-9.886)\n",
      "At episode 9460, reward is 13.208 (+/-13.673)\n",
      "At episode 9480, reward is 11.754 (+/-11.728)\n",
      "At episode 9500, reward is 10.587 (+/-9.214)\n",
      "At episode 9520, reward is 16.704 (+/-11.436)\n",
      "At episode 9540, reward is 13.365 (+/-12.732)\n",
      "At episode 9560, reward is 11.926 (+/-14.236)\n",
      "At episode 9580, reward is 14.024 (+/-11.942)\n",
      "At episode 9600, reward is 11.563 (+/-11.683)\n",
      "At episode 9620, reward is 14.564 (+/-21.066)\n",
      "At episode 9640, reward is 15.027 (+/-12.43)\n",
      "At episode 9660, reward is 11.119 (+/-9.861)\n",
      "At episode 9680, reward is 15.332 (+/-16.432)\n",
      "At episode 9700, reward is 16.29 (+/-14.434)\n",
      "At episode 9720, reward is 11.956 (+/-10.039)\n",
      "At episode 9740, reward is 9.619 (+/-10.47)\n",
      "At episode 9760, reward is 11.12 (+/-8.088)\n",
      "At episode 9780, reward is 7.046 (+/-6.352)\n",
      "At episode 9800, reward is 8.829 (+/-9.941)\n",
      "At episode 9820, reward is 13.532 (+/-12.176)\n",
      "At episode 9840, reward is 8.517 (+/-7.722)\n",
      "At episode 9860, reward is 10.589 (+/-8.502)\n",
      "At episode 9880, reward is 8.756 (+/-8.645)\n",
      "At episode 9900, reward is 8.982 (+/-7.462)\n",
      "At episode 9920, reward is 11.036 (+/-9.436)\n",
      "At episode 9940, reward is 5.796 (+/-4.887)\n",
      "At episode 9960, reward is 12.124 (+/-13.468)\n",
      "At episode 9980, reward is 11.297 (+/-9.855)\n"
     ]
    }
   ],
   "source": [
    "trainer.train(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/flappybird-try-wqQoCzq9/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 103.0000\n",
      "episode 1, total reward: 20.0000\n",
      "episode 2, total reward: 41.0000\n",
      "episode 3, total reward: 45.0000\n",
      "episode 4, total reward: 7.0000\n",
      "episode 5, total reward: 4.0000\n",
      "episode 6, total reward: 199.0000\n",
      "episode 7, total reward: 18.0000\n",
      "episode 8, total reward: 7.0000\n",
      "episode 9, total reward: 12.0000\n",
      "episode 10, total reward: 1.0000\n",
      "episode 11, total reward: 6.0000\n",
      "episode 12, total reward: 9.0000\n",
      "episode 13, total reward: 12.0000\n",
      "episode 14, total reward: 142.0000\n",
      "episode 15, total reward: 76.0000\n",
      "episode 16, total reward: 33.0000\n",
      "episode 17, total reward: 11.0000\n",
      "episode 18, total reward: 12.0000\n",
      "episode 19, total reward: 22.0000\n",
      "episode 20, total reward: 37.0000\n",
      "episode 21, total reward: 73.0000\n",
      "episode 22, total reward: 19.0000\n",
      "episode 23, total reward: 130.0000\n",
      "episode 24, total reward: 1.0000\n",
      "episode 25, total reward: 142.0000\n",
      "episode 26, total reward: 14.0000\n",
      "episode 27, total reward: 10.0000\n",
      "episode 28, total reward: 103.0000\n",
      "episode 29, total reward: 6.0000\n",
      "episode 30, total reward: 45.0000\n",
      "episode 31, total reward: 98.0000\n",
      "episode 32, total reward: 156.0000\n",
      "episode 33, total reward: 7.0000\n",
      "episode 34, total reward: 38.0000\n",
      "episode 35, total reward: 8.0000\n",
      "episode 36, total reward: 19.0000\n",
      "episode 37, total reward: 37.0000\n",
      "episode 38, total reward: 5.0000\n",
      "episode 39, total reward: 76.0000\n",
      "episode 40, total reward: 49.0000\n",
      "episode 41, total reward: 18.0000\n",
      "episode 42, total reward: 11.0000\n",
      "episode 43, total reward: 47.0000\n",
      "episode 44, total reward: 41.0000\n",
      "episode 45, total reward: 14.0000\n",
      "episode 46, total reward: 11.0000\n",
      "episode 47, total reward: 4.0000\n",
      "episode 48, total reward: 24.0000\n",
      "episode 49, total reward: 6.0000\n",
      "reward by 50, mean: 40.5800, std: 46.5618\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "model_name = '02_doubledqn_01'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 124.0000\n",
      "episode 1, total reward: 22.0000\n",
      "episode 2, total reward: 80.0000\n",
      "episode 3, total reward: 19.0000\n",
      "episode 4, total reward: 87.0000\n",
      "episode 5, total reward: 38.0000\n",
      "episode 6, total reward: 2.0000\n",
      "episode 7, total reward: 1.0000\n",
      "episode 8, total reward: 4.0000\n",
      "episode 9, total reward: 125.0000\n",
      "episode 10, total reward: 8.0000\n",
      "episode 11, total reward: 55.0000\n",
      "episode 12, total reward: 24.0000\n",
      "episode 13, total reward: 17.0000\n",
      "episode 14, total reward: 25.0000\n",
      "episode 15, total reward: 32.0000\n",
      "episode 16, total reward: 13.0000\n",
      "episode 17, total reward: 19.0000\n",
      "episode 18, total reward: 44.0000\n",
      "episode 19, total reward: 49.0000\n",
      "episode 20, total reward: 79.0000\n",
      "episode 21, total reward: 22.0000\n",
      "episode 22, total reward: 92.0000\n",
      "episode 23, total reward: 7.0000\n",
      "episode 24, total reward: 7.0000\n",
      "episode 25, total reward: 120.0000\n",
      "episode 26, total reward: 3.0000\n",
      "episode 27, total reward: 19.0000\n",
      "episode 28, total reward: 28.0000\n",
      "episode 29, total reward: 50.0000\n",
      "episode 30, total reward: 1.0000\n",
      "episode 31, total reward: 28.0000\n",
      "episode 32, total reward: 101.0000\n",
      "episode 33, total reward: 9.0000\n",
      "episode 34, total reward: 2.0000\n",
      "episode 35, total reward: 62.0000\n",
      "episode 36, total reward: 127.0000\n",
      "episode 37, total reward: 38.0000\n",
      "episode 38, total reward: 13.0000\n",
      "episode 39, total reward: 11.0000\n",
      "episode 40, total reward: 10.0000\n",
      "episode 41, total reward: 40.0000\n",
      "episode 42, total reward: 4.0000\n",
      "episode 43, total reward: 83.0000\n",
      "episode 44, total reward: 66.0000\n",
      "episode 45, total reward: 38.0000\n",
      "episode 46, total reward: 1.0000\n",
      "episode 47, total reward: 120.0000\n",
      "episode 48, total reward: 216.0000\n",
      "episode 49, total reward: 7.0000\n",
      "reward by 50, mean: 43.8400, std: 45.4495\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "model_name = '02_doubledqn_02'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 63.0000\n",
      "episode 1, total reward: 116.0000\n",
      "episode 2, total reward: 17.0000\n",
      "episode 3, total reward: 29.0000\n",
      "episode 4, total reward: 56.0000\n",
      "episode 5, total reward: 64.0000\n",
      "episode 6, total reward: 22.0000\n",
      "episode 7, total reward: 264.0000\n",
      "episode 8, total reward: 92.0000\n",
      "episode 9, total reward: 13.0000\n",
      "episode 10, total reward: 86.0000\n",
      "episode 11, total reward: 35.0000\n",
      "episode 12, total reward: 95.0000\n",
      "episode 13, total reward: 29.0000\n",
      "episode 14, total reward: 105.0000\n",
      "episode 15, total reward: 99.0000\n",
      "episode 16, total reward: 32.0000\n",
      "episode 17, total reward: 20.0000\n",
      "episode 18, total reward: 53.0000\n",
      "episode 19, total reward: 41.0000\n",
      "episode 20, total reward: 23.0000\n",
      "episode 21, total reward: 2.0000\n",
      "episode 22, total reward: 35.0000\n",
      "episode 23, total reward: 60.0000\n",
      "episode 24, total reward: 53.0000\n",
      "episode 25, total reward: 185.0000\n",
      "episode 26, total reward: 67.0000\n",
      "episode 27, total reward: 8.0000\n",
      "episode 28, total reward: 70.0000\n",
      "episode 29, total reward: 123.0000\n",
      "episode 30, total reward: 2.0000\n",
      "episode 31, total reward: 91.0000\n",
      "episode 32, total reward: 74.0000\n",
      "episode 33, total reward: 1.0000\n",
      "episode 34, total reward: 4.0000\n",
      "episode 35, total reward: 4.0000\n",
      "episode 36, total reward: 4.0000\n",
      "episode 37, total reward: 7.0000\n",
      "episode 38, total reward: 14.0000\n",
      "episode 39, total reward: 41.0000\n",
      "episode 40, total reward: 43.0000\n",
      "episode 41, total reward: 37.0000\n",
      "episode 42, total reward: 77.0000\n",
      "episode 43, total reward: 13.0000\n",
      "episode 44, total reward: 51.0000\n",
      "episode 45, total reward: 94.0000\n",
      "episode 46, total reward: 27.0000\n",
      "episode 47, total reward: 38.0000\n",
      "episode 48, total reward: 35.0000\n",
      "episode 49, total reward: 38.0000\n",
      "reward by 50, mean: 53.0400, std: 48.4706\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "model_name = '02_doubledqn_03'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 14.0000\n",
      "episode 1, total reward: 14.0000\n",
      "episode 2, total reward: 0.0000\n",
      "episode 3, total reward: 0.0000\n",
      "episode 4, total reward: 1.0000\n",
      "episode 5, total reward: 2.0000\n",
      "episode 6, total reward: 0.0000\n",
      "episode 7, total reward: 0.0000\n",
      "episode 8, total reward: 0.0000\n",
      "episode 9, total reward: 10.0000\n",
      "episode 10, total reward: 28.0000\n",
      "episode 11, total reward: 33.0000\n",
      "episode 12, total reward: 48.0000\n",
      "episode 13, total reward: 0.0000\n",
      "episode 14, total reward: 0.0000\n",
      "episode 15, total reward: 2.0000\n",
      "episode 16, total reward: 2.0000\n",
      "episode 17, total reward: 4.0000\n",
      "episode 18, total reward: 11.0000\n",
      "episode 19, total reward: 2.0000\n",
      "episode 20, total reward: 13.0000\n",
      "episode 21, total reward: 11.0000\n",
      "episode 22, total reward: 4.0000\n",
      "episode 23, total reward: 37.0000\n",
      "episode 24, total reward: 10.0000\n",
      "episode 25, total reward: 9.0000\n",
      "episode 26, total reward: 0.0000\n",
      "episode 27, total reward: 1.0000\n",
      "episode 28, total reward: 24.0000\n",
      "episode 29, total reward: 13.0000\n",
      "episode 30, total reward: 9.0000\n",
      "episode 31, total reward: 11.0000\n",
      "episode 32, total reward: 48.0000\n",
      "episode 33, total reward: 5.0000\n",
      "episode 34, total reward: 23.0000\n",
      "episode 35, total reward: 10.0000\n",
      "episode 36, total reward: 0.0000\n",
      "episode 37, total reward: 16.0000\n",
      "episode 38, total reward: 15.0000\n",
      "episode 39, total reward: 13.0000\n",
      "episode 40, total reward: 2.0000\n",
      "episode 41, total reward: 6.0000\n",
      "episode 42, total reward: 1.0000\n",
      "episode 43, total reward: 11.0000\n",
      "episode 44, total reward: 18.0000\n",
      "episode 45, total reward: 1.0000\n",
      "episode 46, total reward: 6.0000\n",
      "episode 47, total reward: 10.0000\n",
      "episode 48, total reward: 38.0000\n",
      "episode 49, total reward: 0.0000\n",
      "reward by 50, mean: 10.7200, std: 12.2932\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "model_name = '02_doubledqn_04'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 8.0000\n",
      "episode 1, total reward: 18.0000\n",
      "episode 2, total reward: 4.0000\n",
      "episode 3, total reward: 27.0000\n",
      "episode 4, total reward: 14.0000\n",
      "episode 5, total reward: 28.0000\n",
      "episode 6, total reward: 32.0000\n",
      "episode 7, total reward: 2.0000\n",
      "episode 8, total reward: 49.0000\n",
      "episode 9, total reward: 13.0000\n",
      "episode 10, total reward: 27.0000\n",
      "episode 11, total reward: 26.0000\n",
      "episode 12, total reward: 5.0000\n",
      "episode 13, total reward: 3.0000\n",
      "episode 14, total reward: 1.0000\n",
      "episode 15, total reward: 70.0000\n",
      "episode 16, total reward: 20.0000\n",
      "episode 17, total reward: 7.0000\n",
      "episode 18, total reward: 32.0000\n",
      "episode 19, total reward: 2.0000\n",
      "episode 20, total reward: 20.0000\n",
      "episode 21, total reward: 2.0000\n",
      "episode 22, total reward: 17.0000\n",
      "episode 23, total reward: 53.0000\n",
      "episode 24, total reward: 10.0000\n",
      "episode 25, total reward: 31.0000\n",
      "episode 26, total reward: 8.0000\n",
      "episode 27, total reward: 12.0000\n",
      "episode 28, total reward: 46.0000\n",
      "episode 29, total reward: 4.0000\n",
      "episode 30, total reward: 3.0000\n",
      "episode 31, total reward: 17.0000\n",
      "episode 32, total reward: 44.0000\n",
      "episode 33, total reward: 7.0000\n",
      "episode 34, total reward: 11.0000\n",
      "episode 35, total reward: 13.0000\n",
      "episode 36, total reward: 17.0000\n",
      "episode 37, total reward: 4.0000\n",
      "episode 38, total reward: 5.0000\n",
      "episode 39, total reward: 24.0000\n",
      "episode 40, total reward: 17.0000\n",
      "episode 41, total reward: 16.0000\n",
      "episode 42, total reward: 39.0000\n",
      "episode 43, total reward: 13.0000\n",
      "episode 44, total reward: 11.0000\n",
      "episode 45, total reward: 5.0000\n",
      "episode 46, total reward: 4.0000\n",
      "episode 47, total reward: 2.0000\n",
      "episode 48, total reward: 73.0000\n",
      "episode 49, total reward: 5.0000\n",
      "reward by 50, mean: 18.4200, std: 17.1465\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "model_name = '02_doubledqn_05'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 33.320, median: 40.580\n"
     ]
    }
   ],
   "source": [
    "res = np.array([40.5800, 43.8400, 53.0400, 10.7200, 18.4200])\n",
    "print('mean: {:.3f}, median: {:.3f}'.format(res.mean(), np.median(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
