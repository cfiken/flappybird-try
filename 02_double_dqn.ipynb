{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import logging\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "\n",
    "# gym\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import gym_ple\n",
    "\n",
    "# disply\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    state: gym.spaces.Box\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: gym.spaces.Box\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir='./logs/', dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.keras.callbacks.TensorBoard(self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "    \n",
    "    @property\n",
    "    def ckpt_path(self):\n",
    "        return os.path.join(self.log_dir, 'model.ckpt')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, is_training: bool, num_outputs: int):\n",
    "        super(AgentModel, self).__init__()\n",
    "        self.is_training = is_training\n",
    "        self.num_outputs = num_outputs\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        relu = tf.nn.relu\n",
    "        self.conv_01 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, padding='same', kernel_initializer=k_init, activation=relu) \n",
    "        self.conv_02 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=4, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.conv_03 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(256, kernel_initializer=k_init, activation=relu)\n",
    "        self.output_layer = tf.keras.layers.Dense(num_outputs, kernel_initializer=k_init)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = inputs\n",
    "        outputs = self.conv_01(outputs)\n",
    "        outputs = self.conv_02(outputs)\n",
    "        outputs = self.conv_03(outputs)\n",
    "        outputs = self.flatten(outputs)\n",
    "        outputs = self.dense(outputs)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, actions, epsilon, input_shape, learning_rate=0.0001):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.input_shape = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self._teacher_model = None\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.build()\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        \n",
    "    def save(self, model_path):\n",
    "        self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        input_shape = (env.width, env.height, env.frame_count)\n",
    "        agent = cls(actions, epsilon, input_shape)\n",
    "        agent.initialize()\n",
    "        agent.model.load_weights(model_path)\n",
    "        return agent\n",
    "        \n",
    "    def build(self):\n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "        model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "        outputs = model(inputs)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # teacher_model を更新するため、両方のモデルで一度計算し重みを取得する\n",
    "        self._teacher_model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "        dummy = np.random.randn(1, *self.input_shape).astype(np.float32)\n",
    "        dummy = tf.convert_to_tensor(dummy)\n",
    "        _ = self.model.call(dummy)\n",
    "        _ = self._teacher_model.call(dummy)\n",
    "        self.update_teacher()\n",
    "    \n",
    "    def policy(self, state) -> int:\n",
    "        '''\n",
    "        epsilon greedy で受け取った state をもとに行動を決定する\n",
    "        '''\n",
    "        # epsilon greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(state)\n",
    "            return np.argmax(estimates)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        '''\n",
    "        ある state の状態価値を推定する\n",
    "        '''\n",
    "        state_as_batch = np.array([state])\n",
    "        return self.model.predict(state_as_batch)[0]\n",
    "    \n",
    "    def update(self, experiences, gamma):\n",
    "        '''\n",
    "        与えられた experiences をもとに学習\n",
    "        '''\n",
    "        states = np.array([e.state for e in experiences])\n",
    "        next_states = np.array([e.next_state for e in experiences])\n",
    "\n",
    "        estimated_values = self.model.predict(states)\n",
    "        next_state_values = self._teacher_model.predict(next_states)\n",
    "        \n",
    "        # train\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.reward\n",
    "            if not e.done:\n",
    "                # DQN と違い、action の推定は学習中のモデルで、その価値の推定は target network で行う\n",
    "                next_action = np.argmax(estimated_values[i])\n",
    "                reward += gamma * next_state_values[i][next_action]\n",
    "            estimated_values[i][e.action] = reward\n",
    "        loss = self.model.train_on_batch(states, estimated_values)\n",
    "        return loss\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def play(self, env, episode_count: int = 2, render: bool = True):\n",
    "        total_rewards = []\n",
    "        for e in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step_with_raw_reward(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                episode_reward += step_reward\n",
    "                state = next_state\n",
    "            print('episode {}, total reward: {:.4f}'.format(e, episode_reward))\n",
    "            total_rewards.append(episode_reward)\n",
    "                \n",
    "        env.reset()\n",
    "        print('average reward by {}: {:.4f}'.format(episode_count, np.mean(total_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer:\n",
    "    \n",
    "    def __init__(self, env, frame_count, width, height, render=False, outdir='./playlogs/'):\n",
    "        self._env = env\n",
    "        if render:\n",
    "            outdir = outdir\n",
    "            env = Monitor(env, directory=outdir, video_callable=(lambda x: x % 5 == 0), force=True)\n",
    "            self._env = env\n",
    "        self.frame_count = frame_count\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "        \n",
    "    def render(self):\n",
    "        self._env.render(mode = 'rgb_array')\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), self.reward_shaping(reward), done\n",
    "    \n",
    "    def step_with_raw_reward(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), reward, done\n",
    "        \n",
    "    def transform(self, state):\n",
    "        grayed = Image.fromarray(state).convert('L')  # h, w, c -> h, w\n",
    "        \n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(np.float32)\n",
    "        resized = np.transpose(resized, (1, 0)) # h, w -> w, h\n",
    "        normalized = resized / 255.0\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        feature = np.transpose(feature, (1, 2, 0))  # [f, w, h] -> [w, h, f]\n",
    "        \n",
    "        return feature\n",
    "\n",
    "    def reward_shaping(self, reward):\n",
    "        if 0.01 > reward > -0.01:\n",
    "            return 0.01\n",
    "        elif reward <= -0.1:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, filename, buffer_size=20000, batch_size=32, gamma=0.98, report_interval=20):\n",
    "        self.file_name = filename\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        self.logger = Logger(dir_name=filename)\n",
    "        self._max_reward = 0\n",
    "        self.teacher_update_freq = 2\n",
    "        \n",
    "    def train(self, env, episode_count=10000, initial_count=200, model_path = None):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if model_path:\n",
    "            agent = Agent.load(env, model_path, epsilon=0.1)\n",
    "        else:\n",
    "            agent = Agent(actions, 0.1, input_shape=(env.width, env.height, env.frame_count))\n",
    "        self.train_loop(env, agent, episode_count, initial_count)\n",
    "        \n",
    "    def train_loop(self, env, agent, episode_count, initial_count):\n",
    "        \n",
    "        for episode in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                e = Experience(state, action, step_reward, next_state, done)\n",
    "                self.experiences.append(e)\n",
    "                episode_reward += step_reward\n",
    "                loss = self.step(episode, agent)\n",
    "                state = next_state\n",
    "                \n",
    "                if not self.training and (len(self.experiences) >= self.buffer_size or episode >= initial_count):\n",
    "                    self.begin_training(agent)\n",
    "                    self.training = True\n",
    "            \n",
    "            self.end_episode(episode, episode_reward, loss, agent)\n",
    "    \n",
    "    def step(self, step, agent):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            loss = agent.update(batch, self.gamma)\n",
    "            self.training_count += 1\n",
    "            return loss\n",
    "            \n",
    "    def begin_training(self, agent):\n",
    "        print('start training!')\n",
    "        self.logger.set_model(agent.model)\n",
    "        \n",
    "    def end_episode(self, episode, reward, loss, agent):\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.ckpt_path)\n",
    "                self._max_reward = reward\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)\n",
    "        \n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'double_dqn_0323'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/flappybird-try-wqQoCzq9/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 288, 512)\n",
    "trainer = Trainer(filename=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 20, reward is 0.193 (+/-0.031)\n",
      "At episode 40, reward is 0.221 (+/-0.098)\n",
      "At episode 60, reward is 0.224 (+/-0.085)\n",
      "At episode 80, reward is 0.206 (+/-0.052)\n",
      "At episode 100, reward is 0.226 (+/-0.094)\n",
      "At episode 120, reward is 0.185 (+/-0.022)\n",
      "At episode 140, reward is 0.219 (+/-0.084)\n",
      "At episode 160, reward is 0.195 (+/-0.039)\n",
      "At episode 180, reward is 0.193 (+/-0.031)\n",
      "start training!\n",
      "At episode 200, reward is 0.241 (+/-0.098)\n",
      "At episode 220, reward is 0.421 (+/-0.403)\n",
      "At episode 240, reward is 0.978 (+/-0.597)\n",
      "At episode 260, reward is 0.789 (+/-0.428)\n",
      "At episode 280, reward is 0.788 (+/-0.423)\n",
      "At episode 300, reward is 0.61 (+/-0.0)\n",
      "At episode 320, reward is 0.719 (+/-0.324)\n",
      "At episode 340, reward is 0.959 (+/-0.532)\n",
      "At episode 360, reward is 1.228 (+/-0.849)\n",
      "At episode 380, reward is 0.975 (+/-0.553)\n",
      "At episode 400, reward is 1.026 (+/-0.571)\n",
      "At episode 420, reward is 1.139 (+/-0.758)\n",
      "At episode 440, reward is 1.106 (+/-0.842)\n",
      "At episode 460, reward is 1.211 (+/-0.992)\n",
      "At episode 480, reward is 1.386 (+/-0.961)\n",
      "At episode 500, reward is 1.044 (+/-0.908)\n",
      "At episode 520, reward is 1.592 (+/-1.297)\n",
      "At episode 540, reward is 1.402 (+/-1.07)\n",
      "At episode 560, reward is 1.289 (+/-1.174)\n",
      "At episode 580, reward is 1.626 (+/-1.023)\n",
      "At episode 600, reward is 1.166 (+/-0.615)\n",
      "At episode 620, reward is 1.543 (+/-1.021)\n",
      "At episode 640, reward is 1.696 (+/-1.116)\n",
      "At episode 660, reward is 1.948 (+/-1.437)\n",
      "At episode 680, reward is 1.659 (+/-1.068)\n",
      "At episode 700, reward is 2.105 (+/-1.686)\n",
      "At episode 720, reward is 2.796 (+/-2.081)\n",
      "At episode 740, reward is 1.952 (+/-1.329)\n",
      "At episode 760, reward is 2.107 (+/-1.371)\n",
      "At episode 780, reward is 3.256 (+/-2.347)\n",
      "At episode 800, reward is 2.462 (+/-1.918)\n",
      "At episode 820, reward is 2.952 (+/-1.824)\n",
      "At episode 840, reward is 4.028 (+/-2.839)\n",
      "At episode 860, reward is 3.395 (+/-2.641)\n",
      "At episode 880, reward is 3.436 (+/-3.99)\n",
      "At episode 900, reward is 3.818 (+/-2.532)\n",
      "At episode 920, reward is 4.471 (+/-3.299)\n",
      "At episode 940, reward is 3.484 (+/-2.425)\n",
      "At episode 960, reward is 4.76 (+/-3.635)\n",
      "At episode 980, reward is 4.12 (+/-4.018)\n",
      "At episode 1000, reward is 5.12 (+/-3.19)\n",
      "At episode 1020, reward is 4.313 (+/-4.231)\n",
      "At episode 1040, reward is 4.908 (+/-3.255)\n",
      "At episode 1060, reward is 7.239 (+/-4.831)\n",
      "At episode 1080, reward is 6.488 (+/-4.593)\n",
      "At episode 1100, reward is 9.326 (+/-7.313)\n",
      "At episode 1120, reward is 7.115 (+/-4.974)\n",
      "At episode 1140, reward is 5.133 (+/-2.67)\n",
      "At episode 1160, reward is 11.116 (+/-10.052)\n",
      "At episode 1180, reward is 9.896 (+/-8.936)\n",
      "At episode 1200, reward is 7.548 (+/-7.593)\n",
      "At episode 1220, reward is 11.014 (+/-11.061)\n",
      "At episode 1240, reward is 7.786 (+/-6.101)\n",
      "At episode 1260, reward is 10.822 (+/-10.112)\n",
      "At episode 1280, reward is 7.623 (+/-8.913)\n",
      "At episode 1300, reward is 10.227 (+/-7.773)\n",
      "At episode 1320, reward is 11.195 (+/-6.817)\n",
      "At episode 1340, reward is 11.142 (+/-9.546)\n",
      "At episode 1360, reward is 11.34 (+/-10.088)\n",
      "At episode 1380, reward is 8.564 (+/-7.771)\n",
      "At episode 1400, reward is 15.676 (+/-15.124)\n",
      "At episode 1420, reward is 11.155 (+/-12.541)\n",
      "At episode 1440, reward is 10.666 (+/-8.224)\n",
      "At episode 1460, reward is 9.67 (+/-12.118)\n",
      "At episode 1480, reward is 14.858 (+/-14.417)\n",
      "At episode 1500, reward is 10.621 (+/-9.119)\n",
      "At episode 1520, reward is 10.944 (+/-9.188)\n",
      "At episode 1540, reward is 10.749 (+/-14.278)\n",
      "At episode 1560, reward is 9.909 (+/-10.752)\n",
      "At episode 1580, reward is 10.203 (+/-7.892)\n",
      "At episode 1600, reward is 12.453 (+/-11.611)\n",
      "At episode 1620, reward is 12.161 (+/-12.271)\n",
      "At episode 1640, reward is 13.858 (+/-11.23)\n",
      "At episode 1660, reward is 10.491 (+/-9.513)\n",
      "At episode 1680, reward is 8.326 (+/-6.676)\n",
      "At episode 1700, reward is 12.736 (+/-8.25)\n",
      "At episode 1720, reward is 9.014 (+/-9.57)\n",
      "At episode 1740, reward is 12.291 (+/-11.943)\n",
      "At episode 1760, reward is 12.402 (+/-11.474)\n",
      "At episode 1780, reward is 6.218 (+/-5.856)\n",
      "At episode 1800, reward is 10.33 (+/-10.919)\n",
      "At episode 1820, reward is 9.86 (+/-8.024)\n",
      "At episode 1840, reward is 11.177 (+/-9.791)\n",
      "At episode 1860, reward is 8.482 (+/-9.67)\n",
      "At episode 1880, reward is 11.879 (+/-7.434)\n",
      "At episode 1900, reward is 11.092 (+/-8.399)\n",
      "At episode 1920, reward is 9.511 (+/-9.888)\n",
      "At episode 1940, reward is 8.073 (+/-6.849)\n",
      "At episode 1960, reward is 9.708 (+/-9.423)\n",
      "At episode 1980, reward is 9.389 (+/-6.865)\n",
      "At episode 2000, reward is 10.628 (+/-9.152)\n",
      "At episode 2020, reward is 10.415 (+/-11.931)\n",
      "At episode 2040, reward is 11.255 (+/-7.225)\n",
      "At episode 2060, reward is 11.076 (+/-9.15)\n",
      "At episode 2080, reward is 9.948 (+/-11.97)\n",
      "At episode 2100, reward is 5.508 (+/-5.688)\n",
      "At episode 2120, reward is 8.681 (+/-6.928)\n",
      "At episode 2140, reward is 10.595 (+/-11.999)\n",
      "At episode 2160, reward is 9.856 (+/-12.109)\n",
      "At episode 2180, reward is 9.174 (+/-7.475)\n",
      "At episode 2200, reward is 11.73 (+/-10.609)\n",
      "At episode 2220, reward is 6.606 (+/-5.896)\n",
      "At episode 2240, reward is 13.535 (+/-10.718)\n",
      "At episode 2260, reward is 6.731 (+/-5.823)\n",
      "At episode 2280, reward is 9.706 (+/-8.111)\n",
      "At episode 2300, reward is 11.849 (+/-9.14)\n",
      "At episode 2320, reward is 7.349 (+/-9.094)\n",
      "At episode 2340, reward is 10.59 (+/-8.362)\n",
      "At episode 2360, reward is 12.446 (+/-8.686)\n",
      "At episode 2380, reward is 10.65 (+/-8.484)\n",
      "At episode 2400, reward is 8.849 (+/-6.127)\n",
      "At episode 2420, reward is 10.561 (+/-11.085)\n",
      "At episode 2440, reward is 8.352 (+/-7.76)\n",
      "At episode 2460, reward is 10.809 (+/-7.576)\n",
      "At episode 2480, reward is 7.022 (+/-7.087)\n",
      "At episode 2500, reward is 10.599 (+/-14.424)\n",
      "At episode 2520, reward is 6.601 (+/-6.463)\n",
      "At episode 2540, reward is 12.135 (+/-9.382)\n",
      "At episode 2560, reward is 6.791 (+/-6.566)\n",
      "At episode 2580, reward is 14.562 (+/-16.635)\n",
      "At episode 2600, reward is 9.812 (+/-9.649)\n",
      "At episode 2620, reward is 7.603 (+/-6.642)\n",
      "At episode 2640, reward is 6.729 (+/-5.639)\n",
      "At episode 2660, reward is 8.498 (+/-9.713)\n",
      "At episode 2680, reward is 9.82 (+/-6.447)\n",
      "At episode 2700, reward is 9.304 (+/-13.863)\n",
      "At episode 2720, reward is 11.957 (+/-9.023)\n",
      "At episode 2740, reward is 9.404 (+/-8.445)\n",
      "At episode 2760, reward is 6.371 (+/-6.195)\n",
      "At episode 2780, reward is 9.045 (+/-8.283)\n",
      "At episode 2800, reward is 13.767 (+/-8.014)\n",
      "At episode 2820, reward is 11.363 (+/-10.573)\n",
      "At episode 2840, reward is 11.478 (+/-15.156)\n",
      "At episode 2860, reward is 13.508 (+/-13.956)\n",
      "At episode 2880, reward is 9.04 (+/-8.161)\n",
      "At episode 2900, reward is 10.32 (+/-9.832)\n",
      "At episode 2920, reward is 8.305 (+/-6.231)\n",
      "At episode 2940, reward is 6.496 (+/-5.322)\n",
      "At episode 2960, reward is 13.365 (+/-15.617)\n",
      "At episode 2980, reward is 10.162 (+/-12.728)\n",
      "At episode 3000, reward is 12.589 (+/-10.399)\n",
      "At episode 3020, reward is 12.35 (+/-11.202)\n",
      "At episode 3040, reward is 9.006 (+/-6.889)\n",
      "At episode 3060, reward is 9.156 (+/-10.734)\n",
      "At episode 3080, reward is 7.53 (+/-7.489)\n",
      "At episode 3100, reward is 11.117 (+/-11.5)\n",
      "At episode 3120, reward is 9.525 (+/-10.042)\n",
      "At episode 3140, reward is 16.319 (+/-12.269)\n",
      "At episode 3160, reward is 13.788 (+/-12.608)\n",
      "At episode 3180, reward is 10.782 (+/-7.581)\n",
      "At episode 3200, reward is 15.824 (+/-15.004)\n",
      "At episode 3220, reward is 9.032 (+/-12.525)\n",
      "At episode 3240, reward is 13.287 (+/-14.884)\n",
      "At episode 3260, reward is 12.661 (+/-11.669)\n",
      "At episode 3280, reward is 8.818 (+/-8.573)\n",
      "At episode 3300, reward is 12.544 (+/-11.933)\n",
      "At episode 3320, reward is 10.738 (+/-8.395)\n",
      "At episode 3340, reward is 10.509 (+/-11.382)\n",
      "At episode 3360, reward is 11.596 (+/-9.068)\n",
      "At episode 3380, reward is 12.444 (+/-13.625)\n",
      "At episode 3400, reward is 11.403 (+/-12.034)\n",
      "At episode 3420, reward is 8.979 (+/-8.787)\n",
      "At episode 3440, reward is 4.036 (+/-3.371)\n",
      "At episode 3460, reward is 7.402 (+/-4.844)\n",
      "At episode 3480, reward is 7.278 (+/-8.344)\n",
      "At episode 3500, reward is 11.173 (+/-7.465)\n",
      "At episode 3520, reward is 13.991 (+/-13.267)\n",
      "At episode 3540, reward is 6.391 (+/-4.551)\n",
      "At episode 3560, reward is 11.042 (+/-10.679)\n",
      "At episode 3580, reward is 13.914 (+/-18.222)\n",
      "At episode 3600, reward is 15.471 (+/-15.393)\n",
      "At episode 3620, reward is 17.964 (+/-17.713)\n",
      "At episode 3640, reward is 11.318 (+/-8.639)\n",
      "At episode 3660, reward is 8.94 (+/-7.396)\n",
      "At episode 3680, reward is 9.211 (+/-8.654)\n",
      "At episode 3700, reward is 15.699 (+/-14.814)\n",
      "At episode 3720, reward is 7.897 (+/-7.317)\n",
      "At episode 3740, reward is 10.489 (+/-6.708)\n",
      "At episode 3760, reward is 15.678 (+/-11.58)\n",
      "At episode 3780, reward is 11.989 (+/-12.078)\n",
      "At episode 3800, reward is 11.126 (+/-7.801)\n",
      "At episode 3820, reward is 7.188 (+/-6.479)\n",
      "At episode 3840, reward is 15.468 (+/-15.645)\n",
      "At episode 3860, reward is 10.69 (+/-8.311)\n",
      "At episode 3880, reward is 11.097 (+/-12.155)\n",
      "At episode 3900, reward is 9.044 (+/-8.318)\n",
      "At episode 3920, reward is 10.132 (+/-9.358)\n",
      "At episode 3940, reward is 15.361 (+/-16.178)\n",
      "At episode 3960, reward is 10.364 (+/-8.941)\n",
      "At episode 3980, reward is 10.595 (+/-10.687)\n",
      "At episode 4000, reward is 9.769 (+/-7.471)\n",
      "At episode 4020, reward is 12.162 (+/-10.299)\n",
      "At episode 4040, reward is 10.844 (+/-8.877)\n",
      "At episode 4060, reward is 14.466 (+/-15.829)\n",
      "At episode 4080, reward is 7.89 (+/-6.356)\n",
      "At episode 4100, reward is 10.052 (+/-11.115)\n",
      "At episode 4120, reward is 11.789 (+/-13.685)\n",
      "At episode 4140, reward is 12.08 (+/-9.722)\n",
      "At episode 4160, reward is 11.957 (+/-11.817)\n",
      "At episode 4180, reward is 11.576 (+/-10.467)\n",
      "At episode 4200, reward is 10.649 (+/-12.84)\n",
      "At episode 4220, reward is 12.061 (+/-13.657)\n",
      "At episode 4240, reward is 7.471 (+/-7.794)\n",
      "At episode 4260, reward is 8.823 (+/-12.658)\n",
      "At episode 4280, reward is 11.525 (+/-8.226)\n",
      "At episode 4300, reward is 10.614 (+/-14.882)\n",
      "At episode 4320, reward is 6.05 (+/-5.16)\n",
      "At episode 4340, reward is 9.572 (+/-12.124)\n",
      "At episode 4360, reward is 7.929 (+/-10.027)\n",
      "At episode 4380, reward is 9.65 (+/-10.35)\n",
      "At episode 4400, reward is 9.756 (+/-9.269)\n",
      "At episode 4420, reward is 6.953 (+/-6.396)\n",
      "At episode 4440, reward is 10.666 (+/-11.507)\n",
      "At episode 4460, reward is 10.463 (+/-10.988)\n",
      "At episode 4480, reward is 9.785 (+/-9.387)\n",
      "At episode 4500, reward is 12.796 (+/-11.263)\n",
      "At episode 4520, reward is 9.534 (+/-8.095)\n",
      "At episode 4540, reward is 14.129 (+/-16.419)\n",
      "At episode 4560, reward is 5.842 (+/-6.473)\n",
      "At episode 4580, reward is 8.075 (+/-6.475)\n",
      "At episode 4600, reward is 8.262 (+/-8.403)\n",
      "At episode 4620, reward is 14.289 (+/-9.92)\n",
      "At episode 4640, reward is 11.371 (+/-10.517)\n",
      "At episode 4660, reward is 14.068 (+/-14.028)\n",
      "At episode 4680, reward is 8.176 (+/-6.301)\n",
      "At episode 4700, reward is 12.511 (+/-9.107)\n",
      "At episode 4720, reward is 10.855 (+/-9.351)\n",
      "At episode 4740, reward is 13.13 (+/-8.758)\n",
      "At episode 4760, reward is 18.103 (+/-13.607)\n",
      "At episode 4780, reward is 12.327 (+/-11.565)\n",
      "At episode 4800, reward is 10.075 (+/-7.874)\n",
      "At episode 4820, reward is 11.739 (+/-11.805)\n",
      "At episode 4840, reward is 10.296 (+/-10.191)\n",
      "At episode 4860, reward is 11.683 (+/-9.154)\n",
      "At episode 4880, reward is 12.032 (+/-13.14)\n",
      "At episode 4900, reward is 10.962 (+/-11.254)\n",
      "At episode 4920, reward is 10.327 (+/-9.21)\n",
      "At episode 4940, reward is 12.198 (+/-10.301)\n",
      "At episode 4960, reward is 8.594 (+/-8.328)\n",
      "At episode 4980, reward is 11.221 (+/-9.6)\n",
      "At episode 5000, reward is 10.63 (+/-10.223)\n",
      "At episode 5020, reward is 10.435 (+/-12.072)\n",
      "At episode 5040, reward is 8.389 (+/-7.978)\n",
      "At episode 5060, reward is 14.73 (+/-12.247)\n",
      "At episode 5080, reward is 6.72 (+/-5.913)\n",
      "At episode 5100, reward is 14.365 (+/-12.209)\n",
      "At episode 5120, reward is 9.816 (+/-9.624)\n",
      "At episode 5140, reward is 10.549 (+/-9.613)\n",
      "At episode 5160, reward is 10.937 (+/-10.265)\n",
      "At episode 5180, reward is 10.68 (+/-12.621)\n",
      "At episode 5200, reward is 9.242 (+/-10.52)\n",
      "At episode 5220, reward is 11.727 (+/-10.024)\n",
      "At episode 5240, reward is 7.374 (+/-6.741)\n",
      "At episode 5260, reward is 9.404 (+/-9.194)\n",
      "At episode 5280, reward is 7.447 (+/-7.77)\n",
      "At episode 5300, reward is 8.888 (+/-5.098)\n",
      "At episode 5320, reward is 11.018 (+/-10.089)\n",
      "At episode 5340, reward is 9.997 (+/-9.516)\n",
      "At episode 5360, reward is 10.671 (+/-8.892)\n",
      "At episode 5380, reward is 11.887 (+/-11.089)\n",
      "At episode 5400, reward is 8.264 (+/-9.035)\n",
      "At episode 5420, reward is 9.214 (+/-10.032)\n",
      "At episode 5440, reward is 5.945 (+/-4.651)\n",
      "At episode 5460, reward is 9.307 (+/-7.949)\n",
      "At episode 5480, reward is 9.152 (+/-10.174)\n",
      "At episode 5500, reward is 17.327 (+/-14.393)\n",
      "At episode 5520, reward is 12.435 (+/-10.005)\n",
      "At episode 5540, reward is 8.315 (+/-7.499)\n",
      "At episode 5560, reward is 15.167 (+/-17.083)\n",
      "At episode 5580, reward is 9.746 (+/-11.536)\n",
      "At episode 5600, reward is 8.742 (+/-8.785)\n",
      "At episode 5620, reward is 12.908 (+/-12.976)\n",
      "At episode 5640, reward is 7.643 (+/-7.972)\n",
      "At episode 5660, reward is 12.517 (+/-15.209)\n",
      "At episode 5680, reward is 10.015 (+/-7.816)\n",
      "At episode 5700, reward is 12.675 (+/-12.975)\n",
      "At episode 5720, reward is 6.085 (+/-5.281)\n",
      "At episode 5740, reward is 9.171 (+/-7.456)\n",
      "At episode 5760, reward is 5.626 (+/-6.044)\n",
      "At episode 5780, reward is 9.323 (+/-9.149)\n",
      "At episode 5800, reward is 7.278 (+/-7.626)\n",
      "At episode 5820, reward is 10.55 (+/-8.157)\n",
      "At episode 5840, reward is 9.568 (+/-7.146)\n",
      "At episode 5860, reward is 13.255 (+/-12.115)\n",
      "At episode 5880, reward is 12.313 (+/-12.653)\n",
      "At episode 5900, reward is 14.976 (+/-9.592)\n",
      "At episode 5920, reward is 6.919 (+/-5.827)\n",
      "At episode 5940, reward is 9.671 (+/-9.457)\n",
      "At episode 5960, reward is 13.283 (+/-9.953)\n",
      "At episode 5980, reward is 11.138 (+/-8.941)\n",
      "At episode 6000, reward is 8.383 (+/-7.132)\n",
      "At episode 6020, reward is 9.034 (+/-7.959)\n",
      "At episode 6040, reward is 10.323 (+/-7.008)\n",
      "At episode 6060, reward is 9.234 (+/-8.046)\n",
      "At episode 6080, reward is 11.572 (+/-10.865)\n",
      "At episode 6100, reward is 11.169 (+/-11.794)\n",
      "At episode 6120, reward is 9.906 (+/-10.378)\n",
      "At episode 6140, reward is 10.004 (+/-13.287)\n",
      "At episode 6160, reward is 12.61 (+/-11.615)\n",
      "At episode 6180, reward is 5.911 (+/-4.356)\n",
      "At episode 6200, reward is 9.067 (+/-6.669)\n",
      "At episode 6220, reward is 10.42 (+/-9.007)\n",
      "At episode 6240, reward is 8.629 (+/-6.94)\n",
      "At episode 6260, reward is 9.855 (+/-5.782)\n",
      "At episode 6280, reward is 9.484 (+/-11.435)\n",
      "At episode 6300, reward is 9.358 (+/-7.362)\n",
      "At episode 6320, reward is 7.704 (+/-8.933)\n",
      "At episode 6340, reward is 9.396 (+/-8.15)\n",
      "At episode 6360, reward is 14.473 (+/-12.636)\n",
      "At episode 6380, reward is 8.596 (+/-10.482)\n",
      "At episode 6400, reward is 5.87 (+/-4.085)\n",
      "At episode 6420, reward is 16.47 (+/-13.453)\n",
      "At episode 6440, reward is 12.233 (+/-11.064)\n",
      "At episode 6460, reward is 6.883 (+/-4.963)\n",
      "At episode 6480, reward is 9.592 (+/-6.679)\n",
      "At episode 6500, reward is 13.625 (+/-11.569)\n",
      "At episode 6520, reward is 11.426 (+/-9.745)\n",
      "At episode 6540, reward is 10.289 (+/-14.516)\n",
      "At episode 6560, reward is 11.648 (+/-10.875)\n",
      "At episode 6580, reward is 7.093 (+/-6.678)\n",
      "At episode 6600, reward is 9.273 (+/-11.601)\n",
      "At episode 6620, reward is 9.975 (+/-8.261)\n",
      "At episode 6640, reward is 9.304 (+/-7.504)\n",
      "At episode 6660, reward is 13.84 (+/-10.302)\n",
      "At episode 6680, reward is 10.406 (+/-9.994)\n",
      "At episode 6700, reward is 10.26 (+/-9.834)\n",
      "At episode 6720, reward is 7.475 (+/-4.819)\n",
      "At episode 6740, reward is 11.352 (+/-10.534)\n",
      "At episode 6760, reward is 12.08 (+/-10.821)\n",
      "At episode 6780, reward is 12.764 (+/-10.237)\n",
      "At episode 6800, reward is 8.493 (+/-7.872)\n",
      "At episode 6820, reward is 9.552 (+/-8.593)\n",
      "At episode 6840, reward is 8.089 (+/-8.611)\n",
      "At episode 6860, reward is 8.384 (+/-5.602)\n",
      "At episode 6880, reward is 9.196 (+/-7.163)\n",
      "At episode 6900, reward is 11.728 (+/-15.418)\n",
      "At episode 6920, reward is 11.669 (+/-10.139)\n",
      "At episode 6940, reward is 14.697 (+/-9.749)\n",
      "At episode 6960, reward is 9.634 (+/-8.3)\n",
      "At episode 6980, reward is 9.999 (+/-7.997)\n",
      "At episode 7000, reward is 13.398 (+/-11.487)\n",
      "At episode 7020, reward is 11.331 (+/-8.455)\n",
      "At episode 7040, reward is 11.044 (+/-12.747)\n",
      "At episode 7060, reward is 14.772 (+/-12.876)\n",
      "At episode 7080, reward is 10.676 (+/-10.016)\n",
      "At episode 7100, reward is 10.002 (+/-10.571)\n",
      "At episode 7120, reward is 17.612 (+/-20.379)\n",
      "At episode 7140, reward is 8.852 (+/-6.079)\n",
      "At episode 7160, reward is 12.79 (+/-9.334)\n",
      "At episode 7180, reward is 14.811 (+/-13.452)\n",
      "At episode 7200, reward is 10.515 (+/-10.338)\n",
      "At episode 7220, reward is 12.557 (+/-10.216)\n",
      "At episode 7240, reward is 11.455 (+/-12.795)\n",
      "At episode 7260, reward is 10.745 (+/-10.174)\n",
      "At episode 7280, reward is 10.738 (+/-10.709)\n",
      "At episode 7300, reward is 16.352 (+/-15.836)\n",
      "At episode 7320, reward is 10.916 (+/-12.364)\n",
      "At episode 7340, reward is 11.733 (+/-12.373)\n",
      "At episode 7360, reward is 8.371 (+/-7.096)\n",
      "At episode 7380, reward is 7.616 (+/-5.005)\n",
      "At episode 7400, reward is 11.125 (+/-8.469)\n",
      "At episode 7420, reward is 10.671 (+/-9.023)\n",
      "At episode 7440, reward is 11.111 (+/-10.471)\n",
      "At episode 7460, reward is 9.843 (+/-8.218)\n",
      "At episode 7480, reward is 11.149 (+/-11.009)\n",
      "At episode 7500, reward is 13.732 (+/-10.713)\n",
      "At episode 7520, reward is 12.544 (+/-16.513)\n",
      "At episode 7540, reward is 7.275 (+/-4.276)\n",
      "At episode 7560, reward is 11.16 (+/-10.363)\n",
      "At episode 7580, reward is 8.281 (+/-8.309)\n",
      "At episode 7600, reward is 16.802 (+/-15.895)\n",
      "At episode 7620, reward is 8.507 (+/-7.639)\n",
      "At episode 7640, reward is 10.161 (+/-12.182)\n",
      "At episode 7660, reward is 14.106 (+/-13.41)\n",
      "At episode 7680, reward is 9.276 (+/-8.649)\n",
      "At episode 7700, reward is 10.873 (+/-9.502)\n",
      "At episode 7720, reward is 9.641 (+/-9.209)\n",
      "At episode 7740, reward is 9.668 (+/-11.832)\n",
      "At episode 7760, reward is 8.651 (+/-8.498)\n",
      "At episode 7780, reward is 7.615 (+/-7.877)\n",
      "At episode 7800, reward is 13.953 (+/-10.167)\n",
      "At episode 7820, reward is 13.957 (+/-12.511)\n",
      "At episode 7840, reward is 10.165 (+/-12.53)\n",
      "At episode 7860, reward is 10.666 (+/-7.346)\n",
      "At episode 7880, reward is 16.579 (+/-17.227)\n",
      "At episode 7900, reward is 7.487 (+/-4.509)\n",
      "At episode 7920, reward is 11.755 (+/-9.742)\n",
      "At episode 7940, reward is 11.513 (+/-12.178)\n",
      "At episode 7960, reward is 16.395 (+/-14.246)\n",
      "At episode 7980, reward is 9.225 (+/-7.8)\n",
      "At episode 8000, reward is 10.578 (+/-8.189)\n",
      "At episode 8020, reward is 12.269 (+/-11.009)\n",
      "At episode 8040, reward is 9.244 (+/-8.379)\n",
      "At episode 8060, reward is 10.79 (+/-9.423)\n",
      "At episode 8080, reward is 11.536 (+/-9.44)\n",
      "At episode 8100, reward is 16.34 (+/-19.122)\n",
      "At episode 8120, reward is 12.081 (+/-17.343)\n",
      "At episode 8140, reward is 11.164 (+/-13.82)\n",
      "At episode 8160, reward is 10.977 (+/-10.309)\n",
      "At episode 8180, reward is 7.49 (+/-7.899)\n",
      "At episode 8200, reward is 9.174 (+/-9.436)\n",
      "At episode 8220, reward is 8.847 (+/-7.747)\n",
      "At episode 8240, reward is 10.865 (+/-8.173)\n",
      "At episode 8260, reward is 10.866 (+/-8.469)\n",
      "At episode 8280, reward is 10.139 (+/-9.723)\n",
      "At episode 8300, reward is 7.514 (+/-4.735)\n",
      "At episode 8320, reward is 10.155 (+/-7.882)\n",
      "At episode 8340, reward is 7.017 (+/-7.977)\n",
      "At episode 8360, reward is 7.118 (+/-5.678)\n",
      "At episode 8380, reward is 10.783 (+/-8.593)\n",
      "At episode 8400, reward is 11.8 (+/-15.671)\n",
      "At episode 8420, reward is 9.152 (+/-9.718)\n",
      "At episode 8440, reward is 9.22 (+/-8.962)\n"
     ]
    }
   ],
   "source": [
    "# trainer.train(obs, model_path='logs/{}/model.ckpt'.format(model_name))\n",
    "trainer.train(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 288, 512, render=True)\n",
    "# obs = Observer(env, 4, 144, 256, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
