{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import logging\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "\n",
    "# gym\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import gym_ple\n",
    "\n",
    "# disply\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select gpu\n",
    "gpu_index = 1\n",
    "from tensorflow.keras.backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=str(gpu_index),\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    state: gym.spaces.Box\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: gym.spaces.Box\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir='./logs/', dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.keras.callbacks.TensorBoard(self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "    \n",
    "    @property\n",
    "    def ckpt_path(self):\n",
    "        return os.path.join(self.log_dir, 'model.ckpt')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, is_training: bool, num_actions: int):\n",
    "        super(AgentModel, self).__init__()\n",
    "        self.is_training = is_training\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        relu = tf.nn.relu\n",
    "        self.conv_01 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, padding='same', kernel_initializer=k_init, activation=relu) \n",
    "        self.conv_02 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=4, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.conv_03 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense_v = tf.keras.layers.Dense(128, kernel_initializer=k_init, activation=relu)\n",
    "        self.dense_ad = tf.keras.layers.Dense(128, kernel_initializer=k_init, activation=relu)\n",
    "        self.output_v_layer = tf.keras.layers.Dense(1, kernel_initializer=k_init)\n",
    "        self.output_ad_layer = tf.keras.layers.Dense(num_actions, kernel_initializer=k_init)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = inputs\n",
    "        outputs = self.conv_01(outputs)\n",
    "        outputs = self.conv_02(outputs)\n",
    "        outputs = self.conv_03(outputs)\n",
    "        outputs = self.flatten(outputs)\n",
    "        output_v = self.dense_v(outputs)\n",
    "        outputs_ad = self.dense_ad(outputs)\n",
    "        output_v = self.output_v_layer(output_v)\n",
    "        outputs_ad = self.output_ad_layer(outputs_ad)\n",
    "        q = output_v + (outputs_ad - tf.reduce_mean(outputs_ad))\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, actions, epsilon, input_shape, learning_rate=0.0001):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.input_shape = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self._teacher_model = None\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.build()\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        \n",
    "    def save(self, model_path):\n",
    "        self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        input_shape = (env.width, env.height, env.frame_count)\n",
    "        agent = cls(actions, epsilon, input_shape)\n",
    "        agent.initialize()\n",
    "        agent.model.load_weights(model_path)\n",
    "        return agent\n",
    "        \n",
    "    def build(self):\n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "        model = AgentModel(is_training=True, num_actions=len(self.actions))\n",
    "        outputs = model(inputs)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # teacher_model を更新するため、両方のモデルで一度計算し重みを取得する\n",
    "        self._teacher_model = AgentModel(is_training=True, num_actions=len(self.actions))\n",
    "        dummy = np.random.randn(1, *self.input_shape).astype(np.float32)\n",
    "        dummy = tf.convert_to_tensor(dummy)\n",
    "        _ = self.model.call(dummy)\n",
    "        _ = self._teacher_model.call(dummy)\n",
    "        self.update_teacher()\n",
    "    \n",
    "    def policy(self, state) -> int:\n",
    "        '''\n",
    "        epsilon greedy で受け取った state をもとに行動を決定する\n",
    "        '''\n",
    "        # epsilon greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(state)\n",
    "            return np.argmax(estimates)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        '''\n",
    "        ある state の状態価値を推定する\n",
    "        '''\n",
    "        state_as_batch = np.array([state])\n",
    "        return self.model.predict(state_as_batch)[0]\n",
    "    \n",
    "    def update(self, experiences, gamma):\n",
    "        '''\n",
    "        与えられた experiences をもとに学習\n",
    "        '''\n",
    "        states = np.array([e.state for e in experiences])\n",
    "        next_states = np.array([e.next_state for e in experiences])\n",
    "\n",
    "        estimated_values = self.model.predict(states)\n",
    "        next_state_values = self._teacher_model.predict(next_states)\n",
    "        \n",
    "        # train\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.reward\n",
    "            if not e.done:\n",
    "                # DQN と違い、action の推定は学習中のモデルで、その価値の推定は target network で行う\n",
    "                next_action = np.argmax(estimated_values[i])\n",
    "                reward += gamma * next_state_values[i][next_action]\n",
    "            estimated_values[i][e.action] = reward\n",
    "        loss = self.model.train_on_batch(states, estimated_values)\n",
    "        return loss\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def play(self, env, episode_count: int = 2, render: bool = True):\n",
    "        total_rewards = []\n",
    "        for e in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step_with_raw_reward(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                episode_reward += step_reward\n",
    "                state = next_state\n",
    "            print('episode {}, total reward: {:.4f}'.format(e, episode_reward))\n",
    "            total_rewards.append(episode_reward)\n",
    "                \n",
    "        env.reset()\n",
    "        print('reward by {}, mean: {:.4f}, std: {:.4f}'.format(episode_count, np.mean(total_rewards), np.std(total_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer:\n",
    "    \n",
    "    def __init__(self, env, frame_count, width, height, render=False, outdir='./playlogs/'):\n",
    "        self._env = env\n",
    "        if render:\n",
    "            outdir = outdir\n",
    "            env = Monitor(env, directory=outdir, video_callable=(lambda x: x % 5 == 0), force=True)\n",
    "            self._env = env\n",
    "        self.frame_count = frame_count\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "        \n",
    "    def render(self):\n",
    "        self._env.render(mode = 'rgb_array')\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), self.reward_shaping(reward), done\n",
    "    \n",
    "    def step_with_raw_reward(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), reward, done\n",
    "        \n",
    "    def transform(self, state):\n",
    "        state = state[:400, :, :]\n",
    "        grayed = Image.fromarray(state).convert('L')  # h, w, c -> h, w\n",
    "        \n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(np.float32)\n",
    "        resized = np.transpose(resized, (1, 0)) # h, w -> w, h\n",
    "        normalized = resized / 255.0\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        feature = np.transpose(feature, (1, 2, 0))  # [f, w, h] -> [w, h, f]\n",
    "        \n",
    "        return feature\n",
    "\n",
    "    def reward_shaping(self, reward):\n",
    "        if 0.01 > reward > -0.01:\n",
    "            return 0.01\n",
    "        elif reward <= -0.1:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, filename, buffer_size=50000, batch_size=32, gamma=0.98, report_interval=20):\n",
    "        self.file_name = filename\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        self.logger = Logger(dir_name=filename)\n",
    "        self._max_reward = 0\n",
    "        self.teacher_update_freq = 10000\n",
    "        \n",
    "    def train(self, env, episode_count=10000, initial_count=200, model_path = None):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if model_path:\n",
    "            agent = Agent.load(env, model_path, epsilon=0.1)\n",
    "        else:\n",
    "            agent = Agent(actions, 0.1, input_shape=(env.width, env.height, env.frame_count))\n",
    "        self.train_loop(env, agent, episode_count, initial_count)\n",
    "        \n",
    "    def train_loop(self, env, agent, episode_count, initial_count):\n",
    "        \n",
    "        for episode in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                e = Experience(state, action, step_reward, next_state, done)\n",
    "                self.experiences.append(e)\n",
    "                episode_reward += step_reward\n",
    "                loss = self.step(episode, agent)\n",
    "                state = next_state\n",
    "                \n",
    "                if not self.training and (len(self.experiences) >= self.buffer_size or episode >= initial_count):\n",
    "                    self.begin_training(agent)\n",
    "                    self.training = True\n",
    "            \n",
    "            self.end_episode(episode, episode_reward, loss, agent)\n",
    "    \n",
    "    def step(self, step, agent):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            loss = agent.update(batch, self.gamma)\n",
    "            self.training_count += 1\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "            return loss\n",
    "            \n",
    "    def begin_training(self, agent):\n",
    "        print('start training!')\n",
    "        self.logger.set_model(agent.model)\n",
    "        \n",
    "    def end_episode(self, episode, reward, loss, agent):\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.ckpt_path)\n",
    "                self._max_reward = reward\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)\n",
    "        \n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '03_duelingddqn_05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128)\n",
    "trainer = Trainer(filename=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 20, reward is 0.206 (+/-0.075)\n",
      "At episode 40, reward is 0.243 (+/-0.092)\n",
      "At episode 60, reward is 0.198 (+/-0.036)\n",
      "At episode 80, reward is 0.23 (+/-0.087)\n",
      "At episode 100, reward is 0.208 (+/-0.06)\n",
      "At episode 120, reward is 0.212 (+/-0.041)\n",
      "At episode 140, reward is 0.197 (+/-0.034)\n",
      "At episode 160, reward is 0.202 (+/-0.04)\n",
      "At episode 180, reward is 0.207 (+/-0.069)\n",
      "start training!\n",
      "At episode 200, reward is 0.233 (+/-0.085)\n",
      "At episode 220, reward is 0.224 (+/-0.096)\n",
      "At episode 240, reward is 0.778 (+/-0.426)\n",
      "At episode 260, reward is 0.926 (+/-0.581)\n",
      "At episode 280, reward is 0.852 (+/-0.515)\n",
      "At episode 300, reward is 0.932 (+/-0.735)\n",
      "At episode 320, reward is 0.86 (+/-0.554)\n",
      "At episode 340, reward is 0.736 (+/-0.415)\n",
      "At episode 360, reward is 0.766 (+/-0.436)\n",
      "At episode 380, reward is 0.624 (+/-0.329)\n",
      "At episode 400, reward is 0.703 (+/-0.433)\n",
      "At episode 420, reward is 0.638 (+/-0.244)\n",
      "At episode 440, reward is 0.808 (+/-0.64)\n",
      "At episode 460, reward is 0.556 (+/-0.115)\n",
      "At episode 480, reward is 0.669 (+/-0.299)\n",
      "At episode 500, reward is 0.811 (+/-0.486)\n",
      "At episode 520, reward is 0.81 (+/-0.664)\n",
      "At episode 540, reward is 0.591 (+/-0.047)\n",
      "At episode 560, reward is 0.615 (+/-0.331)\n",
      "At episode 580, reward is 0.714 (+/-0.667)\n",
      "At episode 600, reward is 0.729 (+/-0.369)\n",
      "At episode 620, reward is 0.699 (+/-0.379)\n",
      "At episode 640, reward is 0.746 (+/-0.405)\n",
      "At episode 660, reward is 0.622 (+/-0.274)\n",
      "At episode 680, reward is 0.747 (+/-0.453)\n",
      "At episode 700, reward is 0.73 (+/-0.476)\n",
      "At episode 720, reward is 0.777 (+/-0.398)\n",
      "At episode 740, reward is 0.849 (+/-0.537)\n",
      "At episode 760, reward is 0.745 (+/-0.405)\n",
      "At episode 780, reward is 0.928 (+/-0.985)\n",
      "At episode 800, reward is 0.564 (+/-0.093)\n",
      "At episode 820, reward is 0.792 (+/-0.467)\n",
      "At episode 840, reward is 1.139 (+/-0.747)\n",
      "At episode 860, reward is 1.191 (+/-1.276)\n",
      "At episode 880, reward is 0.854 (+/-0.485)\n",
      "At episode 900, reward is 0.834 (+/-0.441)\n",
      "At episode 920, reward is 0.916 (+/-0.532)\n",
      "At episode 940, reward is 0.925 (+/-0.776)\n",
      "At episode 960, reward is 0.678 (+/-0.294)\n",
      "At episode 980, reward is 0.806 (+/-0.461)\n",
      "At episode 1000, reward is 0.856 (+/-0.495)\n",
      "At episode 1020, reward is 0.916 (+/-0.681)\n",
      "At episode 1040, reward is 0.68 (+/-0.298)\n",
      "At episode 1060, reward is 0.734 (+/-0.374)\n",
      "At episode 1080, reward is 1.085 (+/-0.883)\n",
      "At episode 1100, reward is 0.663 (+/-0.306)\n",
      "At episode 1120, reward is 1.086 (+/-1.078)\n",
      "At episode 1140, reward is 0.727 (+/-0.363)\n",
      "At episode 1160, reward is 1.258 (+/-1.416)\n",
      "At episode 1180, reward is 0.869 (+/-0.679)\n",
      "At episode 1200, reward is 0.607 (+/-0.016)\n",
      "At episode 1220, reward is 0.677 (+/-0.294)\n",
      "At episode 1240, reward is 0.729 (+/-0.379)\n",
      "At episode 1260, reward is 1.073 (+/-1.244)\n",
      "At episode 1280, reward is 0.652 (+/-0.259)\n",
      "At episode 1300, reward is 0.86 (+/-0.502)\n",
      "At episode 1320, reward is 0.922 (+/-0.692)\n",
      "At episode 1340, reward is 0.993 (+/-0.952)\n",
      "At episode 1360, reward is 0.666 (+/-0.242)\n",
      "At episode 1380, reward is 0.866 (+/-0.515)\n",
      "At episode 1400, reward is 0.667 (+/-0.251)\n",
      "At episode 1420, reward is 0.774 (+/-0.582)\n",
      "At episode 1440, reward is 0.799 (+/-0.449)\n",
      "At episode 1460, reward is 1.098 (+/-0.71)\n",
      "At episode 1480, reward is 1.186 (+/-0.88)\n",
      "At episode 1500, reward is 1.155 (+/-0.763)\n",
      "At episode 1520, reward is 0.937 (+/-0.71)\n",
      "At episode 1540, reward is 0.914 (+/-0.682)\n",
      "At episode 1560, reward is 1.248 (+/-1.142)\n",
      "At episode 1580, reward is 0.79 (+/-0.429)\n",
      "At episode 1600, reward is 0.745 (+/-0.588)\n",
      "At episode 1620, reward is 0.853 (+/-0.624)\n",
      "At episode 1640, reward is 1.006 (+/-0.962)\n",
      "At episode 1660, reward is 0.904 (+/-0.668)\n",
      "At episode 1680, reward is 1.182 (+/-1.02)\n",
      "At episode 1700, reward is 1.288 (+/-0.953)\n",
      "At episode 1720, reward is 0.798 (+/-0.45)\n",
      "At episode 1740, reward is 1.038 (+/-0.993)\n",
      "At episode 1760, reward is 1.108 (+/-1.185)\n",
      "At episode 1780, reward is 1.18 (+/-1.165)\n",
      "At episode 1800, reward is 0.855 (+/-0.497)\n",
      "At episode 1820, reward is 0.798 (+/-0.45)\n",
      "At episode 1840, reward is 0.796 (+/-0.591)\n",
      "At episode 1860, reward is 0.776 (+/-0.394)\n",
      "At episode 1880, reward is 0.843 (+/-0.469)\n",
      "At episode 1900, reward is 1.273 (+/-1.386)\n",
      "At episode 1920, reward is 1.139 (+/-1.264)\n",
      "At episode 1940, reward is 1.034 (+/-0.705)\n",
      "At episode 1960, reward is 0.92 (+/-0.661)\n",
      "At episode 1980, reward is 0.896 (+/-0.498)\n",
      "At episode 2000, reward is 1.61 (+/-1.218)\n",
      "At episode 2020, reward is 1.238 (+/-1.058)\n",
      "At episode 2040, reward is 1.479 (+/-1.581)\n",
      "At episode 2060, reward is 1.348 (+/-1.002)\n",
      "At episode 2080, reward is 1.218 (+/-0.816)\n",
      "At episode 2100, reward is 1.916 (+/-1.142)\n",
      "At episode 2120, reward is 1.126 (+/-1.105)\n",
      "At episode 2140, reward is 1.67 (+/-1.118)\n",
      "At episode 2160, reward is 0.968 (+/-0.55)\n",
      "At episode 2180, reward is 1.97 (+/-1.539)\n",
      "At episode 2200, reward is 1.471 (+/-1.334)\n",
      "At episode 2220, reward is 1.274 (+/-0.834)\n",
      "At episode 2240, reward is 1.673 (+/-1.552)\n",
      "At episode 2260, reward is 1.304 (+/-1.199)\n",
      "At episode 2280, reward is 2.322 (+/-2.139)\n",
      "At episode 2300, reward is 1.704 (+/-1.966)\n",
      "At episode 2320, reward is 1.132 (+/-0.693)\n",
      "At episode 2340, reward is 1.405 (+/-1.422)\n",
      "At episode 2360, reward is 1.659 (+/-1.452)\n",
      "At episode 2380, reward is 1.332 (+/-1.279)\n",
      "At episode 2400, reward is 1.73 (+/-1.323)\n",
      "At episode 2420, reward is 1.566 (+/-1.23)\n",
      "At episode 2440, reward is 1.545 (+/-1.427)\n",
      "At episode 2460, reward is 1.396 (+/-1.494)\n",
      "At episode 2480, reward is 1.362 (+/-0.938)\n",
      "At episode 2500, reward is 1.676 (+/-1.421)\n",
      "At episode 2520, reward is 1.968 (+/-2.229)\n",
      "At episode 2540, reward is 2.06 (+/-1.742)\n",
      "At episode 2560, reward is 2.169 (+/-1.505)\n",
      "At episode 2580, reward is 2.468 (+/-1.999)\n",
      "At episode 2600, reward is 2.314 (+/-1.983)\n",
      "At episode 2620, reward is 2.778 (+/-2.25)\n",
      "At episode 2640, reward is 2.163 (+/-2.672)\n",
      "At episode 2660, reward is 1.904 (+/-1.457)\n",
      "At episode 2680, reward is 1.969 (+/-2.244)\n",
      "At episode 2700, reward is 2.498 (+/-2.389)\n",
      "At episode 2720, reward is 1.546 (+/-1.33)\n",
      "At episode 2740, reward is 2.23 (+/-1.923)\n",
      "At episode 2760, reward is 2.777 (+/-3.08)\n",
      "At episode 2780, reward is 2.803 (+/-2.252)\n",
      "At episode 2800, reward is 1.636 (+/-1.552)\n",
      "At episode 2820, reward is 2.609 (+/-2.65)\n",
      "At episode 2840, reward is 1.328 (+/-1.204)\n",
      "At episode 2860, reward is 2.492 (+/-2.07)\n",
      "At episode 2880, reward is 3.201 (+/-2.566)\n",
      "At episode 2900, reward is 3.609 (+/-3.16)\n",
      "At episode 2920, reward is 2.341 (+/-2.2)\n",
      "At episode 2940, reward is 3.194 (+/-2.76)\n",
      "At episode 2960, reward is 3.263 (+/-2.57)\n",
      "At episode 2980, reward is 3.505 (+/-2.457)\n",
      "At episode 3000, reward is 4.226 (+/-4.015)\n",
      "At episode 3020, reward is 4.42 (+/-5.199)\n",
      "At episode 3040, reward is 3.361 (+/-3.213)\n",
      "At episode 3060, reward is 4.5 (+/-3.797)\n",
      "At episode 3080, reward is 2.566 (+/-3.198)\n",
      "At episode 3100, reward is 2.952 (+/-2.849)\n",
      "At episode 3120, reward is 3.819 (+/-3.243)\n",
      "At episode 3140, reward is 3.825 (+/-3.485)\n",
      "At episode 3160, reward is 2.755 (+/-2.302)\n",
      "At episode 3180, reward is 2.986 (+/-2.942)\n",
      "At episode 3200, reward is 3.849 (+/-3.324)\n",
      "At episode 3220, reward is 4.574 (+/-4.331)\n",
      "At episode 3240, reward is 5.711 (+/-5.388)\n",
      "At episode 3260, reward is 4.661 (+/-4.805)\n",
      "At episode 3280, reward is 2.941 (+/-3.648)\n",
      "At episode 3300, reward is 5.459 (+/-5.507)\n",
      "At episode 3320, reward is 5.369 (+/-4.081)\n",
      "At episode 3340, reward is 4.702 (+/-4.921)\n",
      "At episode 3360, reward is 5.227 (+/-5.244)\n",
      "At episode 3380, reward is 3.84 (+/-4.221)\n",
      "At episode 3400, reward is 6.351 (+/-5.721)\n",
      "At episode 3420, reward is 5.835 (+/-7.508)\n",
      "At episode 3440, reward is 5.359 (+/-5.408)\n",
      "At episode 3460, reward is 6.282 (+/-6.854)\n",
      "At episode 3480, reward is 8.068 (+/-12.429)\n",
      "At episode 3500, reward is 9.287 (+/-11.75)\n",
      "At episode 3520, reward is 8.712 (+/-9.287)\n",
      "At episode 3540, reward is 7.598 (+/-8.087)\n",
      "At episode 3560, reward is 7.448 (+/-6.491)\n",
      "At episode 3580, reward is 9.534 (+/-8.684)\n",
      "At episode 3600, reward is 7.986 (+/-11.261)\n",
      "At episode 3620, reward is 7.83 (+/-8.143)\n",
      "At episode 3640, reward is 9.781 (+/-8.749)\n",
      "At episode 3660, reward is 6.655 (+/-5.182)\n",
      "At episode 3680, reward is 6.846 (+/-7.054)\n",
      "At episode 3700, reward is 8.122 (+/-8.845)\n",
      "At episode 3720, reward is 10.807 (+/-8.048)\n",
      "At episode 3740, reward is 10.15 (+/-8.474)\n",
      "At episode 3760, reward is 10.955 (+/-11.351)\n",
      "At episode 3780, reward is 12.709 (+/-9.291)\n",
      "At episode 3800, reward is 12.052 (+/-13.414)\n",
      "At episode 3820, reward is 10.206 (+/-6.833)\n",
      "At episode 3840, reward is 13.292 (+/-12.27)\n",
      "At episode 3860, reward is 6.224 (+/-4.567)\n",
      "At episode 3880, reward is 6.865 (+/-7.431)\n",
      "At episode 3900, reward is 11.915 (+/-10.878)\n",
      "At episode 3920, reward is 13.895 (+/-13.153)\n",
      "At episode 3940, reward is 9.427 (+/-10.637)\n",
      "At episode 3960, reward is 14.515 (+/-12.116)\n",
      "At episode 3980, reward is 12.182 (+/-10.524)\n",
      "At episode 4000, reward is 13.548 (+/-10.918)\n",
      "At episode 4020, reward is 12.002 (+/-8.535)\n",
      "At episode 4040, reward is 12.079 (+/-10.137)\n",
      "At episode 4060, reward is 11.644 (+/-18.641)\n",
      "At episode 4080, reward is 10.547 (+/-7.194)\n",
      "At episode 4100, reward is 10.941 (+/-8.479)\n",
      "At episode 4120, reward is 13.376 (+/-15.873)\n",
      "At episode 4140, reward is 9.826 (+/-14.445)\n",
      "At episode 4160, reward is 11.304 (+/-11.929)\n",
      "At episode 4180, reward is 6.565 (+/-6.272)\n",
      "At episode 4200, reward is 11.341 (+/-8.532)\n",
      "At episode 4220, reward is 11.095 (+/-10.373)\n",
      "At episode 4240, reward is 13.173 (+/-10.838)\n",
      "At episode 4260, reward is 10.138 (+/-9.254)\n",
      "At episode 4280, reward is 10.76 (+/-7.331)\n",
      "At episode 4300, reward is 8.907 (+/-8.834)\n",
      "At episode 4320, reward is 12.952 (+/-13.763)\n",
      "At episode 4340, reward is 10.139 (+/-9.621)\n",
      "At episode 4360, reward is 11.906 (+/-9.955)\n",
      "At episode 4380, reward is 9.495 (+/-8.89)\n",
      "At episode 4400, reward is 17.67 (+/-14.562)\n",
      "At episode 4420, reward is 7.796 (+/-9.374)\n",
      "At episode 4440, reward is 11.585 (+/-11.365)\n",
      "At episode 4460, reward is 13.33 (+/-9.392)\n",
      "At episode 4480, reward is 12.966 (+/-13.136)\n",
      "At episode 4500, reward is 9.971 (+/-9.622)\n",
      "At episode 4520, reward is 16.001 (+/-16.665)\n",
      "At episode 4540, reward is 13.421 (+/-11.757)\n",
      "At episode 4560, reward is 15.763 (+/-13.799)\n",
      "At episode 4580, reward is 10.855 (+/-10.663)\n",
      "At episode 4600, reward is 12.499 (+/-12.651)\n",
      "At episode 4620, reward is 17.264 (+/-16.333)\n",
      "At episode 4640, reward is 13.268 (+/-10.12)\n",
      "At episode 4660, reward is 9.513 (+/-7.121)\n",
      "At episode 4680, reward is 14.924 (+/-12.344)\n",
      "At episode 4700, reward is 13.381 (+/-16.761)\n",
      "At episode 4720, reward is 10.879 (+/-14.414)\n",
      "At episode 4740, reward is 12.298 (+/-16.632)\n",
      "At episode 4760, reward is 14.864 (+/-14.736)\n",
      "At episode 4780, reward is 10.481 (+/-8.216)\n",
      "At episode 4800, reward is 7.569 (+/-7.402)\n",
      "At episode 4820, reward is 12.621 (+/-12.555)\n",
      "At episode 4840, reward is 11.301 (+/-8.984)\n",
      "At episode 4860, reward is 7.44 (+/-7.113)\n",
      "At episode 4880, reward is 10.512 (+/-12.398)\n",
      "At episode 4900, reward is 12.368 (+/-12.725)\n",
      "At episode 4920, reward is 14.152 (+/-17.557)\n",
      "At episode 4940, reward is 12.94 (+/-15.313)\n",
      "At episode 7780, reward is 9.695 (+/-7.35)\n"
     ]
    }
   ],
   "source": [
    "trainer.train(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 43.0000\n",
      "episode 1, total reward: 71.0000\n",
      "episode 2, total reward: 151.0000\n",
      "episode 3, total reward: 209.0000\n",
      "episode 4, total reward: 8.0000\n",
      "episode 5, total reward: 3.0000\n",
      "episode 6, total reward: 4.0000\n",
      "episode 7, total reward: 34.0000\n",
      "episode 8, total reward: 20.0000\n",
      "episode 9, total reward: 34.0000\n",
      "episode 10, total reward: 1.0000\n",
      "episode 11, total reward: 62.0000\n",
      "episode 12, total reward: 11.0000\n",
      "episode 13, total reward: 11.0000\n",
      "episode 14, total reward: 24.0000\n",
      "episode 15, total reward: 2.0000\n",
      "episode 16, total reward: 31.0000\n",
      "episode 17, total reward: 29.0000\n",
      "episode 18, total reward: 118.0000\n",
      "episode 19, total reward: 29.0000\n",
      "episode 20, total reward: 1.0000\n",
      "episode 21, total reward: 19.0000\n",
      "episode 22, total reward: 7.0000\n",
      "episode 23, total reward: 7.0000\n",
      "episode 24, total reward: 97.0000\n",
      "episode 25, total reward: 7.0000\n",
      "episode 26, total reward: 1.0000\n",
      "episode 27, total reward: 206.0000\n",
      "episode 28, total reward: 27.0000\n",
      "episode 29, total reward: 21.0000\n",
      "episode 30, total reward: 79.0000\n",
      "episode 31, total reward: 57.0000\n",
      "episode 32, total reward: 79.0000\n",
      "episode 33, total reward: 13.0000\n",
      "episode 34, total reward: 19.0000\n",
      "episode 35, total reward: 23.0000\n",
      "episode 36, total reward: 2.0000\n",
      "episode 37, total reward: 4.0000\n",
      "episode 38, total reward: 37.0000\n",
      "episode 39, total reward: 7.0000\n",
      "episode 40, total reward: 43.0000\n",
      "episode 41, total reward: 80.0000\n",
      "episode 42, total reward: 28.0000\n",
      "episode 43, total reward: 14.0000\n",
      "episode 44, total reward: 14.0000\n",
      "episode 45, total reward: 16.0000\n",
      "episode 46, total reward: 21.0000\n",
      "episode 47, total reward: 46.0000\n",
      "episode 48, total reward: 55.0000\n",
      "episode 49, total reward: 38.0000\n",
      "reward by 50, mean: 39.2600, std: 46.6475\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "model_name = '03_duelingddqn_01'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 22.0000\n",
      "episode 1, total reward: 28.0000\n",
      "episode 2, total reward: 78.0000\n",
      "episode 3, total reward: 22.0000\n",
      "episode 4, total reward: 47.0000\n",
      "episode 5, total reward: 7.0000\n",
      "episode 6, total reward: 74.0000\n",
      "episode 7, total reward: 1.0000\n",
      "episode 8, total reward: 34.0000\n",
      "episode 9, total reward: 109.0000\n",
      "episode 10, total reward: 2.0000\n",
      "episode 11, total reward: 1.0000\n",
      "episode 12, total reward: 18.0000\n",
      "episode 13, total reward: 83.0000\n",
      "episode 14, total reward: 41.0000\n",
      "episode 15, total reward: 77.0000\n",
      "episode 16, total reward: 17.0000\n",
      "episode 17, total reward: 16.0000\n",
      "episode 18, total reward: 5.0000\n",
      "episode 19, total reward: 33.0000\n",
      "episode 20, total reward: 14.0000\n",
      "episode 21, total reward: 16.0000\n",
      "episode 22, total reward: 16.0000\n",
      "episode 23, total reward: 25.0000\n",
      "episode 24, total reward: 11.0000\n",
      "episode 25, total reward: 86.0000\n",
      "episode 26, total reward: 14.0000\n",
      "episode 27, total reward: 12.0000\n",
      "episode 28, total reward: 33.0000\n",
      "episode 29, total reward: 76.0000\n",
      "episode 30, total reward: 21.0000\n",
      "episode 31, total reward: 54.0000\n",
      "episode 32, total reward: 20.0000\n",
      "episode 33, total reward: 49.0000\n",
      "episode 34, total reward: 23.0000\n",
      "episode 35, total reward: 2.0000\n",
      "episode 36, total reward: 12.0000\n",
      "episode 37, total reward: 44.0000\n",
      "episode 38, total reward: 5.0000\n",
      "episode 39, total reward: 21.0000\n",
      "episode 40, total reward: 13.0000\n",
      "episode 41, total reward: 61.0000\n",
      "episode 42, total reward: 49.0000\n",
      "episode 43, total reward: 53.0000\n",
      "episode 44, total reward: 5.0000\n",
      "episode 45, total reward: 2.0000\n",
      "episode 46, total reward: 34.0000\n",
      "episode 47, total reward: 38.0000\n",
      "episode 48, total reward: 32.0000\n",
      "episode 49, total reward: 49.0000\n",
      "reward by 50, mean: 32.1000, std: 26.0847\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "model_name = '03_duelingddqn_02'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 136.0000\n",
      "episode 1, total reward: 65.0000\n",
      "episode 2, total reward: 10.0000\n",
      "episode 3, total reward: 67.0000\n",
      "episode 4, total reward: 19.0000\n",
      "episode 5, total reward: 2.0000\n",
      "episode 6, total reward: 15.0000\n",
      "episode 7, total reward: 61.0000\n",
      "episode 8, total reward: 71.0000\n",
      "episode 9, total reward: 195.0000\n",
      "episode 10, total reward: 55.0000\n",
      "episode 11, total reward: 58.0000\n",
      "episode 12, total reward: 30.0000\n",
      "episode 13, total reward: 17.0000\n",
      "episode 14, total reward: 25.0000\n",
      "episode 15, total reward: 55.0000\n",
      "episode 16, total reward: 11.0000\n",
      "episode 17, total reward: 51.0000\n",
      "episode 18, total reward: 22.0000\n",
      "episode 19, total reward: 23.0000\n",
      "episode 20, total reward: 98.0000\n",
      "episode 21, total reward: 49.0000\n",
      "episode 22, total reward: 35.0000\n",
      "episode 23, total reward: 0.0000\n",
      "episode 24, total reward: 88.0000\n",
      "episode 25, total reward: 41.0000\n",
      "episode 26, total reward: 1.0000\n",
      "episode 27, total reward: 8.0000\n",
      "episode 28, total reward: 0.0000\n",
      "episode 29, total reward: 46.0000\n",
      "episode 30, total reward: 73.0000\n",
      "episode 31, total reward: 95.0000\n",
      "episode 32, total reward: 92.0000\n",
      "episode 33, total reward: 2.0000\n",
      "episode 34, total reward: 38.0000\n",
      "episode 35, total reward: 14.0000\n",
      "episode 36, total reward: 13.0000\n",
      "episode 37, total reward: 17.0000\n",
      "episode 38, total reward: 77.0000\n",
      "episode 39, total reward: 20.0000\n",
      "episode 40, total reward: 94.0000\n",
      "episode 41, total reward: 13.0000\n",
      "episode 42, total reward: 1.0000\n",
      "episode 43, total reward: 22.0000\n",
      "episode 44, total reward: 13.0000\n",
      "episode 45, total reward: 5.0000\n",
      "episode 46, total reward: 2.0000\n",
      "episode 47, total reward: 10.0000\n",
      "episode 48, total reward: 71.0000\n",
      "episode 49, total reward: 62.0000\n",
      "reward by 50, mean: 41.7600, std: 39.2971\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "model_name = '03_duelingddqn_03'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 33.0000\n",
      "episode 1, total reward: 5.0000\n",
      "episode 2, total reward: 35.0000\n",
      "episode 3, total reward: 1.0000\n",
      "episode 4, total reward: 51.0000\n",
      "episode 5, total reward: 4.0000\n",
      "episode 6, total reward: 2.0000\n",
      "episode 7, total reward: 17.0000\n",
      "episode 8, total reward: 7.0000\n",
      "episode 9, total reward: 20.0000\n",
      "episode 10, total reward: 1.0000\n",
      "episode 11, total reward: 2.0000\n",
      "episode 12, total reward: 9.0000\n",
      "episode 13, total reward: 29.0000\n",
      "episode 14, total reward: 5.0000\n",
      "episode 15, total reward: 5.0000\n",
      "episode 16, total reward: 4.0000\n",
      "episode 17, total reward: 3.0000\n",
      "episode 18, total reward: 0.0000\n",
      "episode 19, total reward: 13.0000\n",
      "episode 20, total reward: 5.0000\n",
      "episode 21, total reward: 54.0000\n",
      "episode 22, total reward: 1.0000\n",
      "episode 23, total reward: 35.0000\n",
      "episode 24, total reward: 1.0000\n",
      "episode 25, total reward: 7.0000\n",
      "episode 26, total reward: 7.0000\n",
      "episode 27, total reward: 8.0000\n",
      "episode 28, total reward: 21.0000\n",
      "episode 29, total reward: 92.0000\n",
      "episode 30, total reward: 3.0000\n",
      "episode 31, total reward: 4.0000\n",
      "episode 32, total reward: 4.0000\n",
      "episode 33, total reward: 3.0000\n",
      "episode 34, total reward: 2.0000\n",
      "episode 35, total reward: 4.0000\n",
      "episode 36, total reward: 15.0000\n",
      "episode 37, total reward: 11.0000\n",
      "episode 38, total reward: 8.0000\n",
      "episode 39, total reward: 4.0000\n",
      "episode 40, total reward: 1.0000\n",
      "episode 41, total reward: 25.0000\n",
      "episode 42, total reward: 13.0000\n",
      "episode 43, total reward: 14.0000\n",
      "episode 44, total reward: 7.0000\n",
      "episode 45, total reward: 2.0000\n",
      "episode 46, total reward: 4.0000\n",
      "episode 47, total reward: 4.0000\n",
      "episode 48, total reward: 1.0000\n",
      "episode 49, total reward: 81.0000\n",
      "reward by 50, mean: 13.7400, std: 19.3905\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "model_name = '03_duelingddqn_04'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 26.0000\n",
      "episode 1, total reward: 15.0000\n",
      "episode 2, total reward: 6.0000\n",
      "episode 3, total reward: 7.0000\n",
      "episode 4, total reward: 26.0000\n",
      "episode 5, total reward: 61.0000\n",
      "episode 6, total reward: 37.0000\n",
      "episode 7, total reward: 2.0000\n",
      "episode 8, total reward: 20.0000\n",
      "episode 9, total reward: 10.0000\n",
      "episode 10, total reward: 14.0000\n",
      "episode 11, total reward: 10.0000\n",
      "episode 12, total reward: 7.0000\n",
      "episode 13, total reward: 56.0000\n",
      "episode 14, total reward: 13.0000\n",
      "episode 15, total reward: 5.0000\n",
      "episode 16, total reward: 12.0000\n",
      "episode 17, total reward: 54.0000\n",
      "episode 18, total reward: 19.0000\n",
      "episode 19, total reward: 2.0000\n",
      "episode 20, total reward: 23.0000\n",
      "episode 21, total reward: 27.0000\n",
      "episode 22, total reward: 27.0000\n",
      "episode 23, total reward: 95.0000\n",
      "episode 24, total reward: 15.0000\n",
      "episode 25, total reward: 8.0000\n",
      "episode 26, total reward: 1.0000\n",
      "episode 27, total reward: 1.0000\n",
      "episode 28, total reward: 131.0000\n",
      "episode 29, total reward: 25.0000\n",
      "episode 30, total reward: 5.0000\n",
      "episode 31, total reward: 23.0000\n",
      "episode 32, total reward: 47.0000\n",
      "episode 33, total reward: 32.0000\n",
      "episode 34, total reward: 1.0000\n",
      "episode 35, total reward: 44.0000\n",
      "episode 36, total reward: 17.0000\n",
      "episode 37, total reward: 1.0000\n",
      "episode 38, total reward: 22.0000\n",
      "episode 39, total reward: 40.0000\n",
      "episode 40, total reward: 58.0000\n",
      "episode 41, total reward: 2.0000\n",
      "episode 42, total reward: 20.0000\n",
      "episode 43, total reward: 34.0000\n",
      "episode 44, total reward: 52.0000\n",
      "episode 45, total reward: 142.0000\n",
      "episode 46, total reward: 22.0000\n",
      "episode 47, total reward: 115.0000\n",
      "episode 48, total reward: 13.0000\n",
      "episode 49, total reward: 2.0000\n",
      "reward by 50, mean: 28.9400, std: 32.0440\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "model_name = '03_duelingddqn_05'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 31.160, median: 32.100\n"
     ]
    }
   ],
   "source": [
    "res = np.array([39.2600, 32.1000, 41.7600, 13.7400, 28.9400])\n",
    "print('mean: {:.3f}, median: {:.3f}'.format(res.mean(), np.median(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
