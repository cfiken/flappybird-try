{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import logging\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "\n",
    "# gym\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import gym_ple\n",
    "\n",
    "# disply\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select gpu\n",
    "gpu_index = 0\n",
    "from tensorflow.keras.backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=str(gpu_index),\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    state: gym.spaces.Box\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: gym.spaces.Box\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir='./logs/', dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.keras.callbacks.TensorBoard(self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "    \n",
    "    @property\n",
    "    def ckpt_path(self):\n",
    "        return os.path.join(self.log_dir, 'model.ckpt')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, is_training: bool, num_actions: int):\n",
    "        super(AgentModel, self).__init__()\n",
    "        self.is_training = is_training\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        relu = tf.nn.relu\n",
    "        self.conv_01 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, padding='same', kernel_initializer=k_init, activation=relu) \n",
    "        self.conv_02 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=4, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.conv_03 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense_v = tf.keras.layers.Dense(128, kernel_initializer=k_init, activation=relu)\n",
    "        self.dense_ad = tf.keras.layers.Dense(128, kernel_initializer=k_init, activation=relu)\n",
    "        self.output_v_layer = tf.keras.layers.Dense(1, kernel_initializer=k_init)\n",
    "        self.output_ad_layer = tf.keras.layers.Dense(num_actions, kernel_initializer=k_init)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = inputs\n",
    "        outputs = self.conv_01(outputs)\n",
    "        outputs = self.conv_02(outputs)\n",
    "        outputs = self.conv_03(outputs)\n",
    "        outputs = self.flatten(outputs)\n",
    "        output_v = self.dense_v(outputs)\n",
    "        outputs_ad = self.dense_ad(outputs)\n",
    "        output_v = self.output_v_layer(output_v)\n",
    "        outputs_ad = self.output_ad_layer(outputs_ad)\n",
    "        q = output_v + (outputs_ad - tf.reduce_mean(outputs_ad))\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, actions, epsilon, input_shape, learning_rate=0.0001):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.input_shape = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self._teacher_model = None\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.build()\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        \n",
    "    def save(self, model_path):\n",
    "        self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        input_shape = (env.width, env.height, env.frame_count)\n",
    "        agent = cls(actions, epsilon, input_shape)\n",
    "        agent.initialize()\n",
    "        agent.model.load_weights(model_path)\n",
    "        return agent\n",
    "        \n",
    "    def build(self):\n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "        model = AgentModel(is_training=True, num_actions=len(self.actions))\n",
    "        outputs = model(inputs)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # teacher_model を更新するため、両方のモデルで一度計算し重みを取得する\n",
    "        self._teacher_model = AgentModel(is_training=True, num_actions=len(self.actions))\n",
    "        dummy = np.random.randn(1, *self.input_shape).astype(np.float32)\n",
    "        dummy = tf.convert_to_tensor(dummy)\n",
    "        _ = self.model.call(dummy)\n",
    "        _ = self._teacher_model.call(dummy)\n",
    "        self.update_teacher()\n",
    "    \n",
    "    def policy(self, state) -> int:\n",
    "        '''\n",
    "        epsilon greedy で受け取った state をもとに行動を決定する\n",
    "        '''\n",
    "        # epsilon greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(state)\n",
    "            return np.argmax(estimates)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        '''\n",
    "        ある state の状態価値を推定する\n",
    "        '''\n",
    "        state_as_batch = np.array([state])\n",
    "        return self.model.predict(state_as_batch)[0]\n",
    "    \n",
    "    def update(self, experiences, gamma):\n",
    "        '''\n",
    "        与えられた experiences をもとに学習\n",
    "        '''\n",
    "        states = np.array([e.state for e in experiences])\n",
    "        next_states = np.array([e.next_state for e in experiences])\n",
    "\n",
    "        estimated_values = self.model.predict(states)\n",
    "        next_state_values = self._teacher_model.predict(next_states)\n",
    "        \n",
    "        # train\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.reward\n",
    "            if not e.done:\n",
    "                reward += gamma * np.max(next_state_values[i])\n",
    "            estimated_values[i][e.action] = reward\n",
    "        loss = self.model.train_on_batch(states, estimated_values)\n",
    "        return loss\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def play(self, env, episode_count: int = 2, render: bool = True):\n",
    "        total_rewards = []\n",
    "        for e in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step_with_raw_reward(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                episode_reward += step_reward\n",
    "                state = next_state\n",
    "            print('episode {}, total reward: {:.4f}'.format(e, episode_reward))\n",
    "            total_rewards.append(episode_reward)\n",
    "                \n",
    "        env.reset()\n",
    "        print('reward by {}, mean: {:.4f}, std: {:.4f}'.format(episode_count, np.mean(total_rewards), np.std(total_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer:\n",
    "    \n",
    "    def __init__(self, env, frame_count, width, height, render=False, outdir='./playlogs/'):\n",
    "        self._env = env\n",
    "        if render:\n",
    "            outdir = outdir\n",
    "            env = Monitor(env, directory=outdir, video_callable=(lambda x: x % 5 == 0), force=True)\n",
    "            self._env = env\n",
    "        self.frame_count = frame_count\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "        \n",
    "    def render(self):\n",
    "        self._env.render(mode = 'rgb_array')\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), self.reward_shaping(reward), done\n",
    "    \n",
    "    def step_with_raw_reward(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), reward, done\n",
    "        \n",
    "    def transform(self, state):\n",
    "        state = state[:400, :, :]\n",
    "        grayed = Image.fromarray(state).convert('L')  # h, w, c -> h, w\n",
    "        \n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(np.float32)\n",
    "        resized = np.transpose(resized, (1, 0)) # h, w -> w, h\n",
    "        normalized = resized / 255.0\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        feature = np.transpose(feature, (1, 2, 0))  # [f, w, h] -> [w, h, f]\n",
    "        \n",
    "        return feature\n",
    "\n",
    "    def reward_shaping(self, reward):\n",
    "        if 0.01 > reward > -0.01:\n",
    "            return 0.01\n",
    "        elif reward <= -0.1:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, filename, buffer_size=50000, batch_size=32, gamma=0.98, report_interval=20):\n",
    "        self.file_name = filename\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        self.logger = Logger(dir_name=filename)\n",
    "        self._max_reward = 0\n",
    "        self.teacher_update_freq = 10000\n",
    "        \n",
    "    def train(self, env, episode_count=10000, initial_count=200, model_path = None):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if model_path:\n",
    "            agent = Agent.load(env, model_path, epsilon=0.1)\n",
    "        else:\n",
    "            agent = Agent(actions, 0.1, input_shape=(env.width, env.height, env.frame_count))\n",
    "        self.train_loop(env, agent, episode_count, initial_count)\n",
    "        \n",
    "    def train_loop(self, env, agent, episode_count, initial_count):\n",
    "        \n",
    "        for episode in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                e = Experience(state, action, step_reward, next_state, done)\n",
    "                self.experiences.append(e)\n",
    "                episode_reward += step_reward\n",
    "                loss = self.step(episode, agent)\n",
    "                state = next_state\n",
    "                \n",
    "                if not self.training and (len(self.experiences) >= self.buffer_size or episode >= initial_count):\n",
    "                    self.begin_training(agent)\n",
    "                    self.training = True\n",
    "            \n",
    "            self.end_episode(episode, episode_reward, loss, agent)\n",
    "    \n",
    "    def step(self, step, agent):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            loss = agent.update(batch, self.gamma)\n",
    "            self.training_count += 1\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "            return loss\n",
    "            \n",
    "    def begin_training(self, agent):\n",
    "        print('start training!')\n",
    "        self.logger.set_model(agent.model)\n",
    "        \n",
    "    def end_episode(self, episode, reward, loss, agent):\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.ckpt_path)\n",
    "                self._max_reward = reward\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)\n",
    "        \n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '03_dueling_05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128)\n",
    "trainer = Trainer(filename=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 20, reward is 0.203 (+/-0.045)\n",
      "At episode 40, reward is 0.197 (+/-0.044)\n",
      "At episode 60, reward is 0.198 (+/-0.044)\n",
      "At episode 80, reward is 0.203 (+/-0.049)\n",
      "At episode 100, reward is 0.221 (+/-0.051)\n",
      "At episode 120, reward is 0.195 (+/-0.046)\n",
      "At episode 140, reward is 0.218 (+/-0.055)\n",
      "At episode 160, reward is 0.214 (+/-0.071)\n",
      "At episode 180, reward is 0.195 (+/-0.036)\n",
      "start training!\n",
      "At episode 200, reward is 0.221 (+/-0.055)\n",
      "At episode 220, reward is 0.319 (+/-0.331)\n",
      "At episode 240, reward is 1.006 (+/-0.664)\n",
      "At episode 260, reward is 1.087 (+/-0.726)\n",
      "At episode 280, reward is 0.855 (+/-0.627)\n",
      "At episode 300, reward is 0.916 (+/-0.7)\n",
      "At episode 320, reward is 0.698 (+/-0.388)\n",
      "At episode 340, reward is 1.002 (+/-0.735)\n",
      "At episode 360, reward is 1.042 (+/-0.729)\n",
      "At episode 380, reward is 0.972 (+/-0.719)\n",
      "At episode 400, reward is 1.178 (+/-0.851)\n",
      "At episode 420, reward is 0.944 (+/-0.728)\n",
      "At episode 440, reward is 0.88 (+/-0.81)\n",
      "At episode 460, reward is 0.79 (+/-0.579)\n",
      "At episode 480, reward is 1.346 (+/-1.075)\n",
      "At episode 500, reward is 0.791 (+/-0.459)\n",
      "At episode 520, reward is 0.679 (+/-0.294)\n",
      "At episode 540, reward is 0.734 (+/-0.374)\n",
      "At episode 560, reward is 0.719 (+/-0.327)\n",
      "At episode 580, reward is 0.841 (+/-0.466)\n",
      "At episode 600, reward is 1.006 (+/-0.74)\n",
      "At episode 620, reward is 0.745 (+/-0.405)\n",
      "At episode 640, reward is 0.897 (+/-0.71)\n",
      "At episode 660, reward is 0.964 (+/-0.665)\n",
      "At episode 680, reward is 0.638 (+/-0.246)\n",
      "At episode 700, reward is 0.789 (+/-0.576)\n",
      "At episode 720, reward is 1.094 (+/-0.852)\n",
      "At episode 740, reward is 0.959 (+/-0.74)\n",
      "At episode 760, reward is 0.792 (+/-0.433)\n",
      "At episode 780, reward is 1.045 (+/-0.589)\n",
      "At episode 800, reward is 1.412 (+/-1.118)\n",
      "At episode 820, reward is 0.823 (+/-0.7)\n",
      "At episode 840, reward is 1.207 (+/-1.397)\n",
      "At episode 860, reward is 0.977 (+/-0.688)\n",
      "At episode 880, reward is 0.663 (+/-0.302)\n",
      "At episode 900, reward is 1.253 (+/-0.883)\n",
      "At episode 920, reward is 1.12 (+/-0.874)\n",
      "At episode 940, reward is 0.73 (+/-0.416)\n",
      "At episode 960, reward is 0.907 (+/-0.554)\n",
      "At episode 980, reward is 0.924 (+/-0.663)\n",
      "At episode 1000, reward is 0.88 (+/-0.688)\n",
      "At episode 1020, reward is 0.869 (+/-0.518)\n",
      "At episode 1040, reward is 1.112 (+/-0.865)\n",
      "At episode 1060, reward is 1.136 (+/-1.146)\n",
      "At episode 1080, reward is 0.713 (+/-0.384)\n",
      "At episode 1100, reward is 0.87 (+/-0.866)\n",
      "At episode 1120, reward is 1.099 (+/-0.604)\n",
      "At episode 1140, reward is 0.869 (+/-0.648)\n",
      "At episode 1160, reward is 1.042 (+/-0.595)\n",
      "At episode 1180, reward is 1.072 (+/-0.856)\n",
      "At episode 1200, reward is 0.862 (+/-0.698)\n",
      "At episode 1220, reward is 1.045 (+/-0.714)\n",
      "At episode 1240, reward is 1.084 (+/-1.078)\n",
      "At episode 1260, reward is 0.788 (+/-0.424)\n",
      "At episode 1280, reward is 0.867 (+/-0.67)\n",
      "At episode 1300, reward is 0.924 (+/-0.544)\n",
      "At episode 1320, reward is 0.865 (+/-0.638)\n",
      "At episode 1340, reward is 0.858 (+/-0.656)\n",
      "At episode 1360, reward is 1.11 (+/-1.183)\n",
      "At episode 1380, reward is 0.916 (+/-1.009)\n",
      "At episode 1400, reward is 1.31 (+/-1.205)\n",
      "At episode 1420, reward is 1.1 (+/-0.927)\n",
      "At episode 1440, reward is 1.025 (+/-0.726)\n",
      "At episode 1460, reward is 0.768 (+/-0.377)\n",
      "At episode 1480, reward is 1.468 (+/-1.306)\n",
      "At episode 1500, reward is 1.289 (+/-1.063)\n",
      "At episode 1520, reward is 1.56 (+/-1.457)\n",
      "At episode 1540, reward is 1.057 (+/-0.748)\n",
      "At episode 1560, reward is 1.652 (+/-1.139)\n",
      "At episode 1580, reward is 1.022 (+/-0.683)\n",
      "At episode 1600, reward is 1.264 (+/-0.761)\n",
      "At episode 1620, reward is 1.206 (+/-1.043)\n",
      "At episode 1640, reward is 1.194 (+/-1.045)\n",
      "At episode 1660, reward is 1.444 (+/-1.459)\n",
      "At episode 1680, reward is 1.794 (+/-1.299)\n",
      "At episode 1700, reward is 1.757 (+/-1.318)\n",
      "At episode 1720, reward is 1.19 (+/-1.021)\n",
      "At episode 1740, reward is 1.883 (+/-1.95)\n",
      "At episode 1760, reward is 1.4 (+/-1.079)\n",
      "At episode 1780, reward is 1.364 (+/-1.088)\n",
      "At episode 1800, reward is 1.043 (+/-1.167)\n",
      "At episode 1820, reward is 1.542 (+/-1.182)\n",
      "At episode 1840, reward is 1.59 (+/-1.307)\n",
      "At episode 1860, reward is 1.623 (+/-1.77)\n",
      "At episode 1880, reward is 2.175 (+/-1.507)\n",
      "At episode 1900, reward is 1.894 (+/-1.648)\n",
      "At episode 1920, reward is 2.635 (+/-2.118)\n",
      "At episode 1940, reward is 1.393 (+/-1.083)\n",
      "At episode 1960, reward is 2.434 (+/-2.056)\n",
      "At episode 1980, reward is 2.422 (+/-1.889)\n",
      "At episode 2000, reward is 2.511 (+/-1.932)\n",
      "At episode 2020, reward is 1.968 (+/-1.681)\n",
      "At episode 2040, reward is 1.582 (+/-1.508)\n",
      "At episode 2060, reward is 2.008 (+/-1.897)\n",
      "At episode 2080, reward is 1.368 (+/-1.13)\n",
      "At episode 2100, reward is 1.974 (+/-1.711)\n",
      "At episode 2120, reward is 1.986 (+/-1.8)\n",
      "At episode 2140, reward is 2.964 (+/-2.529)\n",
      "At episode 2160, reward is 2.049 (+/-1.938)\n",
      "At episode 2180, reward is 1.951 (+/-1.422)\n",
      "At episode 2200, reward is 2.081 (+/-1.65)\n",
      "At episode 2220, reward is 1.387 (+/-1.36)\n",
      "At episode 2240, reward is 1.806 (+/-1.844)\n",
      "At episode 2260, reward is 2.246 (+/-1.675)\n",
      "At episode 2280, reward is 2.013 (+/-1.626)\n",
      "At episode 2300, reward is 1.653 (+/-1.524)\n",
      "At episode 2320, reward is 2.966 (+/-2.343)\n",
      "At episode 2340, reward is 2.577 (+/-1.677)\n",
      "At episode 2360, reward is 2.12 (+/-1.924)\n",
      "At episode 2380, reward is 3.625 (+/-3.072)\n",
      "At episode 2400, reward is 1.935 (+/-1.979)\n",
      "At episode 2420, reward is 2.178 (+/-1.908)\n",
      "At episode 2440, reward is 1.166 (+/-0.929)\n",
      "At episode 2460, reward is 3.276 (+/-2.545)\n",
      "At episode 2480, reward is 1.895 (+/-1.713)\n",
      "At episode 2500, reward is 3.571 (+/-3.467)\n",
      "At episode 2520, reward is 2.638 (+/-1.821)\n",
      "At episode 2540, reward is 2.778 (+/-2.939)\n",
      "At episode 2560, reward is 2.383 (+/-2.554)\n",
      "At episode 2580, reward is 2.99 (+/-2.996)\n",
      "At episode 2600, reward is 2.577 (+/-2.645)\n",
      "At episode 2620, reward is 1.71 (+/-2.33)\n",
      "At episode 2640, reward is 1.849 (+/-1.944)\n",
      "At episode 2660, reward is 3.405 (+/-3.486)\n",
      "At episode 2680, reward is 2.702 (+/-2.248)\n",
      "At episode 2700, reward is 4.682 (+/-4.572)\n",
      "At episode 2720, reward is 4.16 (+/-6.295)\n",
      "At episode 2740, reward is 4.665 (+/-5.689)\n",
      "At episode 2760, reward is 4.442 (+/-3.826)\n",
      "At episode 2780, reward is 3.545 (+/-3.192)\n",
      "At episode 2800, reward is 3.988 (+/-4.004)\n",
      "At episode 2820, reward is 4.46 (+/-4.712)\n",
      "At episode 2840, reward is 3.165 (+/-3.347)\n",
      "At episode 2860, reward is 4.404 (+/-4.438)\n",
      "At episode 2880, reward is 5.512 (+/-4.685)\n",
      "At episode 2900, reward is 4.395 (+/-4.591)\n",
      "At episode 2920, reward is 4.692 (+/-4.839)\n",
      "At episode 2940, reward is 8.015 (+/-4.855)\n",
      "At episode 2960, reward is 4.799 (+/-5.51)\n",
      "At episode 2980, reward is 5.061 (+/-3.752)\n",
      "At episode 3000, reward is 7.491 (+/-6.018)\n",
      "At episode 3020, reward is 4.999 (+/-3.208)\n",
      "At episode 3040, reward is 4.26 (+/-3.919)\n",
      "At episode 3060, reward is 5.526 (+/-4.422)\n",
      "At episode 3080, reward is 6.292 (+/-7.369)\n",
      "At episode 3100, reward is 7.84 (+/-5.517)\n",
      "At episode 3120, reward is 3.32 (+/-3.254)\n",
      "At episode 3140, reward is 5.507 (+/-7.432)\n",
      "At episode 3160, reward is 4.547 (+/-3.86)\n",
      "At episode 3180, reward is 6.406 (+/-6.535)\n",
      "At episode 3200, reward is 6.896 (+/-5.652)\n",
      "At episode 3220, reward is 6.802 (+/-6.268)\n",
      "At episode 3240, reward is 5.294 (+/-7.063)\n",
      "At episode 3260, reward is 7.062 (+/-7.877)\n",
      "At episode 3280, reward is 5.232 (+/-5.688)\n",
      "At episode 3300, reward is 6.859 (+/-5.928)\n",
      "At episode 3320, reward is 6.696 (+/-5.057)\n",
      "At episode 3340, reward is 9.18 (+/-8.65)\n",
      "At episode 3360, reward is 5.019 (+/-4.573)\n",
      "At episode 3380, reward is 8.818 (+/-5.252)\n",
      "At episode 3400, reward is 12.177 (+/-8.141)\n",
      "At episode 3420, reward is 8.662 (+/-8.355)\n",
      "At episode 3440, reward is 11.916 (+/-10.017)\n",
      "At episode 3460, reward is 9.371 (+/-4.327)\n",
      "At episode 3480, reward is 10.462 (+/-6.958)\n",
      "At episode 3500, reward is 9.796 (+/-9.581)\n",
      "At episode 3520, reward is 12.924 (+/-9.16)\n",
      "At episode 3540, reward is 8.994 (+/-6.015)\n",
      "At episode 3560, reward is 5.681 (+/-5.357)\n",
      "At episode 3580, reward is 16.47 (+/-19.755)\n",
      "At episode 3600, reward is 10.288 (+/-11.861)\n",
      "At episode 3620, reward is 8.622 (+/-7.338)\n",
      "At episode 3640, reward is 11.928 (+/-12.242)\n",
      "At episode 3660, reward is 9.309 (+/-7.115)\n",
      "At episode 3680, reward is 10.673 (+/-6.581)\n",
      "At episode 3700, reward is 15.466 (+/-11.321)\n",
      "At episode 3720, reward is 11.25 (+/-12.106)\n",
      "At episode 3740, reward is 10.927 (+/-9.347)\n",
      "At episode 3760, reward is 9.549 (+/-9.829)\n",
      "At episode 3780, reward is 12.518 (+/-11.004)\n",
      "At episode 3800, reward is 11.772 (+/-12.791)\n",
      "At episode 3820, reward is 14.919 (+/-10.562)\n",
      "At episode 3840, reward is 8.658 (+/-7.985)\n",
      "At episode 3860, reward is 13.539 (+/-11.609)\n",
      "At episode 3880, reward is 12.887 (+/-11.593)\n",
      "At episode 3900, reward is 9.964 (+/-8.816)\n",
      "At episode 3920, reward is 12.838 (+/-11.433)\n",
      "At episode 3940, reward is 11.328 (+/-10.828)\n",
      "At episode 3960, reward is 9.358 (+/-10.94)\n",
      "At episode 3980, reward is 13.515 (+/-21.076)\n",
      "At episode 4000, reward is 11.542 (+/-10.839)\n",
      "At episode 4020, reward is 13.975 (+/-16.782)\n",
      "At episode 4040, reward is 9.242 (+/-8.314)\n",
      "At episode 4060, reward is 7.145 (+/-4.322)\n",
      "At episode 4080, reward is 9.101 (+/-9.535)\n",
      "At episode 4100, reward is 13.214 (+/-10.56)\n",
      "At episode 4120, reward is 12.046 (+/-9.138)\n",
      "At episode 4140, reward is 14.49 (+/-11.77)\n",
      "At episode 4160, reward is 8.999 (+/-9.316)\n",
      "At episode 4180, reward is 8.074 (+/-7.639)\n",
      "At episode 4200, reward is 8.177 (+/-6.243)\n",
      "At episode 4220, reward is 11.988 (+/-8.079)\n",
      "At episode 4240, reward is 12.034 (+/-10.349)\n",
      "At episode 4260, reward is 10.319 (+/-8.326)\n",
      "At episode 4280, reward is 13.917 (+/-11.799)\n",
      "At episode 4300, reward is 13.714 (+/-12.064)\n",
      "At episode 4320, reward is 15.874 (+/-12.7)\n",
      "At episode 4340, reward is 12.077 (+/-15.317)\n",
      "At episode 4360, reward is 12.188 (+/-13.807)\n",
      "At episode 4380, reward is 11.147 (+/-10.005)\n",
      "At episode 4400, reward is 9.892 (+/-10.129)\n",
      "At episode 4420, reward is 10.383 (+/-8.591)\n",
      "At episode 4440, reward is 11.964 (+/-10.539)\n",
      "At episode 4460, reward is 9.349 (+/-6.557)\n",
      "At episode 4480, reward is 11.145 (+/-11.649)\n",
      "At episode 4500, reward is 7.562 (+/-6.73)\n",
      "At episode 4520, reward is 14.596 (+/-13.018)\n",
      "At episode 4540, reward is 12.746 (+/-10.314)\n",
      "At episode 4560, reward is 12.171 (+/-10.69)\n",
      "At episode 4580, reward is 13.309 (+/-13.455)\n",
      "At episode 4600, reward is 8.735 (+/-5.149)\n",
      "At episode 4620, reward is 10.652 (+/-15.32)\n",
      "At episode 4640, reward is 7.76 (+/-6.509)\n",
      "At episode 4660, reward is 16.436 (+/-13.686)\n",
      "At episode 4680, reward is 11.202 (+/-10.238)\n",
      "At episode 4700, reward is 11.189 (+/-8.715)\n",
      "At episode 4720, reward is 12.529 (+/-9.695)\n",
      "At episode 4740, reward is 15.002 (+/-11.329)\n",
      "At episode 4760, reward is 12.897 (+/-12.115)\n"
     ]
    }
   ],
   "source": [
    "trainer.train(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 9.0000\n",
      "episode 1, total reward: 40.0000\n",
      "episode 2, total reward: 62.0000\n",
      "episode 3, total reward: 25.0000\n",
      "episode 4, total reward: 161.0000\n",
      "episode 5, total reward: 28.0000\n",
      "episode 6, total reward: 15.0000\n",
      "episode 7, total reward: 124.0000\n",
      "episode 8, total reward: 79.0000\n",
      "episode 9, total reward: 2.0000\n",
      "episode 10, total reward: 4.0000\n",
      "episode 11, total reward: 224.0000\n",
      "episode 12, total reward: 14.0000\n",
      "episode 13, total reward: 0.0000\n",
      "episode 14, total reward: 53.0000\n",
      "episode 15, total reward: 46.0000\n",
      "episode 16, total reward: 23.0000\n",
      "episode 17, total reward: 19.0000\n",
      "episode 18, total reward: 39.0000\n",
      "episode 19, total reward: 13.0000\n",
      "episode 20, total reward: 1.0000\n",
      "episode 21, total reward: 53.0000\n",
      "episode 22, total reward: 3.0000\n",
      "episode 23, total reward: 8.0000\n",
      "episode 24, total reward: 34.0000\n",
      "episode 25, total reward: 17.0000\n",
      "episode 26, total reward: 2.0000\n",
      "episode 27, total reward: 98.0000\n",
      "episode 28, total reward: 6.0000\n",
      "episode 29, total reward: 140.0000\n",
      "episode 30, total reward: 21.0000\n",
      "episode 31, total reward: 55.0000\n",
      "episode 32, total reward: 77.0000\n",
      "episode 33, total reward: 49.0000\n",
      "episode 34, total reward: 55.0000\n",
      "episode 35, total reward: 8.0000\n",
      "episode 36, total reward: 20.0000\n",
      "episode 37, total reward: 31.0000\n",
      "episode 38, total reward: 206.0000\n",
      "episode 39, total reward: 32.0000\n",
      "episode 40, total reward: 9.0000\n",
      "episode 41, total reward: 5.0000\n",
      "episode 42, total reward: 70.0000\n",
      "episode 43, total reward: 1.0000\n",
      "episode 44, total reward: 29.0000\n",
      "episode 45, total reward: 28.0000\n",
      "episode 46, total reward: 77.0000\n",
      "episode 47, total reward: 62.0000\n",
      "episode 48, total reward: 1.0000\n",
      "episode 49, total reward: 49.0000\n",
      "reward by 50, mean: 44.5400, std: 50.0976\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "model_name = '03_dueling_01'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 35.0000\n",
      "episode 1, total reward: 19.0000\n",
      "episode 2, total reward: 37.0000\n",
      "episode 3, total reward: 34.0000\n",
      "episode 4, total reward: 49.0000\n",
      "episode 5, total reward: 2.0000\n",
      "episode 6, total reward: 1.0000\n",
      "episode 7, total reward: 61.0000\n",
      "episode 8, total reward: 5.0000\n",
      "episode 9, total reward: 12.0000\n",
      "episode 10, total reward: 58.0000\n",
      "episode 11, total reward: 77.0000\n",
      "episode 12, total reward: 3.0000\n",
      "episode 13, total reward: 5.0000\n",
      "episode 14, total reward: 30.0000\n",
      "episode 15, total reward: 50.0000\n",
      "episode 16, total reward: 19.0000\n",
      "episode 17, total reward: 16.0000\n",
      "episode 18, total reward: 25.0000\n",
      "episode 19, total reward: 46.0000\n",
      "episode 20, total reward: 50.0000\n",
      "episode 21, total reward: 14.0000\n",
      "episode 22, total reward: 166.0000\n",
      "episode 23, total reward: 68.0000\n",
      "episode 24, total reward: 47.0000\n",
      "episode 25, total reward: 3.0000\n",
      "episode 26, total reward: 7.0000\n",
      "episode 27, total reward: 8.0000\n",
      "episode 28, total reward: 26.0000\n",
      "episode 29, total reward: 39.0000\n",
      "episode 30, total reward: 3.0000\n",
      "episode 31, total reward: 106.0000\n",
      "episode 32, total reward: 29.0000\n",
      "episode 33, total reward: 5.0000\n",
      "episode 34, total reward: 26.0000\n",
      "episode 35, total reward: 37.0000\n",
      "episode 36, total reward: 260.0000\n",
      "episode 37, total reward: 140.0000\n",
      "episode 38, total reward: 256.0000\n",
      "episode 39, total reward: 1.0000\n",
      "episode 40, total reward: 69.0000\n",
      "episode 41, total reward: 13.0000\n",
      "episode 42, total reward: 255.0000\n",
      "episode 43, total reward: 17.0000\n",
      "episode 44, total reward: 157.0000\n",
      "episode 45, total reward: 22.0000\n",
      "episode 46, total reward: 35.0000\n",
      "episode 47, total reward: 46.0000\n",
      "episode 48, total reward: 90.0000\n",
      "episode 49, total reward: 55.0000\n",
      "reward by 50, mean: 52.6800, std: 63.8550\n"
     ]
    }
   ],
   "source": [
    "# 2\n",
    "model_name = '03_dueling_02'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 20.0000\n",
      "episode 1, total reward: 46.0000\n",
      "episode 2, total reward: 23.0000\n",
      "episode 3, total reward: 8.0000\n",
      "episode 4, total reward: 13.0000\n",
      "episode 5, total reward: 4.0000\n",
      "episode 6, total reward: 22.0000\n",
      "episode 7, total reward: 74.0000\n",
      "episode 8, total reward: 15.0000\n",
      "episode 9, total reward: 6.0000\n",
      "episode 10, total reward: 27.0000\n",
      "episode 11, total reward: 55.0000\n",
      "episode 12, total reward: 1.0000\n",
      "episode 13, total reward: 19.0000\n",
      "episode 14, total reward: 2.0000\n",
      "episode 15, total reward: 1.0000\n",
      "episode 16, total reward: 31.0000\n",
      "episode 17, total reward: 10.0000\n",
      "episode 18, total reward: 19.0000\n",
      "episode 19, total reward: 26.0000\n",
      "episode 20, total reward: 27.0000\n",
      "episode 21, total reward: 85.0000\n",
      "episode 22, total reward: 12.0000\n",
      "episode 23, total reward: 1.0000\n",
      "episode 24, total reward: 6.0000\n",
      "episode 25, total reward: 1.0000\n",
      "episode 26, total reward: 5.0000\n",
      "episode 27, total reward: 12.0000\n",
      "episode 28, total reward: 6.0000\n",
      "episode 29, total reward: 3.0000\n",
      "episode 30, total reward: 90.0000\n",
      "episode 31, total reward: 1.0000\n",
      "episode 32, total reward: 7.0000\n",
      "episode 33, total reward: 23.0000\n",
      "episode 34, total reward: 13.0000\n",
      "episode 35, total reward: 39.0000\n",
      "episode 36, total reward: 34.0000\n",
      "episode 37, total reward: 7.0000\n",
      "episode 38, total reward: 1.0000\n",
      "episode 39, total reward: 7.0000\n",
      "episode 40, total reward: 74.0000\n",
      "episode 41, total reward: 24.0000\n",
      "episode 42, total reward: 17.0000\n",
      "episode 43, total reward: 24.0000\n",
      "episode 44, total reward: 1.0000\n",
      "episode 45, total reward: 20.0000\n",
      "episode 46, total reward: 3.0000\n",
      "episode 47, total reward: 62.0000\n",
      "episode 48, total reward: 6.0000\n",
      "episode 49, total reward: 24.0000\n",
      "reward by 50, mean: 21.1400, std: 22.4259\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "model_name = '03_dueling_03'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 21.0000\n",
      "episode 1, total reward: 6.0000\n",
      "episode 2, total reward: 4.0000\n",
      "episode 3, total reward: 58.0000\n",
      "episode 4, total reward: 104.0000\n",
      "episode 5, total reward: 52.0000\n",
      "episode 6, total reward: 86.0000\n",
      "episode 7, total reward: 4.0000\n",
      "episode 8, total reward: 4.0000\n",
      "episode 9, total reward: 7.0000\n",
      "episode 10, total reward: 59.0000\n",
      "episode 11, total reward: 5.0000\n",
      "episode 12, total reward: 35.0000\n",
      "episode 13, total reward: 37.0000\n",
      "episode 14, total reward: 9.0000\n",
      "episode 15, total reward: 135.0000\n",
      "episode 16, total reward: 1.0000\n",
      "episode 17, total reward: 52.0000\n",
      "episode 18, total reward: 80.0000\n",
      "episode 19, total reward: 50.0000\n",
      "episode 20, total reward: 6.0000\n",
      "episode 21, total reward: 33.0000\n",
      "episode 22, total reward: 4.0000\n",
      "episode 23, total reward: 32.0000\n",
      "episode 24, total reward: 10.0000\n",
      "episode 25, total reward: 13.0000\n",
      "episode 26, total reward: 264.0000\n",
      "episode 27, total reward: 26.0000\n",
      "episode 28, total reward: 69.0000\n",
      "episode 29, total reward: 14.0000\n",
      "episode 30, total reward: 38.0000\n",
      "episode 31, total reward: 101.0000\n",
      "episode 32, total reward: 71.0000\n",
      "episode 33, total reward: 11.0000\n",
      "episode 34, total reward: 7.0000\n",
      "episode 35, total reward: 92.0000\n",
      "episode 36, total reward: 54.0000\n",
      "episode 37, total reward: 9.0000\n",
      "episode 38, total reward: 63.0000\n",
      "episode 39, total reward: 32.0000\n",
      "episode 40, total reward: 70.0000\n",
      "episode 41, total reward: 14.0000\n",
      "episode 42, total reward: 37.0000\n",
      "episode 43, total reward: 91.0000\n",
      "episode 44, total reward: 40.0000\n",
      "episode 45, total reward: 10.0000\n",
      "episode 46, total reward: 58.0000\n",
      "episode 47, total reward: 49.0000\n",
      "episode 48, total reward: 22.0000\n",
      "episode 49, total reward: 15.0000\n",
      "reward by 50, mean: 43.2800, std: 44.9889\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "model_name = '03_dueling_04'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 18.0000\n",
      "episode 1, total reward: 23.0000\n",
      "episode 2, total reward: 59.0000\n",
      "episode 3, total reward: 53.0000\n",
      "episode 4, total reward: 2.0000\n",
      "episode 5, total reward: 95.0000\n",
      "episode 6, total reward: 95.0000\n",
      "episode 7, total reward: 116.0000\n",
      "episode 8, total reward: 74.0000\n",
      "episode 9, total reward: 37.0000\n",
      "episode 10, total reward: 2.0000\n",
      "episode 11, total reward: 35.0000\n",
      "episode 12, total reward: 2.0000\n",
      "episode 13, total reward: 47.0000\n",
      "episode 14, total reward: 264.0000\n",
      "episode 15, total reward: 2.0000\n",
      "episode 16, total reward: 88.0000\n",
      "episode 17, total reward: 235.0000\n",
      "episode 18, total reward: 14.0000\n",
      "episode 19, total reward: 93.0000\n",
      "episode 20, total reward: 5.0000\n",
      "episode 21, total reward: 27.0000\n",
      "episode 22, total reward: 264.0000\n",
      "episode 23, total reward: 264.0000\n",
      "episode 24, total reward: 64.0000\n",
      "episode 25, total reward: 24.0000\n",
      "episode 26, total reward: 124.0000\n",
      "episode 27, total reward: 67.0000\n",
      "episode 28, total reward: 8.0000\n",
      "episode 29, total reward: 127.0000\n",
      "episode 30, total reward: 80.0000\n",
      "episode 31, total reward: 83.0000\n",
      "episode 32, total reward: 109.0000\n",
      "episode 33, total reward: 264.0000\n",
      "episode 34, total reward: 20.0000\n",
      "episode 35, total reward: 5.0000\n",
      "episode 36, total reward: 243.0000\n",
      "episode 37, total reward: 43.0000\n",
      "episode 38, total reward: 4.0000\n",
      "episode 39, total reward: 38.0000\n",
      "episode 40, total reward: 50.0000\n",
      "episode 41, total reward: 35.0000\n",
      "episode 42, total reward: 4.0000\n",
      "episode 43, total reward: 8.0000\n",
      "episode 44, total reward: 101.0000\n",
      "episode 45, total reward: 142.0000\n",
      "episode 46, total reward: 91.0000\n",
      "episode 47, total reward: 43.0000\n",
      "episode 48, total reward: 70.0000\n",
      "episode 49, total reward: 19.0000\n",
      "reward by 50, mean: 75.6000, std: 76.5208\n"
     ]
    }
   ],
   "source": [
    "# 5\n",
    "model_name = '03_dueling_05'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 47.448, median: 44.540\n"
     ]
    }
   ],
   "source": [
    "res = np.array([44.5400, 52.6800, 21.1400, 43.2800, 75.6000])\n",
    "print('mean: {:.3f}, median: {:.3f}'.format(res.mean(), np.median(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
