{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import logging\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "\n",
    "# gym\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import gym_ple\n",
    "\n",
    "# disply\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    state: gym.spaces.Box\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: gym.spaces.Box\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir='./logs/', dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.keras.callbacks.TensorBoard(self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "    \n",
    "    @property\n",
    "    def ckpt_path(self):\n",
    "        return os.path.join(self.log_dir, 'model.ckpt')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, is_training: bool, num_actions: int):\n",
    "        super(AgentModel, self).__init__()\n",
    "        self.is_training = is_training\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        relu = tf.nn.relu\n",
    "        self.conv_01 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, padding='same', kernel_initializer=k_init, activation=relu) \n",
    "        self.conv_02 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=4, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.conv_03 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense_v = tf.keras.layers.Dense(128, kernel_initializer=k_init, activation=relu)\n",
    "        self.dense_ad = tf.keras.layers.Dense(128, kernel_initializer=k_init, activation=relu)\n",
    "        self.output_v_layer = tf.keras.layers.Dense(1, kernel_initializer=k_init)\n",
    "        self.output_ad_layer = tf.keras.layers.Dense(num_actions, kernel_initializer=k_init)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = inputs\n",
    "        outputs = self.conv_01(outputs)\n",
    "        outputs = self.conv_02(outputs)\n",
    "        outputs = self.conv_03(outputs)\n",
    "        outputs = self.flatten(outputs)\n",
    "        output_v = self.dense_v(outputs)\n",
    "        outputs_ad = self.dense_ad(outputs)\n",
    "        output_v = self.output_v_layer(output_v)\n",
    "        outputs_ad = self.output_ad_layer(outputs_ad)\n",
    "        q = output_v + (outputs_ad - 0.5 * tf.reduce_mean(outputs_ad))\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, actions, epsilon, input_shape, learning_rate=0.0001):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.input_shape = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self._teacher_model = None\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.build()\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        \n",
    "    def save(self, model_path):\n",
    "        self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        input_shape = (env.width, env.height, env.frame_count)\n",
    "        agent = cls(actions, epsilon, input_shape)\n",
    "        agent.initialize()\n",
    "        agent.model.load_weights(model_path)\n",
    "        return agent\n",
    "        \n",
    "    def build(self):\n",
    "        inputs = tf.keras.Input(shape=self.input_shape)\n",
    "        model = AgentModel(is_training=True, num_actions=len(self.actions))\n",
    "        outputs = model(inputs)\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        # teacher_model を更新するため、両方のモデルで一度計算し重みを取得する\n",
    "        self._teacher_model = AgentModel(is_training=True, num_actions=len(self.actions))\n",
    "        dummy = np.random.randn(1, *self.input_shape).astype(np.float32)\n",
    "        dummy = tf.convert_to_tensor(dummy)\n",
    "        _ = self.model.call(dummy)\n",
    "        _ = self._teacher_model.call(dummy)\n",
    "        self.update_teacher()\n",
    "    \n",
    "    def policy(self, state) -> int:\n",
    "        '''\n",
    "        epsilon greedy で受け取った state をもとに行動を決定する\n",
    "        '''\n",
    "        # epsilon greedy\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(state)\n",
    "            return np.argmax(estimates)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        '''\n",
    "        ある state の状態価値を推定する\n",
    "        '''\n",
    "        state_as_batch = np.array([state])\n",
    "        return self.model.predict(state_as_batch)[0]\n",
    "    \n",
    "    def update(self, experiences, gamma):\n",
    "        '''\n",
    "        与えられた experiences をもとに学習\n",
    "        '''\n",
    "        states = np.array([e.state for e in experiences])\n",
    "        next_states = np.array([e.next_state for e in experiences])\n",
    "\n",
    "        estimated_values = self.model.predict(states)\n",
    "        next_state_values = self._teacher_model.predict(next_states)\n",
    "        \n",
    "        # train\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.reward\n",
    "            if not e.done:\n",
    "                reward += gamma * np.max(next_state_values[i])\n",
    "            estimated_values[i][e.action] = reward\n",
    "        loss = self.model.train_on_batch(states, estimated_values)\n",
    "        return loss\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def play(self, env, episode_count: int = 2, render: bool = True):\n",
    "        total_rewards = []\n",
    "        for e in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step_with_raw_reward(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                episode_reward += step_reward\n",
    "                state = next_state\n",
    "            print('episode {}, total reward: {:.4f}'.format(e, episode_reward))\n",
    "            total_rewards.append(episode_reward)\n",
    "                \n",
    "        env.reset()\n",
    "        print('reward by {}, mean: {:.4f}, std: {:.4f}'.format(episode_count, np.mean(total_rewards), np.std(total_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer:\n",
    "    \n",
    "    def __init__(self, env, frame_count, width, height, render=False, outdir='./playlogs/'):\n",
    "        self._env = env\n",
    "        if render:\n",
    "            outdir = outdir\n",
    "            env = Monitor(env, directory=outdir, video_callable=(lambda x: x % 5 == 0), force=True)\n",
    "            self._env = env\n",
    "        self.frame_count = frame_count\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "        \n",
    "    def render(self):\n",
    "        self._env.render(mode = 'rgb_array')\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), self.reward_shaping(reward), done\n",
    "    \n",
    "    def step_with_raw_reward(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), reward, done\n",
    "        \n",
    "    def transform(self, state):\n",
    "        grayed = Image.fromarray(state).convert('L')  # h, w, c -> h, w\n",
    "        \n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(np.float32)\n",
    "        resized = np.transpose(resized, (1, 0)) # h, w -> w, h\n",
    "        normalized = resized / 255.0\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        feature = np.transpose(feature, (1, 2, 0))  # [f, w, h] -> [w, h, f]\n",
    "        \n",
    "        return feature\n",
    "\n",
    "    def reward_shaping(self, reward):\n",
    "        if 0.01 > reward > -0.01:\n",
    "            return 0.01\n",
    "        elif reward <= -0.1:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, filename, buffer_size=20000, batch_size=32, gamma=0.98, report_interval=20):\n",
    "        self.file_name = filename\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        self.logger = Logger(dir_name=filename)\n",
    "        self._max_reward = 0\n",
    "        self.teacher_update_freq = 2\n",
    "        \n",
    "    def train(self, env, episode_count=7000, initial_count=200, model_path = None):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if model_path:\n",
    "            agent = Agent.load(env, model_path, epsilon=0.1)\n",
    "        else:\n",
    "            agent = Agent(actions, 0.1, input_shape=(env.width, env.height, env.frame_count))\n",
    "        self.train_loop(env, agent, episode_count, initial_count)\n",
    "        \n",
    "    def train_loop(self, env, agent, episode_count, initial_count):\n",
    "        \n",
    "        for episode in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                e = Experience(state, action, step_reward, next_state, done)\n",
    "                self.experiences.append(e)\n",
    "                episode_reward += step_reward\n",
    "                loss = self.step(episode, agent)\n",
    "                state = next_state\n",
    "                \n",
    "                if not self.training and (len(self.experiences) >= self.buffer_size or episode >= initial_count):\n",
    "                    self.begin_training(agent)\n",
    "                    self.training = True\n",
    "            \n",
    "            self.end_episode(episode, episode_reward, loss, agent)\n",
    "    \n",
    "    def step(self, step, agent):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            loss = agent.update(batch, self.gamma)\n",
    "            self.training_count += 1\n",
    "            return loss\n",
    "            \n",
    "    def begin_training(self, agent):\n",
    "        print('start training!')\n",
    "        self.logger.set_model(agent.model)\n",
    "        \n",
    "    def end_episode(self, episode, reward, loss, agent):\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.ckpt_path)\n",
    "                self._max_reward = reward\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)\n",
    "        \n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'dqn_dueling_0329'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/flappybird-try-wqQoCzq9/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 288, 512)\n",
    "trainer = Trainer(filename=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 20, reward is 1.289 (+/-0.865)\n",
      "At episode 40, reward is 1.102 (+/-0.725)\n",
      "At episode 60, reward is 0.934 (+/-0.708)\n",
      "At episode 80, reward is 0.98 (+/-0.561)\n",
      "At episode 100, reward is 0.9 (+/-0.501)\n",
      "At episode 120, reward is 0.922 (+/-0.667)\n",
      "At episode 140, reward is 0.77 (+/-0.375)\n",
      "At episode 160, reward is 0.788 (+/-0.422)\n",
      "At episode 180, reward is 0.816 (+/-0.639)\n",
      "start training!\n",
      "At episode 200, reward is 1.015 (+/-0.752)\n",
      "At episode 220, reward is 0.842 (+/-0.599)\n",
      "At episode 240, reward is 0.946 (+/-0.978)\n",
      "At episode 260, reward is 0.724 (+/-0.368)\n",
      "At episode 280, reward is 0.715 (+/-0.549)\n",
      "At episode 300, reward is 1.01 (+/-0.976)\n",
      "At episode 320, reward is 0.831 (+/-0.661)\n",
      "At episode 340, reward is 1.35 (+/-1.181)\n",
      "At episode 360, reward is 0.878 (+/-0.505)\n",
      "At episode 380, reward is 1.16 (+/-0.923)\n",
      "At episode 400, reward is 1.058 (+/-0.743)\n",
      "At episode 420, reward is 1.382 (+/-0.856)\n",
      "At episode 440, reward is 1.554 (+/-1.165)\n",
      "At episode 460, reward is 1.69 (+/-1.541)\n",
      "At episode 480, reward is 1.483 (+/-1.115)\n",
      "At episode 500, reward is 1.626 (+/-1.617)\n",
      "At episode 520, reward is 1.958 (+/-1.416)\n",
      "At episode 540, reward is 1.868 (+/-1.612)\n",
      "At episode 560, reward is 2.47 (+/-1.361)\n",
      "At episode 580, reward is 1.979 (+/-1.135)\n",
      "At episode 600, reward is 2.551 (+/-1.533)\n",
      "At episode 620, reward is 2.552 (+/-1.748)\n",
      "At episode 640, reward is 2.699 (+/-1.861)\n",
      "At episode 660, reward is 3.079 (+/-1.889)\n",
      "At episode 680, reward is 2.429 (+/-1.932)\n",
      "At episode 700, reward is 2.881 (+/-2.235)\n",
      "At episode 720, reward is 3.261 (+/-1.979)\n",
      "At episode 740, reward is 3.328 (+/-1.997)\n",
      "At episode 760, reward is 3.987 (+/-3.167)\n",
      "At episode 780, reward is 3.9 (+/-3.893)\n",
      "At episode 800, reward is 4.662 (+/-4.732)\n",
      "At episode 820, reward is 5.209 (+/-3.513)\n",
      "At episode 840, reward is 4.795 (+/-2.181)\n",
      "At episode 860, reward is 5.777 (+/-4.531)\n",
      "At episode 880, reward is 8.098 (+/-6.251)\n",
      "At episode 900, reward is 4.381 (+/-3.665)\n",
      "At episode 920, reward is 4.349 (+/-4.331)\n",
      "At episode 940, reward is 7.626 (+/-7.924)\n",
      "At episode 960, reward is 8.743 (+/-5.702)\n",
      "At episode 980, reward is 9.077 (+/-12.939)\n",
      "At episode 1000, reward is 11.175 (+/-7.401)\n",
      "At episode 1020, reward is 6.382 (+/-5.643)\n",
      "At episode 1040, reward is 8.634 (+/-9.167)\n",
      "At episode 1060, reward is 8.808 (+/-6.416)\n",
      "At episode 1080, reward is 8.403 (+/-7.627)\n",
      "At episode 1100, reward is 10.246 (+/-6.977)\n",
      "At episode 1120, reward is 11.02 (+/-10.663)\n",
      "At episode 1140, reward is 15.195 (+/-9.182)\n",
      "At episode 1160, reward is 10.249 (+/-9.491)\n",
      "At episode 1180, reward is 11.507 (+/-8.079)\n",
      "At episode 1200, reward is 8.692 (+/-6.443)\n",
      "At episode 1220, reward is 11.141 (+/-8.06)\n",
      "At episode 1240, reward is 11.137 (+/-10.067)\n",
      "At episode 1260, reward is 12.907 (+/-10.946)\n",
      "At episode 1280, reward is 10.429 (+/-6.473)\n",
      "At episode 1300, reward is 8.216 (+/-5.594)\n",
      "At episode 1320, reward is 16.046 (+/-16.621)\n",
      "At episode 1340, reward is 13.1 (+/-10.426)\n",
      "At episode 1360, reward is 12.448 (+/-10.391)\n",
      "At episode 1380, reward is 14.663 (+/-17.414)\n",
      "At episode 1400, reward is 8.63 (+/-7.186)\n",
      "At episode 1420, reward is 13.45 (+/-11.181)\n",
      "At episode 1440, reward is 10.546 (+/-8.469)\n",
      "At episode 1460, reward is 11.086 (+/-10.383)\n",
      "At episode 1480, reward is 13.028 (+/-10.503)\n",
      "At episode 1500, reward is 8.445 (+/-7.413)\n",
      "At episode 1520, reward is 12.166 (+/-13.233)\n",
      "At episode 1540, reward is 15.411 (+/-16.396)\n",
      "At episode 1560, reward is 10.906 (+/-7.937)\n",
      "At episode 1580, reward is 9.714 (+/-7.72)\n",
      "At episode 1600, reward is 9.342 (+/-8.443)\n",
      "At episode 1620, reward is 7.733 (+/-6.356)\n",
      "At episode 1640, reward is 12.139 (+/-11.188)\n",
      "At episode 1660, reward is 11.621 (+/-9.539)\n",
      "At episode 1680, reward is 10.994 (+/-10.687)\n",
      "At episode 1700, reward is 11.696 (+/-10.661)\n",
      "At episode 1720, reward is 10.089 (+/-7.3)\n",
      "At episode 1740, reward is 10.143 (+/-9.711)\n",
      "At episode 1760, reward is 18.668 (+/-16.746)\n",
      "At episode 1780, reward is 12.996 (+/-8.963)\n",
      "At episode 1800, reward is 10.239 (+/-8.853)\n",
      "At episode 1820, reward is 12.597 (+/-13.324)\n",
      "At episode 1840, reward is 12.893 (+/-8.095)\n",
      "At episode 1860, reward is 11.516 (+/-9.404)\n",
      "At episode 1880, reward is 12.217 (+/-9.03)\n",
      "At episode 1900, reward is 11.304 (+/-8.363)\n",
      "At episode 1920, reward is 12.771 (+/-20.726)\n",
      "At episode 1940, reward is 13.163 (+/-13.13)\n",
      "At episode 1960, reward is 6.238 (+/-4.569)\n",
      "At episode 1980, reward is 12.629 (+/-11.836)\n",
      "At episode 2000, reward is 11.102 (+/-9.736)\n",
      "At episode 2020, reward is 14.2 (+/-11.158)\n",
      "At episode 2040, reward is 11.915 (+/-9.762)\n",
      "At episode 2060, reward is 10.392 (+/-6.103)\n",
      "At episode 2080, reward is 16.265 (+/-12.923)\n",
      "At episode 2100, reward is 13.714 (+/-17.712)\n",
      "At episode 2120, reward is 9.702 (+/-7.068)\n",
      "At episode 2140, reward is 10.457 (+/-9.405)\n",
      "At episode 2160, reward is 9.755 (+/-7.522)\n",
      "At episode 2180, reward is 11.023 (+/-7.083)\n",
      "At episode 2200, reward is 9.205 (+/-11.021)\n",
      "At episode 2220, reward is 13.027 (+/-8.995)\n",
      "At episode 2240, reward is 6.333 (+/-5.388)\n",
      "At episode 2260, reward is 9.171 (+/-9.411)\n",
      "At episode 2280, reward is 10.798 (+/-8.401)\n",
      "At episode 2300, reward is 12.384 (+/-10.329)\n",
      "At episode 2320, reward is 10.591 (+/-10.058)\n",
      "At episode 2340, reward is 7.599 (+/-6.97)\n",
      "At episode 2360, reward is 10.934 (+/-10.973)\n",
      "At episode 2380, reward is 13.136 (+/-11.784)\n",
      "At episode 2400, reward is 12.281 (+/-9.939)\n",
      "At episode 2420, reward is 8.691 (+/-6.683)\n",
      "At episode 2440, reward is 12.326 (+/-10.369)\n",
      "At episode 2460, reward is 12.062 (+/-11.506)\n",
      "At episode 2480, reward is 11.382 (+/-11.063)\n",
      "At episode 2500, reward is 10.0 (+/-12.153)\n",
      "At episode 2520, reward is 10.581 (+/-8.575)\n",
      "At episode 2540, reward is 13.907 (+/-17.855)\n",
      "At episode 2560, reward is 7.284 (+/-5.35)\n",
      "At episode 2580, reward is 12.555 (+/-10.807)\n",
      "At episode 2600, reward is 11.647 (+/-9.417)\n",
      "At episode 2620, reward is 8.687 (+/-6.99)\n",
      "At episode 2640, reward is 12.619 (+/-10.418)\n",
      "At episode 2660, reward is 8.585 (+/-8.441)\n",
      "At episode 2680, reward is 10.089 (+/-10.137)\n",
      "At episode 2700, reward is 11.343 (+/-11.656)\n",
      "At episode 2720, reward is 10.454 (+/-8.739)\n",
      "At episode 2740, reward is 8.916 (+/-6.741)\n",
      "At episode 2760, reward is 7.663 (+/-5.784)\n",
      "At episode 2780, reward is 15.319 (+/-14.158)\n",
      "At episode 2800, reward is 13.074 (+/-15.728)\n",
      "At episode 2820, reward is 8.364 (+/-11.158)\n",
      "At episode 2840, reward is 10.947 (+/-8.164)\n",
      "At episode 2860, reward is 10.815 (+/-10.361)\n",
      "At episode 2880, reward is 10.897 (+/-9.614)\n",
      "At episode 2900, reward is 12.971 (+/-8.217)\n",
      "At episode 2920, reward is 9.367 (+/-9.56)\n",
      "At episode 2940, reward is 12.411 (+/-12.304)\n",
      "At episode 2960, reward is 11.066 (+/-8.432)\n",
      "At episode 2980, reward is 12.083 (+/-9.05)\n",
      "At episode 3000, reward is 11.525 (+/-8.033)\n",
      "At episode 3020, reward is 10.698 (+/-8.641)\n",
      "At episode 3040, reward is 12.698 (+/-11.283)\n",
      "At episode 3060, reward is 14.681 (+/-9.708)\n",
      "At episode 3080, reward is 9.623 (+/-12.201)\n",
      "At episode 3100, reward is 9.861 (+/-5.723)\n",
      "At episode 3120, reward is 11.389 (+/-9.04)\n",
      "At episode 3140, reward is 11.724 (+/-12.18)\n",
      "At episode 3160, reward is 10.148 (+/-10.36)\n",
      "At episode 3180, reward is 14.148 (+/-14.936)\n",
      "At episode 3200, reward is 8.999 (+/-6.079)\n",
      "At episode 3220, reward is 10.497 (+/-6.336)\n",
      "At episode 3240, reward is 8.349 (+/-9.237)\n",
      "At episode 3260, reward is 13.638 (+/-13.364)\n",
      "At episode 3280, reward is 10.604 (+/-10.437)\n",
      "At episode 3300, reward is 8.162 (+/-9.727)\n",
      "At episode 3320, reward is 8.719 (+/-7.417)\n",
      "At episode 3340, reward is 11.416 (+/-7.571)\n",
      "At episode 3360, reward is 10.912 (+/-9.714)\n",
      "At episode 3380, reward is 7.995 (+/-7.437)\n",
      "At episode 3400, reward is 13.035 (+/-9.808)\n",
      "At episode 3420, reward is 10.278 (+/-8.845)\n",
      "At episode 3440, reward is 6.678 (+/-6.048)\n",
      "At episode 3460, reward is 8.627 (+/-7.745)\n",
      "At episode 3480, reward is 13.265 (+/-11.562)\n",
      "At episode 3500, reward is 11.74 (+/-9.486)\n",
      "At episode 3520, reward is 14.104 (+/-10.248)\n",
      "At episode 3540, reward is 9.964 (+/-8.728)\n",
      "At episode 3560, reward is 10.361 (+/-9.609)\n",
      "At episode 3580, reward is 9.799 (+/-9.797)\n",
      "At episode 3600, reward is 14.028 (+/-7.638)\n",
      "At episode 3620, reward is 10.678 (+/-8.517)\n",
      "At episode 3640, reward is 11.837 (+/-9.775)\n",
      "At episode 3660, reward is 8.551 (+/-8.462)\n",
      "At episode 3680, reward is 10.275 (+/-6.839)\n",
      "At episode 3700, reward is 10.633 (+/-9.481)\n",
      "At episode 3720, reward is 10.526 (+/-10.266)\n",
      "At episode 3740, reward is 13.177 (+/-12.227)\n",
      "At episode 3760, reward is 12.343 (+/-7.565)\n",
      "At episode 3780, reward is 9.286 (+/-7.124)\n",
      "At episode 3800, reward is 14.307 (+/-14.817)\n",
      "At episode 3820, reward is 6.537 (+/-6.242)\n",
      "At episode 3840, reward is 11.27 (+/-9.475)\n",
      "At episode 3860, reward is 10.517 (+/-9.379)\n",
      "At episode 3880, reward is 8.813 (+/-8.138)\n",
      "At episode 3900, reward is 10.86 (+/-11.515)\n",
      "At episode 3920, reward is 8.9 (+/-10.868)\n",
      "At episode 3940, reward is 9.267 (+/-8.352)\n",
      "At episode 3960, reward is 9.271 (+/-6.517)\n",
      "At episode 3980, reward is 10.61 (+/-9.91)\n",
      "At episode 4000, reward is 14.598 (+/-18.174)\n",
      "At episode 4020, reward is 9.026 (+/-6.313)\n",
      "At episode 4040, reward is 8.821 (+/-8.726)\n",
      "At episode 4060, reward is 9.275 (+/-7.033)\n",
      "At episode 4080, reward is 11.035 (+/-9.573)\n",
      "At episode 4100, reward is 16.956 (+/-12.116)\n",
      "At episode 4120, reward is 9.576 (+/-7.988)\n",
      "At episode 4140, reward is 13.242 (+/-12.336)\n",
      "At episode 4160, reward is 13.022 (+/-11.351)\n",
      "At episode 4180, reward is 8.716 (+/-8.52)\n",
      "At episode 4200, reward is 20.397 (+/-18.263)\n",
      "At episode 4220, reward is 12.882 (+/-11.009)\n",
      "At episode 4240, reward is 12.601 (+/-12.481)\n",
      "At episode 4260, reward is 8.057 (+/-5.422)\n",
      "At episode 4280, reward is 8.916 (+/-6.424)\n",
      "At episode 4300, reward is 11.667 (+/-9.543)\n",
      "At episode 4320, reward is 14.222 (+/-13.006)\n",
      "At episode 4340, reward is 10.156 (+/-7.916)\n",
      "At episode 4360, reward is 10.975 (+/-9.031)\n",
      "At episode 4380, reward is 7.07 (+/-5.806)\n",
      "At episode 4400, reward is 13.86 (+/-13.669)\n",
      "At episode 4420, reward is 9.925 (+/-6.877)\n",
      "At episode 4440, reward is 10.904 (+/-10.269)\n",
      "At episode 4460, reward is 11.33 (+/-12.522)\n",
      "At episode 4480, reward is 9.197 (+/-8.958)\n",
      "At episode 4500, reward is 17.899 (+/-17.047)\n",
      "At episode 4520, reward is 11.513 (+/-10.269)\n",
      "At episode 4540, reward is 14.946 (+/-11.163)\n",
      "At episode 4560, reward is 12.153 (+/-11.93)\n",
      "At episode 4580, reward is 12.251 (+/-9.644)\n",
      "At episode 4600, reward is 10.714 (+/-6.888)\n",
      "At episode 4620, reward is 12.042 (+/-10.536)\n",
      "At episode 4640, reward is 11.283 (+/-12.818)\n",
      "At episode 4660, reward is 7.589 (+/-7.07)\n",
      "At episode 4680, reward is 12.148 (+/-11.225)\n",
      "At episode 4700, reward is 13.594 (+/-13.473)\n",
      "At episode 4720, reward is 12.447 (+/-11.758)\n",
      "At episode 4740, reward is 10.538 (+/-8.382)\n",
      "At episode 4760, reward is 9.458 (+/-9.33)\n",
      "At episode 4780, reward is 10.303 (+/-12.137)\n",
      "At episode 4800, reward is 10.027 (+/-11.526)\n",
      "At episode 4820, reward is 8.43 (+/-7.219)\n",
      "At episode 4840, reward is 9.986 (+/-7.469)\n",
      "At episode 4860, reward is 9.398 (+/-7.864)\n",
      "At episode 4880, reward is 8.548 (+/-8.211)\n",
      "At episode 4900, reward is 11.86 (+/-12.427)\n",
      "At episode 4920, reward is 15.489 (+/-9.897)\n",
      "At episode 4940, reward is 7.537 (+/-6.191)\n",
      "At episode 4960, reward is 11.247 (+/-8.509)\n",
      "At episode 4980, reward is 7.203 (+/-5.724)\n",
      "At episode 5000, reward is 11.168 (+/-10.45)\n",
      "At episode 5020, reward is 12.103 (+/-9.684)\n",
      "At episode 5040, reward is 10.15 (+/-8.869)\n",
      "At episode 5060, reward is 11.177 (+/-9.909)\n",
      "At episode 5080, reward is 8.062 (+/-5.895)\n",
      "At episode 5100, reward is 13.28 (+/-17.459)\n",
      "At episode 5120, reward is 12.063 (+/-7.628)\n",
      "At episode 5140, reward is 9.636 (+/-9.368)\n",
      "At episode 5160, reward is 11.19 (+/-8.821)\n",
      "At episode 5180, reward is 8.768 (+/-8.365)\n",
      "At episode 5200, reward is 11.912 (+/-12.181)\n",
      "At episode 5220, reward is 16.177 (+/-19.196)\n",
      "At episode 5240, reward is 7.781 (+/-8.232)\n",
      "At episode 5260, reward is 7.335 (+/-7.372)\n",
      "At episode 5280, reward is 15.894 (+/-15.769)\n",
      "At episode 5300, reward is 11.626 (+/-15.102)\n",
      "At episode 5320, reward is 14.781 (+/-11.801)\n",
      "At episode 5340, reward is 11.943 (+/-8.513)\n",
      "At episode 5360, reward is 12.199 (+/-12.581)\n",
      "At episode 5380, reward is 13.933 (+/-14.856)\n",
      "At episode 5400, reward is 6.891 (+/-4.731)\n",
      "At episode 5420, reward is 12.74 (+/-8.543)\n",
      "At episode 5440, reward is 7.889 (+/-6.734)\n",
      "At episode 5460, reward is 7.853 (+/-5.337)\n",
      "At episode 5480, reward is 10.753 (+/-12.659)\n",
      "At episode 5500, reward is 8.662 (+/-6.518)\n",
      "At episode 5520, reward is 10.998 (+/-13.397)\n",
      "At episode 5540, reward is 11.648 (+/-8.939)\n",
      "At episode 5560, reward is 8.4 (+/-7.131)\n",
      "At episode 5580, reward is 13.969 (+/-10.019)\n",
      "At episode 5600, reward is 9.166 (+/-9.909)\n",
      "At episode 5620, reward is 11.731 (+/-9.986)\n",
      "At episode 5640, reward is 10.823 (+/-9.827)\n",
      "At episode 5660, reward is 10.752 (+/-8.888)\n",
      "At episode 5680, reward is 8.293 (+/-6.38)\n",
      "At episode 5700, reward is 11.419 (+/-8.275)\n",
      "At episode 5720, reward is 13.355 (+/-11.359)\n",
      "At episode 5740, reward is 11.709 (+/-7.576)\n",
      "At episode 5760, reward is 7.508 (+/-6.976)\n",
      "At episode 5780, reward is 11.096 (+/-14.95)\n",
      "At episode 5800, reward is 8.023 (+/-5.077)\n",
      "At episode 5820, reward is 10.636 (+/-7.888)\n",
      "At episode 5840, reward is 14.629 (+/-16.298)\n",
      "At episode 5860, reward is 20.951 (+/-18.458)\n",
      "At episode 5880, reward is 9.36 (+/-10.836)\n",
      "At episode 5900, reward is 7.412 (+/-5.513)\n",
      "At episode 5920, reward is 12.937 (+/-9.528)\n",
      "At episode 5940, reward is 18.187 (+/-19.326)\n",
      "At episode 5960, reward is 11.612 (+/-10.576)\n",
      "At episode 5980, reward is 11.188 (+/-9.874)\n",
      "At episode 6000, reward is 14.407 (+/-10.836)\n",
      "At episode 6020, reward is 11.195 (+/-13.12)\n",
      "At episode 6040, reward is 8.586 (+/-8.431)\n",
      "At episode 6060, reward is 12.105 (+/-10.805)\n",
      "At episode 6080, reward is 10.498 (+/-10.03)\n",
      "At episode 6100, reward is 10.595 (+/-10.997)\n",
      "At episode 6120, reward is 9.406 (+/-6.93)\n",
      "At episode 6140, reward is 10.178 (+/-8.241)\n",
      "At episode 6160, reward is 12.311 (+/-11.911)\n",
      "At episode 6180, reward is 6.554 (+/-4.497)\n",
      "At episode 6200, reward is 8.194 (+/-5.442)\n",
      "At episode 6220, reward is 12.435 (+/-12.117)\n",
      "At episode 6240, reward is 16.248 (+/-17.009)\n",
      "At episode 6260, reward is 7.27 (+/-6.765)\n",
      "At episode 6280, reward is 9.926 (+/-9.476)\n",
      "At episode 6300, reward is 9.386 (+/-9.049)\n",
      "At episode 6320, reward is 11.933 (+/-10.355)\n",
      "At episode 6340, reward is 12.149 (+/-13.35)\n",
      "At episode 6360, reward is 11.569 (+/-11.4)\n",
      "At episode 6380, reward is 9.895 (+/-7.92)\n",
      "At episode 6400, reward is 13.024 (+/-12.863)\n",
      "At episode 6420, reward is 19.941 (+/-21.571)\n",
      "At episode 6440, reward is 11.911 (+/-8.586)\n",
      "At episode 6460, reward is 11.719 (+/-14.815)\n",
      "At episode 6480, reward is 6.894 (+/-5.541)\n",
      "At episode 6500, reward is 12.136 (+/-9.325)\n",
      "At episode 6520, reward is 10.355 (+/-6.629)\n",
      "At episode 6540, reward is 15.794 (+/-16.221)\n",
      "At episode 6560, reward is 12.135 (+/-8.179)\n",
      "At episode 6580, reward is 13.492 (+/-10.744)\n",
      "At episode 6600, reward is 9.381 (+/-9.047)\n",
      "At episode 6620, reward is 11.652 (+/-12.711)\n",
      "At episode 6640, reward is 15.504 (+/-17.221)\n",
      "At episode 6660, reward is 13.958 (+/-10.783)\n",
      "At episode 6680, reward is 10.597 (+/-9.598)\n",
      "At episode 6700, reward is 10.913 (+/-11.001)\n",
      "At episode 6720, reward is 11.024 (+/-8.436)\n",
      "At episode 6740, reward is 12.804 (+/-14.587)\n",
      "At episode 6760, reward is 13.643 (+/-16.726)\n",
      "At episode 6780, reward is 11.45 (+/-9.649)\n",
      "At episode 6800, reward is 12.736 (+/-9.619)\n",
      "At episode 6820, reward is 11.598 (+/-11.558)\n",
      "At episode 6840, reward is 12.948 (+/-7.488)\n",
      "At episode 6860, reward is 10.901 (+/-9.099)\n",
      "At episode 6880, reward is 10.351 (+/-11.482)\n",
      "At episode 6900, reward is 10.74 (+/-9.404)\n",
      "At episode 6920, reward is 13.077 (+/-9.354)\n",
      "At episode 6940, reward is 12.787 (+/-10.779)\n",
      "At episode 6960, reward is 7.991 (+/-8.534)\n",
      "At episode 6980, reward is 10.718 (+/-8.235)\n"
     ]
    }
   ],
   "source": [
    "trainer.train(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 288, 512, render=True)\n",
    "# obs = Observer(env, 4, 144, 256, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, total reward: 32.0000\n",
      "episode 1, total reward: 50.0000\n",
      "episode 2, total reward: 29.0000\n",
      "episode 3, total reward: 264.0000\n",
      "episode 4, total reward: 121.0000\n",
      "episode 5, total reward: 34.0000\n",
      "episode 6, total reward: 22.0000\n",
      "episode 7, total reward: 10.0000\n",
      "episode 8, total reward: 106.0000\n",
      "episode 9, total reward: 133.0000\n",
      "episode 10, total reward: 71.0000\n",
      "episode 11, total reward: 47.0000\n",
      "episode 12, total reward: 10.0000\n",
      "episode 13, total reward: 19.0000\n",
      "episode 14, total reward: 130.0000\n",
      "episode 15, total reward: 34.0000\n",
      "episode 16, total reward: 3.0000\n",
      "episode 17, total reward: 22.0000\n",
      "episode 18, total reward: 116.0000\n",
      "episode 19, total reward: 73.0000\n",
      "episode 20, total reward: 55.0000\n",
      "episode 21, total reward: 82.0000\n",
      "episode 22, total reward: 63.0000\n",
      "episode 23, total reward: 127.0000\n",
      "episode 24, total reward: 26.0000\n",
      "episode 25, total reward: 31.0000\n",
      "episode 26, total reward: 136.0000\n",
      "episode 27, total reward: 65.0000\n",
      "episode 28, total reward: 32.0000\n",
      "episode 29, total reward: 94.0000\n",
      "episode 30, total reward: 148.0000\n",
      "episode 31, total reward: 67.0000\n",
      "episode 32, total reward: 36.0000\n",
      "episode 33, total reward: 53.0000\n",
      "episode 34, total reward: 19.0000\n",
      "episode 35, total reward: 130.0000\n",
      "episode 36, total reward: 2.0000\n",
      "episode 37, total reward: 1.0000\n",
      "episode 38, total reward: 71.0000\n",
      "episode 39, total reward: 58.0000\n",
      "episode 40, total reward: 3.0000\n",
      "episode 41, total reward: 4.0000\n",
      "episode 42, total reward: 46.0000\n",
      "episode 43, total reward: 1.0000\n",
      "episode 44, total reward: 23.0000\n",
      "episode 45, total reward: 47.0000\n",
      "episode 46, total reward: 17.0000\n",
      "episode 47, total reward: 63.0000\n",
      "episode 48, total reward: 32.0000\n",
      "episode 49, total reward: 89.0000\n",
      "reward by 50, mean: 58.9400, std: 50.8002\n"
     ]
    }
   ],
   "source": [
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
