{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import logging\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "\n",
    "# gym\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import gym_ple\n",
    "\n",
    "# disply\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select gpu\n",
    "gpu_index = 1\n",
    "from tensorflow.keras.backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=str(gpu_index),\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    state: gym.spaces.Box\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: gym.spaces.Box\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir='./logs/', dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.keras.callbacks.TensorBoard(self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "    \n",
    "    @property\n",
    "    def ckpt_path(self):\n",
    "        return os.path.join(self.log_dir, 'model.ckpt')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNoisyLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_outputs: int):\n",
    "        super(AgentNoisyLayer, self).__init__(trainable=True, name='noisynet', dtype=tf.float32)\n",
    "        self.num_outputs = num_outputs\n",
    "        # dense layer のみ noisy network に変更する\n",
    "        self.relu = relu = tf.nn.relu\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        self.v_dense_w_mu = self.add_weight(\"v_dense_w_mu\", [1024, 128], initializer=k_init)\n",
    "        self.ad_dense_w_mu = self.add_weight(\"ad_dense_w_mu\", [1024, 128], initializer=k_init)\n",
    "        \n",
    "        self.v_output_w_mu = self.add_weight(\"v_output_w_mu\", [128, 1], initializer=k_init)\n",
    "        self.ad_output_w_mu = self.add_weight(\"ad_output_w_mu\", [128, num_outputs], initializer=k_init)\n",
    "        \n",
    "        self.v_dense_b_mu = self.add_weight(\"v_dense_b_mu\", [128], initializer='zero')\n",
    "        self.ad_dense_b_mu = self.add_weight(\"ad_dense_b_mu\", [128], initializer='zero')\n",
    "\n",
    "        self.v_output_b_mu = self.add_weight(\"v_output_b_mu\", (), initializer='zero')\n",
    "        self.ad_output_b_mu = self.add_weight(\"ad_output_b_mu\", [num_outputs], initializer='zero')\n",
    "\n",
    "        self.v_dense_w_sigma = self.add_weight(\"v_dense_w_sigma\", [1024, 128])\n",
    "        self.ad_dense_w_sigma = self.add_weight(\"ad_dense_w_sigma\", [1024, 128])\n",
    "        \n",
    "        self.v_output_w_sigma = self.add_weight(\"v_output_w_sigma\", [128, 1])\n",
    "        self.ad_output_w_sigma = self.add_weight(\"ad_output_w_sigma\", [128, num_outputs])\n",
    "        \n",
    "        self.v_dense_b_sigma = self.add_weight(\"v_dense_b_sigma\", [128])\n",
    "        self.ad_dense_b_sigma = self.add_weight(\"ad_dense_b_sigma\", [128])\n",
    "\n",
    "        self.v_output_b_sigma = self.add_weight(\"v_output_b_sigma\", ())\n",
    "        self.ad_output_b_sigma = self.add_weight(\"ad_output_b_sigma\", [num_outputs])\n",
    "        \n",
    "    def call(self, x):\n",
    "        # noise\n",
    "        dense_in_noise = tf.random.normal([1024])\n",
    "        v_dense_out_noise = tf.random.normal([128])\n",
    "        ad_dense_out_noise = tf.random.normal([128])\n",
    "        d_in = self._noise_w(dense_in_noise)\n",
    "        v_d_out = self._noise_w(v_dense_out_noise)\n",
    "        ad_d_out = self._noise_w(ad_dense_out_noise)\n",
    "        \n",
    "        v_noise_dense_w = tf.matmul(tf.expand_dims(d_in, 1), tf.expand_dims(v_d_out, 0))\n",
    "        ad_noise_dense_w = tf.matmul(tf.expand_dims(d_in, 1), tf.expand_dims(ad_d_out, 0))\n",
    "        v_dense_w = self.v_dense_w_mu + tf.multiply(self.v_dense_w_sigma, v_noise_dense_w)\n",
    "        ad_dense_w = self.ad_dense_w_mu + tf.multiply(self.ad_dense_w_sigma, ad_noise_dense_w)\n",
    "        v_dense_b = self.v_dense_b_mu + tf.multiply(self.v_dense_b_sigma, self._noise_b(v_dense_out_noise))\n",
    "        ad_dense_b = self.ad_dense_b_mu + tf.multiply(self.ad_dense_b_sigma, self._noise_b(ad_dense_out_noise))\n",
    "        \n",
    "        v_output_in_noise = tf.random.normal([128])\n",
    "        v_output_out_noise = tf.random.normal([1])\n",
    "        ad_output_in_noise = tf.random.normal([128])\n",
    "        ad_output_out_noise = tf.random.normal([self.num_outputs])\n",
    "\n",
    "        v_o_in = self._noise_w(v_output_in_noise)\n",
    "        ad_o_in = self._noise_w(ad_output_in_noise)\n",
    "        v_o_out = self._noise_w(v_output_out_noise)\n",
    "        ad_o_out = self._noise_w(ad_output_out_noise)\n",
    "        v_noise_output_w = tf.matmul(tf.expand_dims(v_o_in, 1), tf.expand_dims(v_o_out, 0))\n",
    "        ad_noise_output_w = tf.matmul(tf.expand_dims(ad_o_in, 1), tf.expand_dims(ad_o_out, 0))\n",
    "        v_output_w = self.v_output_w_mu + tf.multiply(self.v_output_w_sigma, v_noise_output_w)\n",
    "        ad_output_w = self.ad_output_w_mu + tf.multiply(self.ad_output_w_sigma, ad_noise_output_w)\n",
    "        v_output_b = self.v_output_b_mu + tf.multiply(self.v_output_b_sigma, self._noise_b(v_output_out_noise))\n",
    "        ad_output_b = self.ad_output_b_mu + tf.multiply(self.ad_output_b_sigma, self._noise_b(ad_output_out_noise))\n",
    "        \n",
    "        # forward\n",
    "        v_outputs = tf.matmul(tf.to_float(x), v_dense_w) + v_dense_b\n",
    "        ad_outputs = tf.matmul(tf.to_float(x), ad_dense_w) + ad_dense_b\n",
    "        v_outputs = self.relu(v_outputs)\n",
    "        ad_outputs = self.relu(ad_outputs)\n",
    "        v_outputs = tf.matmul(v_outputs, v_output_w) + v_output_b\n",
    "        ad_outputs = tf.matmul(ad_outputs, ad_output_w) + ad_output_b\n",
    "        outputs = v_outputs + (ad_outputs - tf.reduce_mean(ad_outputs))\n",
    "        return outputs\n",
    "    \n",
    "    def _noise_w(self, x):\n",
    "        return tf.multiply(tf.sign(x), tf.pow(tf.abs(x), 0.5))\n",
    "    \n",
    "    def _noise_b(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, is_training: bool, num_outputs: int):\n",
    "        super(AgentModel, self).__init__()\n",
    "        self.is_training = is_training\n",
    "        self.num_outputs = num_outputs\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        relu = tf.nn.relu\n",
    "        self.conv_01 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, padding='same', kernel_initializer=k_init, activation=relu) \n",
    "        self.conv_02 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=4, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.conv_03 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(256, kernel_initializer=k_init, activation=relu) # 1024 x 256\n",
    "        self.output_layer = tf.keras.layers.Dense(num_outputs, kernel_initializer=k_init) # 256 x 2\n",
    "        \n",
    "        # dense layer のみ noisy network に変更する\n",
    "        self.noisy_layer = AgentNoisyLayer(num_outputs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = inputs\n",
    "        outputs = self.conv_01(outputs)\n",
    "        outputs = self.conv_02(outputs)\n",
    "        outputs = self.conv_03(outputs)\n",
    "        outputs = self.flatten(outputs)\n",
    "        outputs = self.noisy_layer(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        return [shape[0], self.num_outputs]\n",
    "    \n",
    "    def get_noise_mean(self):\n",
    "        all_weights = self.noisy_layer.get_weights()\n",
    "        noise_weights = np.mean([np.mean(np.abs(w)) for w in all_weights[8:12]])\n",
    "        noise_biases = np.mean([np.mean(np.abs(w)) for w in all_weights[12:16]])\n",
    "        return noise_weights, noise_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, actions, epsilon, input_shape, learning_rate=0.0001):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.input_shape = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self._teacher_model = None\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.build()\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        \n",
    "    def save(self, model_path):\n",
    "        self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        input_shape = (env.width, env.height, env.frame_count)\n",
    "        agent = cls(actions, epsilon, input_shape)\n",
    "        agent.initialize()\n",
    "        agent.model.load_weights(model_path)\n",
    "        return agent\n",
    "        \n",
    "    def build(self):\n",
    "        self.model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "\n",
    "        # teacher_model を更新するため、両方のモデルで一度計算し重みを取得する\n",
    "        self._teacher_model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "        dummy = np.random.randn(1, *self.input_shape).astype(np.float32)\n",
    "        dummy = tf.convert_to_tensor(dummy)\n",
    "        _ = self.model.call(dummy)\n",
    "        _ = self._teacher_model.call(dummy)\n",
    "        self.update_teacher()\n",
    "    \n",
    "    def policy(self, state) -> int:\n",
    "        '''\n",
    "        epsilon greedy で受け取った state をもとに行動を決定する\n",
    "        '''\n",
    "        # no longer use epsilon greedy\n",
    "        estimates = self.estimate(state)\n",
    "        return np.argmax(estimates)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        '''\n",
    "        ある state の状態価値を推定する\n",
    "        '''\n",
    "        state_as_batch = np.array([state])\n",
    "        return self.model.predict(state_as_batch)[0]\n",
    "    \n",
    "    def update(self, experiences, gamma):\n",
    "        '''\n",
    "        与えられた experiences をもとに学習\n",
    "        '''\n",
    "        states = np.array([e.state for e in experiences])\n",
    "        next_states = np.array([e.next_state for e in experiences])\n",
    "\n",
    "        estimated_values = self.model.predict(states)\n",
    "        next_state_values = self._teacher_model.predict(next_states)\n",
    "        \n",
    "        # train\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.reward\n",
    "            if not e.done:\n",
    "                reward += gamma * np.max(next_state_values[i])\n",
    "            estimated_values[i][e.action] = reward\n",
    "        loss = self.model.train_on_batch(states, estimated_values)\n",
    "        return loss\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def play(self, env, episode_count: int = 2, render: bool = True):\n",
    "        total_rewards = []\n",
    "        for e in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step_with_raw_reward(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                episode_reward += step_reward\n",
    "                state = next_state\n",
    "            print('episode {}, total reward: {:.4f}'.format(e, episode_reward))\n",
    "            total_rewards.append(episode_reward)\n",
    "                \n",
    "        env.reset()\n",
    "        print('reward by {}, mean: {:.4f}, std: {:.4f}'.format(episode_count, np.mean(total_rewards), np.std(total_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer:\n",
    "    \n",
    "    def __init__(self, env, frame_count, width, height, render=False, outdir='./playlogs/'):\n",
    "        self._env = env\n",
    "        if render:\n",
    "            outdir = outdir\n",
    "            env = Monitor(env, directory=outdir, video_callable=(lambda x: x % 5 == 0), force=True)\n",
    "            self._env = env\n",
    "        self.frame_count = frame_count\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "        \n",
    "    def render(self):\n",
    "        self._env.render(mode = 'rgb_array')\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), self.reward_shaping(reward), done\n",
    "    \n",
    "    def step_with_raw_reward(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), reward, done\n",
    "        \n",
    "    def transform(self, state):\n",
    "        state = state[:400, :, :]\n",
    "        grayed = Image.fromarray(state).convert('L')  # h, w, c -> h, w\n",
    "        \n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(np.float32)\n",
    "        resized = np.transpose(resized, (1, 0)) # h, w -> w, h\n",
    "        normalized = resized / 255.0\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        feature = np.transpose(feature, (1, 2, 0))  # [f, w, h] -> [w, h, f]\n",
    "        \n",
    "        return feature\n",
    "\n",
    "    def reward_shaping(self, reward):\n",
    "        if 0.01 > reward > -0.01:\n",
    "            return 0.01\n",
    "        elif reward <= -0.1:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, filename, buffer_size=50000, batch_size=32, gamma=0.98, report_interval=20):\n",
    "        self.file_name = filename\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        self.logger = Logger(dir_name=filename)\n",
    "        self._max_reward = 0\n",
    "        self.teacher_update_freq = 10000\n",
    "        \n",
    "    def train(self, env, episode_count=10000, initial_count=200, model_path = None):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if model_path:\n",
    "            agent = Agent.load(env, model_path, epsilon=0.1)\n",
    "        else:\n",
    "            agent = Agent(actions, 0.1, input_shape=(env.width, env.height, env.frame_count))\n",
    "        self.train_loop(env, agent, episode_count, initial_count)\n",
    "        \n",
    "    def train_loop(self, env, agent, episode_count, initial_count):\n",
    "        \n",
    "        for episode in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                e = Experience(state, action, step_reward, next_state, done)\n",
    "                self.experiences.append(e)\n",
    "                episode_reward += step_reward\n",
    "                loss = self.step(episode, agent)\n",
    "                state = next_state\n",
    "                \n",
    "                if not self.training and (len(self.experiences) >= self.buffer_size or episode >= initial_count):\n",
    "                    self.begin_training(agent)\n",
    "                    self.training = True\n",
    "            \n",
    "            self.end_episode(episode, episode_reward, loss, agent)\n",
    "    \n",
    "    def step(self, step, agent):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            loss = agent.update(batch, self.gamma)\n",
    "            self.training_count += 1\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "            return loss\n",
    "            \n",
    "    def begin_training(self, agent):\n",
    "        print('start training!')\n",
    "        self.logger.set_model(agent.model)\n",
    "        \n",
    "    def end_episode(self, episode, reward, loss, agent):\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            noise_w, noise_b = agent.model.get_noise_mean()\n",
    "            self.logger.write(self.training_count, \"noise_weights\", noise_w)\n",
    "            self.logger.write(self.training_count, \"noise_biases\", noise_b)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.ckpt_path)\n",
    "                self._max_reward = reward\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)\n",
    "        \n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '04_noisynetdueling_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/flappybird-try-wqQoCzq9/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128)\n",
    "trainer = Trainer(filename=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 20, reward is 0.914 (+/-0.542)\n",
      "At episode 40, reward is 0.952 (+/-1.043)\n",
      "At episode 60, reward is 1.022 (+/-0.722)\n",
      "At episode 80, reward is 1.072 (+/-0.957)\n",
      "At episode 100, reward is 1.092 (+/-0.753)\n",
      "At episode 120, reward is 0.833 (+/-0.455)\n",
      "At episode 140, reward is 1.076 (+/-0.859)\n",
      "At episode 160, reward is 0.828 (+/-0.479)\n",
      "At episode 180, reward is 1.184 (+/-0.791)\n",
      "start training!\n",
      "At episode 200, reward is 0.828 (+/-0.504)\n",
      "At episode 220, reward is 0.946 (+/-0.672)\n",
      "At episode 240, reward is 0.649 (+/-0.25)\n",
      "At episode 260, reward is 0.679 (+/-0.523)\n",
      "At episode 280, reward is 0.957 (+/-0.712)\n",
      "At episode 300, reward is 0.859 (+/-0.524)\n",
      "At episode 320, reward is 0.861 (+/-0.503)\n",
      "At episode 340, reward is 0.546 (+/-0.129)\n",
      "At episode 360, reward is 0.921 (+/-0.967)\n",
      "At episode 380, reward is 1.043 (+/-1.242)\n",
      "At episode 400, reward is 0.839 (+/-0.715)\n",
      "At episode 420, reward is 0.622 (+/-0.277)\n",
      "At episode 440, reward is 0.765 (+/-0.513)\n",
      "At episode 460, reward is 0.885 (+/-0.699)\n",
      "At episode 480, reward is 0.741 (+/-0.399)\n",
      "At episode 500, reward is 0.714 (+/-0.459)\n",
      "At episode 520, reward is 0.759 (+/-0.481)\n",
      "At episode 540, reward is 0.617 (+/-0.26)\n",
      "At episode 560, reward is 0.62 (+/-0.282)\n",
      "At episode 580, reward is 0.512 (+/-0.383)\n",
      "At episode 600, reward is 1.04 (+/-0.735)\n",
      "At episode 620, reward is 0.752 (+/-0.418)\n",
      "At episode 640, reward is 0.686 (+/-0.343)\n",
      "At episode 660, reward is 0.732 (+/-0.364)\n",
      "At episode 680, reward is 0.691 (+/-0.348)\n",
      "At episode 700, reward is 0.992 (+/-0.699)\n",
      "At episode 720, reward is 0.763 (+/-0.487)\n",
      "At episode 740, reward is 0.753 (+/-0.49)\n",
      "At episode 760, reward is 0.719 (+/-0.372)\n",
      "At episode 780, reward is 0.8 (+/-0.681)\n",
      "At episode 800, reward is 0.896 (+/-0.687)\n",
      "At episode 820, reward is 0.77 (+/-0.389)\n",
      "At episode 840, reward is 0.759 (+/-0.643)\n",
      "At episode 860, reward is 0.968 (+/-0.851)\n",
      "At episode 880, reward is 0.823 (+/-0.671)\n",
      "At episode 900, reward is 0.882 (+/-0.683)\n",
      "At episode 920, reward is 0.723 (+/-0.42)\n",
      "At episode 940, reward is 0.641 (+/-0.319)\n",
      "At episode 960, reward is 0.771 (+/-0.643)\n",
      "At episode 980, reward is 0.661 (+/-0.374)\n",
      "At episode 1000, reward is 0.684 (+/-0.402)\n",
      "At episode 1020, reward is 0.592 (+/-0.056)\n",
      "At episode 1040, reward is 0.755 (+/-0.586)\n",
      "At episode 1060, reward is 0.826 (+/-0.506)\n",
      "At episode 1080, reward is 0.77 (+/-0.435)\n",
      "At episode 1100, reward is 0.631 (+/-0.248)\n",
      "At episode 1120, reward is 1.045 (+/-1.299)\n",
      "At episode 1140, reward is 0.798 (+/-0.451)\n",
      "At episode 1160, reward is 1.16 (+/-0.761)\n",
      "At episode 1180, reward is 0.86 (+/-0.67)\n",
      "At episode 1200, reward is 1.05 (+/-0.973)\n",
      "At episode 1220, reward is 1.126 (+/-1.184)\n",
      "At episode 1240, reward is 0.686 (+/-0.435)\n",
      "At episode 1260, reward is 0.81 (+/-0.479)\n",
      "At episode 1280, reward is 1.023 (+/-0.954)\n",
      "At episode 1300, reward is 0.895 (+/-0.683)\n",
      "At episode 1320, reward is 0.819 (+/-0.482)\n",
      "At episode 1340, reward is 0.568 (+/-0.35)\n",
      "At episode 1360, reward is 0.621 (+/-0.257)\n",
      "At episode 1380, reward is 0.767 (+/-0.473)\n",
      "At episode 1400, reward is 0.929 (+/-0.534)\n",
      "At episode 1420, reward is 0.735 (+/-0.593)\n",
      "At episode 1440, reward is 0.844 (+/-0.466)\n",
      "At episode 1460, reward is 0.554 (+/-0.125)\n",
      "At episode 1480, reward is 0.56 (+/-0.356)\n",
      "At episode 1500, reward is 0.944 (+/-0.97)\n",
      "At episode 1520, reward is 0.802 (+/-0.454)\n",
      "At episode 1540, reward is 0.769 (+/-0.438)\n",
      "At episode 1560, reward is 0.825 (+/-0.475)\n",
      "At episode 1580, reward is 1.134 (+/-0.763)\n",
      "At episode 1600, reward is 1.137 (+/-0.703)\n",
      "At episode 1620, reward is 1.036 (+/-0.716)\n",
      "At episode 1640, reward is 1.182 (+/-0.741)\n",
      "At episode 1660, reward is 1.004 (+/-0.534)\n",
      "At episode 1680, reward is 0.908 (+/-0.646)\n",
      "At episode 1700, reward is 0.818 (+/-0.642)\n",
      "At episode 1720, reward is 0.852 (+/-0.476)\n",
      "At episode 1740, reward is 0.995 (+/-0.582)\n",
      "At episode 1760, reward is 0.737 (+/-0.376)\n",
      "At episode 1780, reward is 0.804 (+/-0.459)\n",
      "At episode 1800, reward is 1.124 (+/-0.797)\n",
      "At episode 1820, reward is 1.05 (+/-0.74)\n",
      "At episode 1840, reward is 0.734 (+/-0.374)\n",
      "At episode 1860, reward is 0.79 (+/-0.432)\n",
      "At episode 1880, reward is 1.31 (+/-0.742)\n",
      "At episode 1900, reward is 0.772 (+/-0.435)\n",
      "At episode 1920, reward is 0.92 (+/-0.54)\n",
      "At episode 1940, reward is 1.048 (+/-0.737)\n",
      "At episode 1960, reward is 0.971 (+/-0.701)\n",
      "At episode 1980, reward is 1.178 (+/-0.761)\n",
      "At episode 2000, reward is 0.865 (+/-0.509)\n",
      "At episode 2020, reward is 0.772 (+/-0.387)\n",
      "At episode 2040, reward is 0.667 (+/-0.251)\n",
      "At episode 2060, reward is 0.676 (+/-0.285)\n",
      "At episode 2080, reward is 0.832 (+/-0.443)\n",
      "At episode 2100, reward is 1.006 (+/-1.056)\n",
      "At episode 2120, reward is 1.168 (+/-0.934)\n",
      "At episode 2140, reward is 1.12 (+/-0.847)\n",
      "At episode 2160, reward is 0.917 (+/-0.531)\n",
      "At episode 2180, reward is 1.311 (+/-1.055)\n",
      "At episode 2200, reward is 0.79 (+/-0.426)\n",
      "At episode 2220, reward is 1.079 (+/-0.58)\n",
      "At episode 2240, reward is 1.056 (+/-0.742)\n",
      "At episode 2260, reward is 0.956 (+/-0.65)\n",
      "At episode 2280, reward is 1.339 (+/-0.659)\n",
      "At episode 2300, reward is 0.836 (+/-0.453)\n",
      "At episode 2320, reward is 1.158 (+/-0.718)\n",
      "At episode 2340, reward is 1.312 (+/-1.156)\n",
      "At episode 2360, reward is 1.17 (+/-1.187)\n",
      "At episode 2380, reward is 1.309 (+/-0.985)\n",
      "At episode 2400, reward is 2.048 (+/-1.188)\n",
      "At episode 2420, reward is 1.628 (+/-1.159)\n",
      "At episode 2440, reward is 1.456 (+/-0.841)\n",
      "At episode 2460, reward is 1.449 (+/-1.186)\n",
      "At episode 2480, reward is 1.204 (+/-0.799)\n",
      "At episode 2500, reward is 1.232 (+/-0.756)\n",
      "At episode 2520, reward is 1.492 (+/-0.865)\n",
      "At episode 2540, reward is 1.656 (+/-1.286)\n",
      "At episode 2560, reward is 1.714 (+/-1.515)\n",
      "At episode 2580, reward is 1.392 (+/-0.902)\n",
      "At episode 2600, reward is 1.907 (+/-1.243)\n",
      "At episode 2620, reward is 2.081 (+/-1.353)\n",
      "At episode 2640, reward is 1.94 (+/-1.839)\n",
      "At episode 2660, reward is 1.804 (+/-1.11)\n",
      "At episode 2680, reward is 1.32 (+/-1.08)\n",
      "At episode 2700, reward is 1.816 (+/-1.283)\n",
      "At episode 2720, reward is 1.671 (+/-1.185)\n",
      "At episode 2740, reward is 2.011 (+/-1.565)\n",
      "At episode 2760, reward is 2.169 (+/-1.383)\n",
      "At episode 2780, reward is 2.146 (+/-1.773)\n",
      "At episode 2800, reward is 3.236 (+/-2.144)\n",
      "At episode 2820, reward is 1.382 (+/-0.872)\n",
      "At episode 2840, reward is 2.244 (+/-1.576)\n",
      "At episode 2860, reward is 2.062 (+/-1.556)\n",
      "At episode 2880, reward is 2.456 (+/-1.414)\n",
      "At episode 2900, reward is 1.574 (+/-1.03)\n",
      "At episode 2920, reward is 2.294 (+/-1.745)\n",
      "At episode 2940, reward is 2.696 (+/-2.853)\n",
      "At episode 2960, reward is 3.257 (+/-1.733)\n",
      "At episode 2980, reward is 2.424 (+/-2.011)\n",
      "At episode 3000, reward is 2.053 (+/-1.809)\n",
      "At episode 3020, reward is 3.773 (+/-2.188)\n",
      "At episode 3040, reward is 3.6 (+/-1.845)\n",
      "At episode 3060, reward is 4.314 (+/-2.862)\n",
      "At episode 3080, reward is 3.368 (+/-2.099)\n",
      "At episode 3100, reward is 3.343 (+/-2.038)\n",
      "At episode 3120, reward is 3.932 (+/-2.382)\n",
      "At episode 3140, reward is 4.826 (+/-2.472)\n",
      "At episode 3160, reward is 4.104 (+/-3.206)\n",
      "At episode 3180, reward is 4.494 (+/-3.607)\n",
      "At episode 3200, reward is 4.355 (+/-2.695)\n",
      "At episode 3220, reward is 4.295 (+/-2.445)\n",
      "At episode 3240, reward is 4.434 (+/-3.418)\n",
      "At episode 3260, reward is 4.935 (+/-4.214)\n",
      "At episode 3280, reward is 5.034 (+/-3.563)\n",
      "At episode 3300, reward is 5.859 (+/-4.93)\n",
      "At episode 3320, reward is 4.584 (+/-3.286)\n",
      "At episode 3340, reward is 4.91 (+/-4.701)\n",
      "At episode 3360, reward is 4.281 (+/-3.826)\n",
      "At episode 3380, reward is 4.112 (+/-3.969)\n",
      "At episode 3400, reward is 6.32 (+/-6.783)\n",
      "At episode 3420, reward is 7.21 (+/-6.119)\n",
      "At episode 3440, reward is 7.039 (+/-6.619)\n",
      "At episode 3460, reward is 5.824 (+/-4.908)\n",
      "At episode 3480, reward is 7.503 (+/-3.969)\n",
      "At episode 3500, reward is 14.491 (+/-13.869)\n",
      "At episode 3520, reward is 9.476 (+/-8.711)\n",
      "At episode 3540, reward is 8.148 (+/-7.945)\n",
      "At episode 3560, reward is 9.75 (+/-10.971)\n",
      "At episode 3580, reward is 9.721 (+/-8.831)\n",
      "At episode 3600, reward is 6.486 (+/-8.738)\n",
      "At episode 3620, reward is 8.704 (+/-14.042)\n",
      "At episode 3640, reward is 11.167 (+/-9.923)\n",
      "At episode 3660, reward is 7.156 (+/-7.092)\n",
      "At episode 3680, reward is 8.522 (+/-9.527)\n",
      "At episode 3700, reward is 5.515 (+/-5.127)\n",
      "At episode 3720, reward is 7.686 (+/-7.413)\n",
      "At episode 3740, reward is 7.108 (+/-6.935)\n",
      "At episode 3760, reward is 10.928 (+/-10.295)\n",
      "At episode 3780, reward is 14.141 (+/-17.315)\n",
      "At episode 3800, reward is 13.936 (+/-17.88)\n",
      "At episode 3820, reward is 14.056 (+/-14.836)\n",
      "At episode 3840, reward is 8.3 (+/-10.047)\n",
      "At episode 3860, reward is 20.48 (+/-26.794)\n",
      "At episode 3880, reward is 20.46 (+/-26.469)\n",
      "At episode 3900, reward is 16.704 (+/-22.94)\n",
      "At episode 3920, reward is 22.089 (+/-24.91)\n",
      "At episode 3940, reward is 31.015 (+/-42.458)\n",
      "At episode 3960, reward is 59.403 (+/-49.27)\n",
      "At episode 3980, reward is 25.792 (+/-28.183)\n",
      "At episode 4000, reward is 30.526 (+/-27.188)\n",
      "At episode 4020, reward is 43.84 (+/-30.587)\n",
      "At episode 4040, reward is 24.266 (+/-20.068)\n",
      "At episode 4060, reward is 43.074 (+/-41.275)\n",
      "At episode 4080, reward is 42.34 (+/-51.811)\n",
      "At episode 4100, reward is 40.483 (+/-38.256)\n",
      "At episode 4120, reward is 17.312 (+/-16.379)\n",
      "At episode 4140, reward is 31.967 (+/-36.158)\n",
      "At episode 4160, reward is 19.3 (+/-16.136)\n",
      "At episode 4180, reward is 16.534 (+/-9.492)\n",
      "At episode 4200, reward is 35.2 (+/-27.684)\n",
      "At episode 4220, reward is 24.306 (+/-27.133)\n",
      "At episode 4240, reward is 15.293 (+/-13.671)\n",
      "At episode 4260, reward is 20.407 (+/-22.729)\n",
      "At episode 4280, reward is 20.605 (+/-16.101)\n",
      "At episode 4300, reward is 21.418 (+/-17.361)\n",
      "At episode 4320, reward is 27.243 (+/-28.227)\n",
      "At episode 4340, reward is 35.782 (+/-23.78)\n",
      "At episode 4360, reward is 22.362 (+/-22.163)\n",
      "At episode 4380, reward is 24.515 (+/-21.664)\n",
      "At episode 4400, reward is 41.21 (+/-29.755)\n",
      "At episode 4420, reward is 31.916 (+/-20.127)\n",
      "At episode 4440, reward is 18.617 (+/-17.959)\n",
      "At episode 4460, reward is 21.826 (+/-21.358)\n",
      "At episode 4480, reward is 30.105 (+/-28.195)\n",
      "At episode 4500, reward is 31.818 (+/-21.437)\n",
      "At episode 4520, reward is 23.901 (+/-18.268)\n",
      "At episode 4540, reward is 19.309 (+/-15.649)\n",
      "At episode 4560, reward is 33.36 (+/-36.312)\n",
      "At episode 4580, reward is 25.066 (+/-21.728)\n"
     ]
    }
   ],
   "source": [
    "trainer.train(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "model_name = '04_noisynetdueling_01'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "model_name = '04_noisynetdueling_02'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "model_name = '04_noisynetdueling_03'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "model_name = '04_noisynetdueling_04'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "model_name = '04_noisynetdueling_05'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array([])\n",
    "print('mean: {:.3f}, median: {:.3f}'.format(res.mean(), np.median(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
