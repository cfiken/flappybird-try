{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "# basic\n",
    "import logging\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# ml\n",
    "import tensorflow as tf\n",
    "\n",
    "# gym\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import gym_ple\n",
    "\n",
    "# disply\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select gpu\n",
    "gpu_index = 0\n",
    "from tensorflow.keras.backend import set_session\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=str(gpu_index),\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(NamedTuple):\n",
    "    state: gym.spaces.Box\n",
    "    action: int\n",
    "    reward: float\n",
    "    next_state: gym.spaces.Box\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir='./logs/', dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.keras.callbacks.TensorBoard(self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "    \n",
    "    @property\n",
    "    def ckpt_path(self):\n",
    "        return os.path.join(self.log_dir, 'model.ckpt')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNoisyLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_outputs: int):\n",
    "        super(AgentNoisyLayer, self).__init__(trainable=True, name='noisynet', dtype=tf.float32)\n",
    "        # dense layer のみ noisy network に変更する\n",
    "        self.num_outputs = num_outputs\n",
    "        self.relu = relu = tf.nn.relu\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        self.dense_w_mu = self.add_weight(\"dense_w_mu\", [1024, 256], initializer=k_init)\n",
    "        self.dense_b_mu = self.add_weight(\"dense_b_mu\", [256], initializer='zero')\n",
    "        self.output_w_mu = self.add_weight(\"output_w_mu\", [256, num_outputs], initializer=k_init)\n",
    "        self.output_b_mu = self.add_weight(\"output_b_mu\", [num_outputs], initializer='zero')\n",
    "        \n",
    "        self.dense_w_sigma = self.add_weight(\"dense_w_sigma\", [1024, 256])\n",
    "        self.dense_b_sigma = self.add_weight(\"dense_b_sigma\", [256])\n",
    "        self.output_w_sigma = self.add_weight(\"output_w_sigma\", [256, num_outputs])\n",
    "        self.output_b_sigma = self.add_weight(\"output_b_sigma\", [num_outputs])\n",
    "        \n",
    "    def call(self, x):\n",
    "        # noise\n",
    "        dense_in_noise = tf.random.normal([1024])\n",
    "        dense_out_noise = tf.random.normal([256])\n",
    "        d_in = self._noise_w(dense_in_noise)\n",
    "        d_out = self._noise_w(dense_out_noise)\n",
    "        noise_dense_w = tf.matmul(tf.expand_dims(d_in, 1), tf.expand_dims(d_out, 0))\n",
    "        dense_w = self.dense_w_mu + tf.multiply(self.dense_w_sigma, noise_dense_w)\n",
    "        dense_b = self.dense_b_mu + tf.multiply(self.dense_b_sigma, self._noise_b(dense_out_noise))\n",
    "\n",
    "        output_in_noise = tf.random.normal([256])\n",
    "        output_out_noise = tf.random.normal([self.num_outputs])\n",
    "        o_in = self._noise_w(output_in_noise)\n",
    "        o_out = self._noise_w(output_out_noise)\n",
    "        noise_output_w = tf.matmul(tf.expand_dims(o_in, 1), tf.expand_dims(o_out, 0))\n",
    "        output_w = self.output_w_mu + tf.multiply(self.output_w_sigma, noise_output_w)\n",
    "        output_b = self.output_b_mu + tf.multiply(self.output_b_sigma, self._noise_b(output_out_noise))\n",
    "        \n",
    "        # forward\n",
    "        outputs = tf.matmul(tf.to_float(x), dense_w) + dense_b\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = tf.matmul(outputs, output_w) + output_b\n",
    "        return outputs\n",
    "    \n",
    "    def _noise_w(self, x):\n",
    "        return tf.multiply(tf.sign(x), tf.pow(tf.abs(x), 0.5))\n",
    "    \n",
    "    def _noise_b(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, is_training: bool, num_outputs: int):\n",
    "        super(AgentModel, self).__init__()\n",
    "        self.is_training = is_training\n",
    "        self.num_outputs = num_outputs\n",
    "        k_init = tf.keras.initializers.glorot_normal()\n",
    "        relu = tf.nn.relu\n",
    "        self.conv_01 = tf.keras.layers.Conv2D(16, kernel_size=8, strides=4, padding='same', kernel_initializer=k_init, activation=relu) \n",
    "        self.conv_02 = tf.keras.layers.Conv2D(32, kernel_size=4, strides=4, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.conv_03 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', kernel_initializer=k_init, activation=relu)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        # dense layer のみ noisy network に変更する\n",
    "        self.noisy_layer = AgentNoisyLayer(num_outputs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        outputs = inputs\n",
    "        outputs = self.conv_01(outputs)\n",
    "        outputs = self.conv_02(outputs)\n",
    "        outputs = self.conv_03(outputs)\n",
    "        outputs = self.flatten(outputs)\n",
    "        outputs = self.noisy_layer(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        return [shape[0], self.num_outputs]\n",
    "    \n",
    "    def get_noise_mean(self):\n",
    "        all_weights = self.noisy_layer.get_weights()\n",
    "        noise_weights = np.mean([np.mean(np.abs(w)) for w in all_weights[4:7:2]])\n",
    "        noise_biases = np.mean([np.mean(np.abs(w)) for w in all_weights[5:8:2]])\n",
    "        return noise_weights, noise_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, actions, epsilon, input_shape, learning_rate=0.0001):\n",
    "        self.actions = actions\n",
    "        self.epsilon = epsilon\n",
    "        self.input_shape = input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "        self._teacher_model = None\n",
    "        self.initialize()\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.build()\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.model.compile(optimizer, loss='mse')\n",
    "        \n",
    "    def save(self, model_path):\n",
    "        self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        input_shape = (env.width, env.height, env.frame_count)\n",
    "        agent = cls(actions, epsilon, input_shape)\n",
    "        agent.initialize()\n",
    "        agent.model.load_weights(model_path)\n",
    "        return agent\n",
    "        \n",
    "    def build(self):\n",
    "        self.model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "\n",
    "        # teacher_model を更新するため、両方のモデルで一度計算し重みを取得する\n",
    "        self._teacher_model = AgentModel(is_training=True, num_outputs=len(self.actions))\n",
    "        dummy = np.random.randn(1, *self.input_shape).astype(np.float32)\n",
    "        dummy = tf.convert_to_tensor(dummy)\n",
    "        _ = self.model.call(dummy)\n",
    "        _ = self._teacher_model.call(dummy)\n",
    "        self.update_teacher()\n",
    "    \n",
    "    def policy(self, state) -> int:\n",
    "        '''\n",
    "        epsilon greedy で受け取った state をもとに行動を決定する\n",
    "        '''\n",
    "        # no longer use epsilon greedy\n",
    "        estimates = self.estimate(state)\n",
    "        return np.argmax(estimates)\n",
    "    \n",
    "    def estimate(self, state):\n",
    "        '''\n",
    "        ある state の状態価値を推定する\n",
    "        '''\n",
    "        state_as_batch = np.array([state])\n",
    "        return self.model.predict(state_as_batch)[0]\n",
    "    \n",
    "    def update(self, experiences, gamma):\n",
    "        '''\n",
    "        与えられた experiences をもとに学習\n",
    "        '''\n",
    "        states = np.array([e.state for e in experiences])\n",
    "        next_states = np.array([e.next_state for e in experiences])\n",
    "\n",
    "        estimated_values = self.model.predict(states)\n",
    "        next_state_values = self._teacher_model.predict(next_states)\n",
    "        \n",
    "        # train\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.reward\n",
    "            if not e.done:\n",
    "                reward += gamma * np.max(next_state_values[i])\n",
    "            estimated_values[i][e.action] = reward\n",
    "        loss = self.model.train_on_batch(states, estimated_values)\n",
    "        return loss\n",
    "    \n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def play(self, env, episode_count: int = 2, render: bool = True):\n",
    "        total_rewards = []\n",
    "        for e in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                action = self.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step_with_raw_reward(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                episode_reward += step_reward\n",
    "                state = next_state\n",
    "            print('episode {}, total reward: {:.4f}'.format(e, episode_reward))\n",
    "            total_rewards.append(episode_reward)\n",
    "                \n",
    "        env.reset()\n",
    "        print('reward by {}, mean: {:.4f}, std: {:.4f}'.format(episode_count, np.mean(total_rewards), np.std(total_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer:\n",
    "    \n",
    "    def __init__(self, env, frame_count, width, height, render=False, outdir='./playlogs/'):\n",
    "        self._env = env\n",
    "        if render:\n",
    "            outdir = outdir\n",
    "            env = Monitor(env, directory=outdir, video_callable=(lambda x: x % 5 == 0), force=True)\n",
    "            self._env = env\n",
    "        self.frame_count = frame_count\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "        \n",
    "    def render(self):\n",
    "        self._env.render(mode = 'rgb_array')\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), self.reward_shaping(reward), done\n",
    "    \n",
    "    def step_with_raw_reward(self, action):\n",
    "        next_state, reward, done, _ = self._env.step(action)\n",
    "        return self.transform(next_state), reward, done\n",
    "        \n",
    "    def transform(self, state):\n",
    "        state = state[:400, :, :]\n",
    "        grayed = Image.fromarray(state).convert('L')  # h, w, c -> h, w\n",
    "        \n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(np.float32)\n",
    "        resized = np.transpose(resized, (1, 0)) # h, w -> w, h\n",
    "        normalized = resized / 255.0\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        feature = np.transpose(feature, (1, 2, 0))  # [f, w, h] -> [w, h, f]\n",
    "        \n",
    "        return feature\n",
    "\n",
    "    def reward_shaping(self, reward):\n",
    "        if 0.01 > reward > -0.01:\n",
    "            return 0.01\n",
    "        elif reward <= -0.1:\n",
    "            return -1.0\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, filename, buffer_size=50000, batch_size=32, gamma=0.98, report_interval=20):\n",
    "        self.file_name = filename\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.experiences = deque(maxlen=buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        self.logger = Logger(dir_name=filename)\n",
    "        self._max_reward = 0\n",
    "        self.teacher_update_freq = 10000\n",
    "        \n",
    "    def train(self, env, episode_count=10000, initial_count=200, model_path = None):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if model_path:\n",
    "            agent = Agent.load(env, model_path, epsilon=0.1)\n",
    "        else:\n",
    "            agent = Agent(actions, 0.1, input_shape=(env.width, env.height, env.frame_count))\n",
    "        self.agent = agent\n",
    "        self.train_loop(env, agent, episode_count, initial_count)\n",
    "        \n",
    "    def train_loop(self, env, agent, episode_count, initial_count):\n",
    "        \n",
    "        for episode in range(episode_count):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action = agent.policy(state)\n",
    "                step_reward = 0\n",
    "                for _ in range(4):\n",
    "                    next_state, reward, done = env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "                    step_reward += reward\n",
    "                e = Experience(state, action, step_reward, next_state, done)\n",
    "                self.experiences.append(e)\n",
    "                episode_reward += step_reward\n",
    "                loss = self.step(episode, agent)\n",
    "                if loss is None:\n",
    "                    from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "                state = next_state\n",
    "                \n",
    "                if not self.training and (len(self.experiences) >= self.buffer_size or episode >= initial_count):\n",
    "                    self.begin_training(agent)\n",
    "                    self.training = True\n",
    "            \n",
    "            self.end_episode(episode, episode_reward, loss, agent)\n",
    "    \n",
    "    def step(self, step, agent):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            loss = agent.update(batch, self.gamma)\n",
    "            self.training_count += 1\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "            return loss\n",
    "        return 0.0\n",
    "            \n",
    "    def begin_training(self, agent):\n",
    "        print('start training!')\n",
    "        self.logger.set_model(agent.model)\n",
    "        \n",
    "    def end_episode(self, episode, reward, loss, agent):\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            noise_w, noise_b = agent.model.get_noise_mean()\n",
    "            self.logger.write(self.training_count, \"noise_weights\", noise_w)\n",
    "            self.logger.write(self.training_count, \"noise_biases\", noise_b)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.ckpt_path)\n",
    "                self._max_reward = reward\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)\n",
    "        \n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '04_noisynetdqn_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kentaro.nakanishi/.local/share/virtualenvs/flappybird-try-wqQoCzq9/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128)\n",
    "trainer = Trainer(filename=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 20, reward is 0.631 (+/-0.426)\n",
      "At episode 40, reward is 0.65 (+/-0.305)\n",
      "At episode 60, reward is 0.658 (+/-0.302)\n",
      "At episode 80, reward is 0.647 (+/-0.311)\n",
      "At episode 100, reward is 0.586 (+/-0.058)\n",
      "At episode 120, reward is 0.826 (+/-0.522)\n",
      "At episode 140, reward is 0.618 (+/-0.331)\n",
      "At episode 160, reward is 0.807 (+/-0.526)\n",
      "At episode 180, reward is 0.591 (+/-0.052)\n",
      "start training!\n",
      "At episode 200, reward is 0.997 (+/-0.549)\n",
      "At episode 220, reward is 0.608 (+/-0.277)\n",
      "At episode 240, reward is 0.515 (+/-0.142)\n",
      "At episode 260, reward is 0.961 (+/-0.547)\n",
      "At episode 280, reward is 0.72 (+/-0.37)\n",
      "At episode 300, reward is 0.796 (+/-0.452)\n",
      "At episode 320, reward is 0.734 (+/-0.413)\n",
      "At episode 340, reward is 0.733 (+/-0.366)\n",
      "At episode 360, reward is 0.804 (+/-0.441)\n",
      "At episode 380, reward is 0.819 (+/-0.511)\n",
      "At episode 400, reward is 0.874 (+/-0.515)\n",
      "At episode 420, reward is 0.724 (+/-0.377)\n",
      "At episode 440, reward is 0.704 (+/-0.33)\n",
      "At episode 460, reward is 0.746 (+/-0.459)\n",
      "At episode 480, reward is 0.752 (+/-0.64)\n",
      "At episode 500, reward is 1.03 (+/-0.704)\n",
      "At episode 520, reward is 0.951 (+/-0.925)\n",
      "At episode 540, reward is 0.627 (+/-0.327)\n",
      "At episode 560, reward is 0.878 (+/-0.524)\n",
      "At episode 580, reward is 0.634 (+/-0.319)\n",
      "At episode 600, reward is 0.879 (+/-0.727)\n",
      "At episode 620, reward is 0.937 (+/-0.806)\n",
      "At episode 640, reward is 0.714 (+/-0.383)\n",
      "At episode 660, reward is 0.833 (+/-0.665)\n",
      "At episode 680, reward is 0.951 (+/-0.69)\n",
      "At episode 700, reward is 0.666 (+/-0.454)\n",
      "At episode 720, reward is 0.835 (+/-0.638)\n",
      "At episode 740, reward is 0.806 (+/-0.509)\n",
      "At episode 760, reward is 0.799 (+/-0.651)\n",
      "At episode 780, reward is 0.626 (+/-0.424)\n",
      "At episode 800, reward is 0.972 (+/-0.766)\n",
      "At episode 820, reward is 0.642 (+/-0.427)\n",
      "At episode 840, reward is 0.786 (+/-0.426)\n",
      "At episode 860, reward is 0.806 (+/-0.502)\n",
      "At episode 880, reward is 0.807 (+/-0.451)\n",
      "At episode 900, reward is 0.782 (+/-0.432)\n",
      "At episode 920, reward is 1.04 (+/-0.981)\n",
      "At episode 940, reward is 0.762 (+/-0.481)\n",
      "At episode 960, reward is 0.624 (+/-0.268)\n",
      "At episode 980, reward is 0.62 (+/-0.329)\n",
      "At episode 1000, reward is 0.845 (+/-0.533)\n",
      "At episode 1020, reward is 0.516 (+/-0.119)\n",
      "At episode 1040, reward is 0.906 (+/-0.937)\n",
      "At episode 1060, reward is 0.849 (+/-0.561)\n",
      "At episode 1080, reward is 0.696 (+/-0.383)\n",
      "At episode 1100, reward is 1.053 (+/-0.639)\n",
      "At episode 1120, reward is 0.912 (+/-0.641)\n",
      "At episode 1140, reward is 0.889 (+/-0.532)\n",
      "At episode 1160, reward is 0.675 (+/-0.448)\n",
      "At episode 1180, reward is 0.888 (+/-0.604)\n",
      "At episode 1200, reward is 0.854 (+/-0.487)\n",
      "At episode 1220, reward is 0.847 (+/-0.62)\n",
      "At episode 1240, reward is 0.817 (+/-0.529)\n",
      "At episode 1260, reward is 0.769 (+/-0.427)\n",
      "At episode 1280, reward is 0.646 (+/-0.37)\n",
      "At episode 1300, reward is 0.634 (+/-0.422)\n",
      "At episode 1320, reward is 0.943 (+/-0.843)\n",
      "At episode 1340, reward is 1.168 (+/-1.21)\n",
      "At episode 1360, reward is 0.519 (+/-0.134)\n",
      "At episode 1380, reward is 0.693 (+/-0.358)\n",
      "At episode 1400, reward is 0.96 (+/-0.712)\n",
      "At episode 1420, reward is 0.858 (+/-0.522)\n",
      "At episode 1440, reward is 0.991 (+/-0.583)\n",
      "At episode 1460, reward is 0.814 (+/-0.513)\n",
      "At episode 1480, reward is 0.824 (+/-0.892)\n",
      "At episode 1500, reward is 0.776 (+/-0.393)\n",
      "At episode 1520, reward is 1.129 (+/-1.088)\n",
      "At episode 1540, reward is 0.899 (+/-0.537)\n",
      "At episode 1560, reward is 0.778 (+/-0.467)\n",
      "At episode 1580, reward is 0.86 (+/-0.502)\n",
      "At episode 1600, reward is 0.922 (+/-0.545)\n",
      "At episode 1620, reward is 1.249 (+/-0.88)\n",
      "At episode 1640, reward is 1.1 (+/-0.605)\n",
      "At episode 1660, reward is 0.99 (+/-0.582)\n",
      "At episode 1680, reward is 1.015 (+/-0.677)\n",
      "At episode 1700, reward is 0.863 (+/-0.631)\n",
      "At episode 1720, reward is 1.003 (+/-0.737)\n",
      "At episode 1740, reward is 1.272 (+/-0.72)\n",
      "At episode 1760, reward is 1.008 (+/-0.741)\n",
      "At episode 1780, reward is 1.17 (+/-0.932)\n",
      "At episode 1800, reward is 1.032 (+/-0.725)\n",
      "At episode 1820, reward is 0.963 (+/-0.77)\n",
      "At episode 1840, reward is 0.991 (+/-0.696)\n",
      "At episode 1860, reward is 0.941 (+/-0.544)\n",
      "At episode 1880, reward is 0.92 (+/-0.54)\n",
      "At episode 1900, reward is 0.996 (+/-0.727)\n",
      "At episode 1920, reward is 1.006 (+/-0.541)\n",
      "At episode 1940, reward is 1.14 (+/-0.701)\n",
      "At episode 1960, reward is 0.933 (+/-0.733)\n",
      "At episode 1980, reward is 0.969 (+/-0.585)\n",
      "At episode 2000, reward is 0.92 (+/-0.538)\n",
      "At episode 2020, reward is 1.216 (+/-0.721)\n",
      "At episode 2040, reward is 1.384 (+/-0.975)\n",
      "At episode 2060, reward is 1.408 (+/-1.03)\n",
      "At episode 2080, reward is 0.862 (+/-0.501)\n",
      "At episode 2100, reward is 1.33 (+/-1.082)\n",
      "At episode 2120, reward is 1.068 (+/-0.87)\n",
      "At episode 2140, reward is 1.19 (+/-0.843)\n",
      "At episode 2160, reward is 1.114 (+/-0.622)\n",
      "At episode 2180, reward is 1.394 (+/-0.977)\n",
      "At episode 2200, reward is 1.292 (+/-1.13)\n",
      "At episode 2220, reward is 1.289 (+/-1.003)\n",
      "At episode 2240, reward is 1.201 (+/-0.709)\n",
      "At episode 2260, reward is 1.005 (+/-0.738)\n",
      "At episode 2280, reward is 1.383 (+/-0.983)\n",
      "At episode 2300, reward is 1.25 (+/-1.145)\n",
      "At episode 2320, reward is 0.912 (+/-0.654)\n",
      "At episode 2340, reward is 1.52 (+/-1.052)\n",
      "At episode 2360, reward is 1.237 (+/-0.744)\n",
      "At episode 2380, reward is 1.412 (+/-0.83)\n",
      "At episode 2400, reward is 1.003 (+/-0.961)\n",
      "At episode 2420, reward is 1.318 (+/-0.822)\n",
      "At episode 2440, reward is 1.666 (+/-1.177)\n",
      "At episode 2460, reward is 1.448 (+/-1.169)\n",
      "At episode 2480, reward is 1.53 (+/-1.153)\n",
      "At episode 2500, reward is 1.185 (+/-1.139)\n",
      "At episode 2520, reward is 1.275 (+/-0.84)\n",
      "At episode 2540, reward is 2.041 (+/-1.305)\n",
      "At episode 2560, reward is 1.434 (+/-0.953)\n",
      "At episode 2580, reward is 1.252 (+/-0.772)\n",
      "At episode 2600, reward is 1.594 (+/-1.103)\n",
      "At episode 2620, reward is 1.225 (+/-0.843)\n",
      "At episode 2640, reward is 1.886 (+/-1.442)\n",
      "At episode 2660, reward is 1.429 (+/-1.221)\n",
      "At episode 2680, reward is 1.89 (+/-1.364)\n",
      "At episode 2700, reward is 1.586 (+/-1.181)\n",
      "At episode 2720, reward is 2.193 (+/-1.446)\n",
      "At episode 2740, reward is 2.276 (+/-1.501)\n",
      "At episode 2760, reward is 2.054 (+/-1.451)\n",
      "At episode 2780, reward is 2.094 (+/-1.57)\n",
      "At episode 2800, reward is 2.903 (+/-1.953)\n",
      "At episode 2820, reward is 2.777 (+/-1.927)\n",
      "At episode 2840, reward is 1.793 (+/-1.473)\n",
      "At episode 2860, reward is 3.003 (+/-2.47)\n",
      "At episode 2880, reward is 2.488 (+/-1.976)\n",
      "At episode 2900, reward is 2.069 (+/-2.065)\n",
      "At episode 2920, reward is 3.016 (+/-1.326)\n",
      "At episode 2940, reward is 2.225 (+/-1.944)\n",
      "At episode 2960, reward is 3.226 (+/-2.435)\n",
      "At episode 2980, reward is 4.045 (+/-2.521)\n",
      "At episode 3000, reward is 3.713 (+/-3.573)\n",
      "At episode 3020, reward is 3.321 (+/-2.31)\n",
      "At episode 3040, reward is 2.967 (+/-2.238)\n",
      "At episode 3060, reward is 5.566 (+/-3.018)\n",
      "At episode 3080, reward is 4.613 (+/-3.309)\n",
      "At episode 3100, reward is 4.191 (+/-3.082)\n",
      "At episode 3120, reward is 3.792 (+/-2.819)\n",
      "At episode 3140, reward is 6.038 (+/-4.654)\n",
      "At episode 3160, reward is 5.993 (+/-6.656)\n",
      "At episode 3180, reward is 4.347 (+/-2.983)\n",
      "At episode 3200, reward is 2.899 (+/-2.554)\n",
      "At episode 3220, reward is 7.495 (+/-5.152)\n",
      "At episode 3240, reward is 5.713 (+/-4.204)\n",
      "At episode 3260, reward is 5.683 (+/-8.526)\n",
      "At episode 3280, reward is 7.036 (+/-5.395)\n",
      "At episode 3300, reward is 5.093 (+/-3.903)\n",
      "At episode 3320, reward is 7.586 (+/-6.182)\n",
      "At episode 3340, reward is 5.993 (+/-5.38)\n",
      "At episode 3360, reward is 5.961 (+/-5.673)\n",
      "At episode 3380, reward is 8.517 (+/-9.105)\n",
      "At episode 3400, reward is 6.365 (+/-8.373)\n",
      "At episode 3420, reward is 5.429 (+/-4.47)\n",
      "At episode 3440, reward is 11.049 (+/-10.495)\n",
      "At episode 3460, reward is 16.942 (+/-12.847)\n",
      "At episode 3480, reward is 12.297 (+/-13.421)\n",
      "At episode 3500, reward is 10.38 (+/-7.326)\n",
      "At episode 3520, reward is 18.348 (+/-17.12)\n",
      "At episode 3540, reward is 10.066 (+/-9.74)\n",
      "At episode 3560, reward is 12.912 (+/-9.698)\n",
      "At episode 3580, reward is 21.802 (+/-20.4)\n",
      "At episode 3600, reward is 10.471 (+/-8.339)\n",
      "At episode 3620, reward is 17.943 (+/-12.23)\n",
      "At episode 3640, reward is 18.397 (+/-14.325)\n",
      "At episode 3660, reward is 20.239 (+/-16.479)\n",
      "At episode 3680, reward is 22.804 (+/-21.919)\n",
      "At episode 3700, reward is 24.408 (+/-21.163)\n",
      "At episode 3720, reward is 20.972 (+/-23.614)\n",
      "At episode 3740, reward is 13.521 (+/-12.623)\n",
      "At episode 3760, reward is 27.958 (+/-30.549)\n",
      "At episode 3780, reward is 18.766 (+/-19.664)\n",
      "At episode 3800, reward is 24.354 (+/-19.17)\n",
      "At episode 3820, reward is 46.125 (+/-55.089)\n",
      "At episode 3840, reward is 24.24 (+/-24.52)\n",
      "At episode 3860, reward is 54.269 (+/-55.512)\n",
      "At episode 3880, reward is 30.318 (+/-27.532)\n",
      "At episode 3900, reward is 24.59 (+/-23.718)\n",
      "At episode 3920, reward is 32.625 (+/-33.856)\n",
      "At episode 3940, reward is 25.246 (+/-25.01)\n",
      "At episode 3960, reward is 17.168 (+/-14.546)\n",
      "At episode 3980, reward is 17.617 (+/-23.574)\n",
      "At episode 4000, reward is 18.3 (+/-16.2)\n",
      "At episode 4020, reward is 14.823 (+/-16.896)\n",
      "At episode 4040, reward is 19.537 (+/-20.711)\n",
      "At episode 4060, reward is 25.809 (+/-22.729)\n",
      "At episode 4080, reward is 18.311 (+/-12.757)\n",
      "At episode 4100, reward is 26.622 (+/-27.782)\n",
      "At episode 4120, reward is 16.666 (+/-17.547)\n",
      "At episode 4140, reward is 17.507 (+/-12.655)\n",
      "At episode 4160, reward is 15.716 (+/-9.637)\n",
      "At episode 4180, reward is 13.014 (+/-12.014)\n",
      "At episode 4200, reward is 17.565 (+/-24.005)\n",
      "At episode 4220, reward is 13.69 (+/-12.037)\n",
      "At episode 4240, reward is 19.763 (+/-10.641)\n",
      "At episode 4260, reward is 8.759 (+/-11.394)\n",
      "At episode 4280, reward is 11.424 (+/-8.934)\n",
      "At episode 4300, reward is 20.707 (+/-11.962)\n",
      "At episode 4320, reward is 15.783 (+/-13.172)\n",
      "At episode 4340, reward is 16.723 (+/-27.667)\n",
      "At episode 4360, reward is 17.79 (+/-14.873)\n",
      "At episode 4380, reward is 15.697 (+/-14.159)\n",
      "At episode 4400, reward is 28.482 (+/-29.617)\n",
      "At episode 4420, reward is 26.445 (+/-28.874)\n",
      "At episode 4440, reward is 16.836 (+/-17.225)\n",
      "At episode 4460, reward is 18.901 (+/-16.663)\n",
      "At episode 4480, reward is 17.419 (+/-16.448)\n",
      "At episode 4500, reward is 23.772 (+/-17.564)\n",
      "At episode 4520, reward is 21.525 (+/-22.135)\n",
      "At episode 4540, reward is 22.147 (+/-23.679)\n",
      "At episode 4560, reward is 21.608 (+/-16.83)\n",
      "At episode 4580, reward is 36.213 (+/-33.91)\n",
      "At episode 4600, reward is 17.446 (+/-17.046)\n",
      "At episode 4620, reward is 18.603 (+/-15.352)\n",
      "At episode 4640, reward is 33.858 (+/-32.644)\n",
      "At episode 4660, reward is 26.107 (+/-33.913)\n",
      "At episode 4680, reward is 39.288 (+/-35.732)\n",
      "At episode 4700, reward is 34.372 (+/-34.937)\n",
      "At episode 4720, reward is 30.849 (+/-29.534)\n",
      "At episode 4740, reward is 21.454 (+/-17.115)\n",
      "At episode 4760, reward is 24.23 (+/-20.148)\n",
      "At episode 4780, reward is 21.965 (+/-32.713)\n",
      "At episode 4800, reward is 16.236 (+/-11.73)\n",
      "At episode 4820, reward is 22.518 (+/-22.842)\n",
      "At episode 4840, reward is 17.55 (+/-11.371)\n",
      "At episode 4860, reward is 17.317 (+/-11.326)\n",
      "At episode 4880, reward is 20.821 (+/-13.208)\n",
      "At episode 4900, reward is 16.138 (+/-13.868)\n",
      "At episode 4920, reward is 26.521 (+/-22.81)\n",
      "At episode 4940, reward is 17.934 (+/-15.304)\n",
      "At episode 4960, reward is 13.861 (+/-11.263)\n",
      "At episode 4980, reward is 20.241 (+/-24.067)\n",
      "At episode 5000, reward is 12.842 (+/-8.461)\n",
      "At episode 5020, reward is 12.807 (+/-6.805)\n",
      "At episode 5040, reward is 13.39 (+/-11.084)\n",
      "At episode 5060, reward is 17.995 (+/-18.039)\n",
      "At episode 5080, reward is 19.099 (+/-19.803)\n",
      "At episode 5100, reward is 28.164 (+/-31.176)\n",
      "At episode 5120, reward is 27.327 (+/-24.108)\n",
      "At episode 5140, reward is 18.144 (+/-13.487)\n",
      "At episode 5160, reward is 16.012 (+/-12.766)\n",
      "At episode 5180, reward is 19.474 (+/-16.128)\n",
      "At episode 5200, reward is 18.591 (+/-16.164)\n",
      "At episode 5220, reward is 27.413 (+/-30.842)\n",
      "At episode 5240, reward is 20.635 (+/-24.052)\n",
      "At episode 5260, reward is 16.963 (+/-18.409)\n",
      "At episode 5280, reward is 25.587 (+/-22.661)\n",
      "At episode 5300, reward is 36.591 (+/-35.614)\n",
      "At episode 5320, reward is 17.698 (+/-11.421)\n",
      "At episode 5340, reward is 26.675 (+/-23.896)\n",
      "At episode 5360, reward is 21.631 (+/-12.929)\n",
      "At episode 5380, reward is 26.309 (+/-22.444)\n",
      "At episode 5400, reward is 18.931 (+/-14.846)\n"
     ]
    }
   ],
   "source": [
    "trainer.train(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FlappyBird-v0')\n",
    "obs = Observer(env, 4, 128, 128, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "model_name = '04_noisynetdqn_01'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "model_name = '04_noisynetdqn_02'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "model_name = '04_noisynetdqn_03'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "model_name = '04_noisynetdqn_04'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "model_name = '04_noisynetdqn_05'\n",
    "agent = Agent.load(obs, 'logs/{}/model.ckpt'.format(model_name))\n",
    "agent.play(obs, episode_count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array([])\n",
    "print('mean: {:.3f}, median: {:.3f}'.format(res.mean(), np.median(res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
